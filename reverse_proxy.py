# reverse_proxy.py (æœ€ç»ˆå®Œç¾ç‰ˆ V5 - å®æ—¶æ¶æ„é€‚é…)

import logging
import requests
import re
import os
import json
from flask import Flask, request, Response
from urllib.parse import urlparse, urlunparse
from datetime import datetime, timedelta
import time
import uuid 
from flask import send_file 
from handler.poster_generator import get_missing_poster
from gevent import spawn, joinall
from websocket import create_connection
from database import custom_collection_db, queries_db
from database.connection import get_db_connection
from handler.custom_collection import RecommendationEngine
import config_manager
import constants
from routes.p115 import _get_cached_115_url

import extensions
import handler.emby as emby
logger = logging.getLogger(__name__)

MISSING_ID_PREFIX = "-800000_"

def to_missing_item_id(tmdb_id): 
    return f"{MISSING_ID_PREFIX}{tmdb_id}"

def is_missing_item_id(item_id):
    return isinstance(item_id, str) and item_id.startswith(MISSING_ID_PREFIX)

def parse_missing_item_id(item_id):
    # ä» -800000_12345 ä¸­æå–å‡º 12345
    return item_id.replace(MISSING_ID_PREFIX, "")
MIMICKED_ID_BASE = 900000
def to_mimicked_id(db_id): return str(-(MIMICKED_ID_BASE + db_id))
def from_mimicked_id(mimicked_id): return -(int(mimicked_id)) - MIMICKED_ID_BASE
def is_mimicked_id(item_id):
    try: return isinstance(item_id, str) and item_id.startswith('-')
    except: return False
MIMICKED_ITEMS_RE = re.compile(r'/emby/Users/([^/]+)/Items/(-(\d+))')
MIMICKED_ITEM_DETAILS_RE = re.compile(r'emby/Users/([^/]+)/Items/(-(\d+))$')

def _get_real_emby_url_and_key():
    base_url = config_manager.APP_CONFIG.get("emby_server_url", "").rstrip('/')
    api_key = config_manager.APP_CONFIG.get("emby_api_key", "")
    if not base_url or not api_key: raise ValueError("EmbyæœåŠ¡å™¨åœ°å€æˆ–API Keyæœªé…ç½®")
    return base_url, api_key

def _fetch_items_in_chunks(base_url, api_key, user_id, item_ids, fields):
    """
    å¹¶å‘åˆ†å—è·å– Emby é¡¹ç›®è¯¦æƒ…ã€‚
    """
    if not item_ids: return []
    
    # å»é‡
    unique_ids = list(dict.fromkeys(item_ids))
    
    def chunk_list(lst, n):
        for i in range(0, len(lst), n): yield lst[i:i + n]
    
    # é€‚å½“å¢å¤§åˆ†å—å¤§å°ä»¥å‡å°‘è¯·æ±‚æ•°
    id_chunks = list(chunk_list(unique_ids, 200))
    target_url = f"{base_url}/emby/Users/{user_id}/Items"
    
    def fetch_chunk(chunk):
        params = {'api_key': api_key, 'Ids': ",".join(chunk), 'Fields': fields}
        try:
            resp = requests.get(target_url, params=params, timeout=20)
            resp.raise_for_status()
            return resp.json().get("Items", [])
        except Exception as e:
            logger.error(f"å¹¶å‘è·å–æŸåˆ†å—æ•°æ®æ—¶å¤±è´¥: {e}")
            return None
            
    greenlets = [spawn(fetch_chunk, chunk) for chunk in id_chunks]
    joinall(greenlets)
    
    all_items = []
    for g in greenlets:
        if g.value: all_items.extend(g.value)
        
    return all_items

def _fetch_sorted_items_via_emby_proxy(user_id, item_ids, sort_by, sort_order, limit, offset, fields, total_record_count):
    """
    [æ¦œå•ç±»ä¸“ç”¨] 
    å½“æˆ‘ä»¬éœ€è¦å¯¹ä¸€ç»„å›ºå®šçš„ ID (æ¥è‡ªæ¦œå•) è¿›è¡Œæ’åºå’Œåˆ†é¡µæ—¶ä½¿ç”¨ã€‚
    åˆ©ç”¨ Emby çš„ GET è¯·æ±‚èƒ½åŠ›ï¼Œè®© Emby å¸®æˆ‘ä»¬è¿‡æ»¤æƒé™å¹¶æ’åºã€‚
    å¦‚æœ ID å¤ªå¤šï¼Œå›é€€åˆ°å†…å­˜æ’åºã€‚
    """
    base_url, api_key = _get_real_emby_url_and_key()
    
    # ä¼°ç®— URL é•¿åº¦
    estimated_ids_length = len(item_ids) * 33 # GUID é•¿åº¦ + é€—å·
    URL_LENGTH_THRESHOLD = 1800 # ä¿å®ˆé˜ˆå€¼

    try:
        if estimated_ids_length < URL_LENGTH_THRESHOLD:
            # --- è·¯å¾„ A: IDåˆ—è¡¨è¾ƒçŸ­ï¼Œç›´æ¥è¯·æ±‚ Emby (æœ€å¿«ï¼Œä¸”è‡ªåŠ¨å¤„ç†æƒé™) ---
            logger.trace(f"  âœ [Emby ä»£ç†æ’åº] IDåˆ—è¡¨è¾ƒçŸ­ ({len(item_ids)}ä¸ª)ï¼Œä½¿ç”¨ GET æ–¹æ³•ã€‚")
            target_url = f"{base_url}/emby/Users/{user_id}/Items"
            emby_params = {
                'api_key': api_key, 'Ids': ",".join(item_ids), 'Fields': fields,
                'SortBy': sort_by, 'SortOrder': sort_order,
                'StartIndex': offset, 'Limit': limit,
            }
            resp = requests.get(target_url, params=emby_params, timeout=25)
            resp.raise_for_status()
            emby_data = resp.json()
            # æ³¨æ„ï¼šEmby è¿”å›çš„ TotalRecordCount æ˜¯ç»è¿‡æƒé™è¿‡æ»¤åçš„æ•°é‡
            # å¦‚æœæˆ‘ä»¬ä¼ å…¥çš„ total_record_count æ˜¯å…¨é‡çš„ï¼Œè¿™é‡Œå¯èƒ½éœ€è¦ä¿®æ­£ï¼Œä½†ä¸ºäº†åˆ†é¡µæ¡æ­£å¸¸ï¼Œé€šå¸¸ç›´æ¥ç”¨ Emby è¿”å›çš„
            return emby_data
        else:
            # --- è·¯å¾„ B: IDåˆ—è¡¨è¶…é•¿ï¼Œå†…å­˜æ’åº (å®‰å…¨å›é€€) ---
            logger.trace(f"  âœ [å†…å­˜æ’åºå›é€€] IDåˆ—è¡¨è¶…é•¿ ({len(item_ids)}ä¸ª)ï¼Œå¯åŠ¨å†…å­˜æ’åºã€‚")
            
            # 1. è·å–æ‰€æœ‰é¡¹ç›®çš„è¯¦æƒ… (Emby ä¼šè‡ªåŠ¨è¿‡æ»¤æ‰æ— æƒè®¿é—®çš„é¡¹ç›®)
            # æˆ‘ä»¬éœ€è¦è·å–ç”¨äºæ’åºçš„å­—æ®µ
            primary_sort_by = sort_by.split(',')[0]
            fields_for_sorting = f"{fields},{primary_sort_by}"
            
            all_items_details = _fetch_items_in_chunks(base_url, api_key, user_id, item_ids, fields_for_sorting)
            
            # æ›´æ–°æ€»æ•° (è¿‡æ»¤åçš„çœŸå®æ•°é‡)
            real_total_count = len(all_items_details)

            # 2. åœ¨å†…å­˜ä¸­æ’åº
            try:
                is_desc = sort_order == 'Descending'
                
                def get_sort_val(item):
                    val = item.get(primary_sort_by)
                    # å¤„ç†æ—¥æœŸ
                    if 'Date' in primary_sort_by or 'Year' in primary_sort_by:
                        return val or "1900-01-01T00:00:00.000Z"
                    # å¤„ç†æ•°å­—
                    if 'Rating' in primary_sort_by or 'Count' in primary_sort_by:
                        return float(val) if val is not None else 0
                    # å¤„ç†å­—ç¬¦ä¸²
                    return str(val or "").lower()

                all_items_details.sort(key=get_sort_val, reverse=is_desc)
            except Exception as sort_e:
                logger.error(f"  âœ å†…å­˜æ’åºæ—¶å‘ç”Ÿé”™è¯¯: {sort_e}", exc_info=True)
            
            # 3. åœ¨å†…å­˜ä¸­åˆ†é¡µ
            paginated_items = all_items_details[offset : offset + limit]
            
            return {"Items": paginated_items, "TotalRecordCount": real_total_count}

    except Exception as e:
        logger.error(f"  âœ Embyä»£ç†æ’åºæˆ–å†…å­˜å›é€€æ—¶å¤±è´¥: {e}", exc_info=True)
        return {"Items": [], "TotalRecordCount": 0}

def handle_get_views():
    """
    è·å–ç”¨æˆ·çš„ä¸»é¡µè§†å›¾åˆ—è¡¨ã€‚
    """
    real_server_id = extensions.EMBY_SERVER_ID
    if not real_server_id:
        return "Proxy is not ready", 503

    try:
        user_id_match = re.search(r'/emby/Users/([^/]+)/Views', request.path)
        if not user_id_match:
            return "Could not determine user from request path", 400
        user_id = user_id_match.group(1)

        # 1. è·å–åŸç”Ÿåº“
        user_visible_native_libs = emby.get_emby_libraries(
            config_manager.APP_CONFIG.get("emby_server_url", ""),
            config_manager.APP_CONFIG.get("emby_api_key", ""),
            user_id
        )
        if user_visible_native_libs is None: user_visible_native_libs = []

        # 2. ç”Ÿæˆè™šæ‹Ÿåº“
        collections = custom_collection_db.get_all_active_custom_collections()
        fake_views_items = []
        
        for coll in collections:
            # ç‰©ç†æ£€æŸ¥ï¼šåº“åœ¨Embyé‡Œæœ‰å®ä½“å—ï¼Ÿ
            real_emby_collection_id = coll.get('emby_collection_id')
            if not real_emby_collection_id:
                continue

            # æƒé™æ£€æŸ¥ï¼šå¦‚æœè®¾ç½®äº† allowed_user_idsï¼Œåˆ™æ£€æŸ¥
            allowed_users = coll.get('allowed_user_ids')
            if allowed_users and isinstance(allowed_users, list):
                if user_id not in allowed_users:
                    continue
            
            # ç”Ÿæˆè™šæ‹Ÿåº“å¯¹è±¡
            db_id = coll['id']
            mimicked_id = to_mimicked_id(db_id)
            # ä½¿ç”¨æ—¶é—´æˆ³å¼ºåˆ¶åˆ·æ–°å°é¢
            image_tags = {"Primary": f"{real_emby_collection_id}?timestamp={int(time.time())}"}
            definition = coll.get('definition_json') or {}
            
            item_type_from_db = definition.get('item_type', 'Movie')
            collection_type = "mixed"
            if not (isinstance(item_type_from_db, list) and len(item_type_from_db) > 1):
                 authoritative_type = item_type_from_db[0] if isinstance(item_type_from_db, list) and item_type_from_db else item_type_from_db if isinstance(item_type_from_db, str) else 'Movie'
                 collection_type = "tvshows" if authoritative_type == 'Series' else "movies"

            fake_view = {
                "Name": coll['name'], "ServerId": real_server_id, "Id": mimicked_id,
                "Guid": str(uuid.uuid4()), "Etag": f"{db_id}{int(time.time())}",
                "DateCreated": "2025-01-01T00:00:00.0000000Z", "CanDelete": False, "CanDownload": False,
                "SortName": coll['name'], "ExternalUrls": [], "ProviderIds": {}, "IsFolder": True,
                "ParentId": "2", "Type": "CollectionFolder", "PresentationUniqueKey": str(uuid.uuid4()),
                "DisplayPreferencesId": f"custom-{db_id}", "ForcedSortName": coll['name'],
                "Taglines": [], "RemoteTrailers": [],
                "UserData": {"PlaybackPositionTicks": 0, "IsFavorite": False, "Played": False},
                "ChildCount": coll.get('in_library_count', 1),
                "PrimaryImageAspectRatio": 1.7777777777777777, 
                "CollectionType": collection_type, "ImageTags": image_tags, "BackdropImageTags": [], 
                "LockedFields": [], "LockData": False
            }
            fake_views_items.append(fake_view)
        
        # 3. åˆå¹¶ä¸æ’åº
        native_views_items = []
        should_merge_native = config_manager.APP_CONFIG.get('proxy_merge_native_libraries', True)
        if should_merge_native:
            all_native_views = user_visible_native_libs
            raw_selection = config_manager.APP_CONFIG.get('proxy_native_view_selection', '')
            selected_native_view_ids = [x.strip() for x in raw_selection.split(',') if x.strip()] if isinstance(raw_selection, str) else raw_selection
            
            if selected_native_view_ids:
                native_views_items = [view for view in all_native_views if view.get("Id") in selected_native_view_ids]
            else:
                native_views_items = []
        
        final_items = []
        native_order = config_manager.APP_CONFIG.get('proxy_native_view_order', 'before')
        if native_order == 'after':
            final_items.extend(fake_views_items)
            final_items.extend(native_views_items)
        else:
            final_items.extend(native_views_items)
            final_items.extend(fake_views_items)

        final_response = {"Items": final_items, "TotalRecordCount": len(final_items)}
        return Response(json.dumps(final_response), mimetype='application/json')
        
    except Exception as e:
        logger.error(f"[PROXY] è·å–è§†å›¾æ•°æ®æ—¶å‡ºé”™: {e}", exc_info=True)
        return "Internal Proxy Error", 500

def handle_get_mimicked_library_details(user_id, mimicked_id):
    try:
        real_db_id = from_mimicked_id(mimicked_id)
        coll = custom_collection_db.get_custom_collection_by_id(real_db_id)
        if not coll: return "Not Found", 404

        real_server_id = extensions.EMBY_SERVER_ID
        real_emby_collection_id = coll.get('emby_collection_id')
        image_tags = {"Primary": real_emby_collection_id} if real_emby_collection_id else {}
        
        definition = coll.get('definition_json') or {}
        item_type_from_db = definition.get('item_type', 'Movie')
        collection_type = "mixed"
        if not (isinstance(item_type_from_db, list) and len(item_type_from_db) > 1):
             authoritative_type = item_type_from_db[0] if isinstance(item_type_from_db, list) and item_type_from_db else item_type_from_db if isinstance(item_type_from_db, str) else 'Movie'
             collection_type = "tvshows" if authoritative_type == 'Series' else "movies"

        fake_library_details = {
            "Name": coll['name'], "ServerId": real_server_id, "Id": mimicked_id,
            "Type": "CollectionFolder",
            "CollectionType": collection_type, "IsFolder": True, "ImageTags": image_tags,
        }
        return Response(json.dumps(fake_library_details), mimetype='application/json')
    except Exception as e:
        logger.error(f"è·å–ä¼ªé€ åº“è¯¦æƒ…æ—¶å‡ºé”™: {e}", exc_info=True)
        return "Internal Server Error", 500

def handle_get_mimicked_library_image(path):
    try:
        tag_with_timestamp = request.args.get('tag') or request.args.get('Tag')
        if not tag_with_timestamp: return "Bad Request", 400
        real_emby_collection_id = tag_with_timestamp.split('?')[0]
        base_url, _ = _get_real_emby_url_and_key()
        image_url = f"{base_url}/Items/{real_emby_collection_id}/Images/Primary"
        headers = {key: value for key, value in request.headers if key.lower() != 'host'}
        headers['Host'] = urlparse(base_url).netloc
        resp = requests.get(image_url, headers=headers, stream=True, params=request.args)
        excluded_headers = ['content-encoding', 'content-length', 'transfer-encoding', 'connection']
        response_headers = [(name, value) for name, value in resp.raw.headers.items() if name.lower() not in excluded_headers]
        return Response(resp.iter_content(chunk_size=8192), resp.status_code, response_headers)
    except Exception as e:
        return "Internal Proxy Error", 500

UNSUPPORTED_METADATA_ENDPOINTS = [
        # '/Items/Prefixes', # Emby ä¸æ”¯æŒæŒ‰å‰ç¼€è¿‡æ»¤è™šæ‹Ÿåº“
        '/Genres',         
        '/Studios',        
        '/Tags',           
        '/OfficialRatings',
        '/Years'           
    ]

def handle_mimicked_library_metadata_endpoint(path, mimicked_id, params):
    """
    å¤„ç†è™šæ‹Ÿåº“çš„å…ƒæ•°æ®è¯·æ±‚ã€‚
    """
    if any(path.endswith(endpoint) for endpoint in UNSUPPORTED_METADATA_ENDPOINTS):
        return Response(json.dumps([]), mimetype='application/json')

    try:
        real_db_id = from_mimicked_id(mimicked_id)
        collection_info = custom_collection_db.get_custom_collection_by_id(real_db_id)
        if not collection_info or not collection_info.get('emby_collection_id'):
            return Response(json.dumps([]), mimetype='application/json')

        real_emby_collection_id = collection_info.get('emby_collection_id')
        base_url, api_key = _get_real_emby_url_and_key()
        target_url = f"{base_url}/{path}"
        
        headers = {k: v for k, v in request.headers if k.lower() not in ['host']}
        headers['Host'] = urlparse(base_url).netloc
        
        new_params = params.copy()
        new_params['ParentId'] = real_emby_collection_id
        new_params['api_key'] = api_key
        
        resp = requests.get(target_url, headers=headers, params=new_params, timeout=15)
        resp.raise_for_status()
        
        return Response(resp.content, resp.status_code, content_type=resp.headers.get('Content-Type'))

    except Exception as e:
        logger.error(f"å¤„ç†è™šæ‹Ÿåº“å…ƒæ•°æ®è¯·æ±‚ '{path}' æ—¶å‡ºé”™: {e}", exc_info=True)
        return Response(json.dumps([]), mimetype='application/json')
    
def handle_get_mimicked_library_items(user_id, mimicked_id, params):
    """
    ã€V8 - å®æ—¶æ¶æ„ + å ä½æµ·æŠ¥é€‚é…ç‰ˆ + æ’åºä¿®å¤ã€‘
    æ”¯æŒï¼šå®æ—¶æƒé™è¿‡æ»¤ã€åŸç”Ÿæ’åºã€æ¦œå•å ä½ç¬¦ã€æ•°é‡é™åˆ¶
    """
    try:
        # 1. è·å–åˆé›†åŸºç¡€ä¿¡æ¯
        real_db_id = from_mimicked_id(mimicked_id)
        collection_info = custom_collection_db.get_custom_collection_by_id(real_db_id)
        if not collection_info:
            return Response(json.dumps({"Items": [], "TotalRecordCount": 0}), mimetype='application/json')

        definition = collection_info.get('definition_json') or {}
        if isinstance(definition, str):
            try: definition = json.loads(definition)
            except: definition = {}

        collection_type = collection_info.get('type')
        
        # 2. è·å–åˆ†é¡µå’Œæ’åºå‚æ•° (å˜é‡å®šä¹‰å¿…é¡»åœ¨æ­¤å¤„)
        emby_limit = int(params.get('Limit', 50))
        offset = int(params.get('StartIndex', 0))
        
        defined_limit = definition.get('limit')
        if defined_limit:
            defined_limit = int(defined_limit)
        
        # --- æ’åºä¼˜å…ˆçº§é€»è¾‘ ---
        req_sort_by = params.get('SortBy')
        req_sort_order = params.get('SortOrder')
        
        defined_sort_by = definition.get('default_sort_by')
        defined_sort_order = definition.get('default_sort_order')

        # é€»è¾‘ï¼šå¦‚æœDBå®šä¹‰äº†ä¸”ä¸æ˜¯noneï¼Œå¼ºåˆ¶åŠ«æŒï¼›å¦åˆ™ä½¿ç”¨å®¢æˆ·ç«¯è¯·æ±‚
        if defined_sort_by and defined_sort_by != 'none':
            # å¼ºåˆ¶åŠ«æŒæ¨¡å¼
            sort_by = defined_sort_by
            sort_order = defined_sort_order or 'Descending'
            is_native_mode = False
        else:
            # åŸç”Ÿ/å®¢æˆ·ç«¯æ¨¡å¼ (è®¾ç½®ä¸º NONE æ—¶)
            sort_by = req_sort_by or 'DateCreated'
            sort_order = req_sort_order or 'Descending'
            is_native_mode = True

        # æ ¸å¿ƒåˆ¤æ–­ï¼šæ˜¯å¦éœ€è¦ Emby åŸç”Ÿæ’åº
        # å½“ä½¿ç”¨åŸç”Ÿæ’åº(is_native_mode=True)æ—¶ï¼Œå¦‚æœæ’åºå­—æ®µä¸æ˜¯æ•°æ®åº“èƒ½å®Œç¾å¤„ç†çš„(å¦‚DateCreated)ï¼Œ
        # å¿…é¡»å¼ºåˆ¶èµ° Emby ä»£ç†æ’åºã€‚
        is_emby_proxy_sort_required = (
            collection_type in ['ai_recommendation', 'ai_recommendation_global'] or 
            'DateLastContentAdded' in sort_by or
            (is_native_mode and sort_by not in ['DateCreated', 'Random'])
        )

        # 3. å‡†å¤‡åŸºç¡€æŸ¥è¯¢å‚æ•°
        tmdb_ids_filter = None
        rules = definition.get('rules', [])
        logic = definition.get('logic', 'AND')
        item_types = definition.get('item_type', ['Movie'])
        target_library_ids = definition.get('target_library_ids', [])

        # 4. åˆ†æµå¤„ç†é€»è¾‘
        
        # --- åœºæ™¯ A: æ¦œå•ç±» (éœ€è¦å¤„ç†å ä½ç¬¦ + ä¸¥æ ¼æƒé™è¿‡æ»¤) ---
        if collection_type == 'list':
            show_placeholders = config_manager.APP_CONFIG.get(constants.CONFIG_OPTION_PROXY_SHOW_MISSING_PLACEHOLDERS, False)
            raw_list_json = collection_info.get('generated_media_info_json')
            raw_list = json.loads(raw_list_json) if isinstance(raw_list_json, str) else (raw_list_json or [])
            
            if raw_list:
                # 1. è·å–è¯¥æ¦œå•ä¸­æ‰€æœ‰æ¶‰åŠçš„ TMDb ID
                tmdb_ids_in_list = [str(i.get('tmdb_id')) for i in raw_list if i.get('tmdb_id')]
                
                # 2. ã€ç”¨æˆ·è§†å›¾ã€‘è·å–å½“å‰ç”¨æˆ·æœ‰æƒçœ‹åˆ°çš„é¡¹ç›®
                items_in_db, _ = queries_db.query_virtual_library_items(
                    rules=rules, logic=logic, user_id=user_id,
                    limit=2000, offset=0, 
                    sort_by='DateCreated', sort_order='Descending',
                    item_types=item_types, target_library_ids=target_library_ids,
                    tmdb_ids=tmdb_ids_in_list
                )
                
                # 3. ã€å…¨å±€è§†å›¾ã€‘è·å–Embyä¸­å®é™…å­˜åœ¨çš„é¡¹ç›®ï¼ˆå¿½ç•¥ç”¨æˆ·æƒé™ï¼Œä¼ å…¥ user_id=Noneï¼‰
                global_existing_items, _ = queries_db.query_virtual_library_items(
                    rules=rules, logic=logic, user_id=None, 
                    limit=2000, offset=0,
                    item_types=item_types, target_library_ids=target_library_ids,
                    tmdb_ids=tmdb_ids_in_list
                )

                # 4. å»ºç«‹æ˜ å°„è¡¨
                local_tmdb_map = {str(i['tmdb_id']): i['Id'] for i in items_in_db if i.get('tmdb_id')}
                local_emby_id_set = {str(i['Id']) for i in items_in_db}
                
                global_tmdb_set = {str(i['tmdb_id']) for i in global_existing_items if i.get('tmdb_id')}
                global_emby_id_set = {str(i['Id']) for i in global_existing_items}
                
                # 5. æ„é€ å®Œæ•´è§†å›¾åˆ—è¡¨
                full_view_list = []
                for raw_item in raw_list:
                    tid = str(raw_item.get('tmdb_id')) if raw_item.get('tmdb_id') else "None"
                    eid = str(raw_item.get('emby_id')) if raw_item.get('emby_id') else "None"

                    if (not tid or tid.lower() == "none") and (not eid or eid.lower() == "none"):
                        continue

                    if defined_limit and len(full_view_list) >= defined_limit:
                        break
                    
                    # åˆ†æ”¯ 1: ç”¨æˆ·æœ‰æƒæŸ¥çœ‹
                    if tid != "None" and tid in local_tmdb_map:
                        full_view_list.append({"is_missing": False, "id": local_tmdb_map[tid], "tmdb_id": tid})
                    elif eid != "None" and eid in local_emby_id_set:
                         full_view_list.append({"is_missing": False, "id": eid, "tmdb_id": tid})

                    # åˆ†æ”¯ 3: é¡¹ç›®å­˜åœ¨äºå…¨å±€åº“ï¼Œä½†ç”¨æˆ·æ— æƒæŸ¥çœ‹ -> ã€è·³è¿‡ï¼Œä¸æ˜¾ç¤ºå ä½ç¬¦ã€‘
                    elif (tid != "None" and tid in global_tmdb_set) or (eid != "None" and eid in global_emby_id_set):
                        continue 

                    # åˆ†æ”¯ 4: é¡¹ç›®ç¡®å®ç¼ºå¤± -> æ˜¾ç¤ºå ä½ç¬¦
                    elif tid != "None":
                        if show_placeholders:
                            full_view_list.append({"is_missing": True, "tmdb_id": tid})

                # 6. åˆ†é¡µ
                paged_part = full_view_list[offset : offset + emby_limit]
                reported_total_count = len(full_view_list)

                # 7. æ‰¹é‡è·å–è¯¦æƒ…
                real_eids = [x['id'] for x in paged_part if not x['is_missing']]
                missing_tids = [x['tmdb_id'] for x in paged_part if x['is_missing']]
                
                status_map = queries_db.get_missing_items_metadata(missing_tids)
                
                base_url, api_key = _get_real_emby_url_and_key()
                full_fields = "PrimaryImageAspectRatio,ImageTags,HasPrimaryImage,ProviderIds,UserData,Name,ProductionYear,CommunityRating,Type"
                emby_details = _fetch_items_in_chunks(base_url, api_key, user_id, real_eids, full_fields)
                emby_map = {item['Id']: item for item in emby_details}

                final_items = []
                for entry in paged_part:
                    if not entry['is_missing']:
                        eid = entry['id']
                        if eid in emby_map:
                            final_items.append(emby_map[eid])
                    else:
                        # å ä½ç¬¦æ„é€ é€»è¾‘
                        tid = entry['tmdb_id']
                        meta = status_map.get(tid, {})
                        status = meta.get('subscription_status', 'WANTED')
                        db_item_type = meta.get('item_type', 'Movie')
                        
                        placeholder = {
                            "Name": meta.get('title', 'æœªçŸ¥å†…å®¹'),
                            "ServerId": extensions.EMBY_SERVER_ID,
                            "Id": to_missing_item_id(tid),
                            "Type": db_item_type,
                            "ProductionYear": int(meta.get('release_year')) if meta.get('release_year') else None,
                            "ImageTags": {"Primary": f"missing_{status}_{tid}"},
                            "HasPrimaryImage": True,
                            "PrimaryImageAspectRatio": 0.6666666666666666,
                            "UserData": {"PlaybackPositionTicks": 0, "PlayCount": 0, "IsFavorite": False, "Played": False},
                            "ProviderIds": {"Tmdb": tid},
                            "LocationType": "Virtual"
                        }
                        r_date = meta.get('release_date')
                        r_year = meta.get('release_year')
                        if r_date:
                            try:
                                if hasattr(r_date, 'strftime'):
                                    placeholder["PremiereDate"] = r_date.strftime('%Y-%m-%dT00:00:00.0000000Z')
                                else:
                                    placeholder["PremiereDate"] = str(r_date)
                            except: pass
                        if "PremiereDate" not in placeholder and r_year:
                            placeholder["PremiereDate"] = f"{r_year}-01-01T00:00:00.0000000Z"
                        if db_item_type == 'Series':
                            placeholder["Status"] = "Released"

                        final_items.append(placeholder)
                
                return Response(json.dumps({"Items": final_items, "TotalRecordCount": reported_total_count}), mimetype='application/json')

        # --- åœºæ™¯ B: ç­›é€‰/æ¨èç±» (ä¿®å¤ç°è‰²å ä½ç¬¦) ---
        else:
            if collection_type in ['ai_recommendation', 'ai_recommendation_global']:
                api_key = config_manager.APP_CONFIG.get("tmdb_api_key")
                if api_key:
                    engine = RecommendationEngine(api_key)
                    if collection_type == 'ai_recommendation':
                        candidate_pool = engine.generate_user_vector(user_id, limit=300, allowed_types=item_types)
                    else:
                        candidate_pool = engine.generate_global_vector(limit=300, allowed_types=item_types)
                    tmdb_ids_filter = [str(i['id']) for i in candidate_pool]

            # æ‰§è¡Œ SQL æŸ¥è¯¢
            sql_limit = defined_limit if is_emby_proxy_sort_required and defined_limit else 5000 if is_emby_proxy_sort_required else min(emby_limit, defined_limit - offset) if (defined_limit and defined_limit > offset) else emby_limit
            sql_offset = 0 if is_emby_proxy_sort_required else offset
            sql_sort = 'Random' if 'ai_recommendation' in collection_type else sort_by

            items, total_count = queries_db.query_virtual_library_items(
                rules=rules, logic=logic, user_id=user_id,
                limit=sql_limit, offset=sql_offset,
                sort_by=sql_sort, sort_order=sort_order,
                item_types=item_types, target_library_ids=target_library_ids,
                tmdb_ids=tmdb_ids_filter
            )

            reported_total_count = min(total_count, defined_limit) if defined_limit else total_count

            if not items:
                return Response(json.dumps({"Items": [], "TotalRecordCount": reported_total_count}), mimetype='application/json')

            final_emby_ids = [i['Id'] for i in items]
            full_fields = "PrimaryImageAspectRatio,ImageTags,HasPrimaryImage,ProviderIds,UserData,Name,ProductionYear,CommunityRating,DateCreated,PremiereDate,Type,RecursiveItemCount,SortName,ChildCount,BasicSyncInfo"

            if is_emby_proxy_sort_required:
                # ä»£ç†æ’åºæ¨¡å¼ï¼šå°†æ‰€æœ‰ ID äº¤ç»™ Emby (æˆ–å†…å­˜) è¿›è¡Œæ’åºå’Œåˆ†é¡µ
                sorted_data = _fetch_sorted_items_via_emby_proxy(
                    user_id, final_emby_ids, sort_by, sort_order, emby_limit, offset, full_fields, reported_total_count
                )
                return Response(json.dumps(sorted_data), mimetype='application/json')
            else:
                # SQL æ’åºæ¨¡å¼ï¼šç›´æ¥è·å–è¯¦æƒ…
                base_url, api_key = _get_real_emby_url_and_key()
                items_from_emby = _fetch_items_in_chunks(base_url, api_key, user_id, final_emby_ids, full_fields)
                items_map = {item['Id']: item for item in items_from_emby}
                
                # è¿‡æ»¤æ‰ Emby å®é™…æ²¡æœ‰è¿”å›çš„é¡¹ç›®
                final_items = [items_map[eid] for eid in final_emby_ids if eid in items_map]
                
                # --- ä¿®å¤å¼€å§‹ ---
                expected_count = len(final_emby_ids)
                actual_count = len(final_items)
                
                if actual_count < expected_count:
                    diff = expected_count - actual_count
                    # 1. å…ˆæ‰§è¡ŒåŸæœ¬çš„å‡æ³•ä¿®æ­£
                    reported_total_count = max(0, reported_total_count - diff)
                    logger.debug(f"æ£€æµ‹åˆ°æƒé™è¿‡æ»¤å¯¼è‡´çš„æ•°é‡å·®å¼‚: SQL={expected_count}, Emby={actual_count}. åˆæ­¥ä¿®æ­£ TotalRecordCount ä¸º {reported_total_count}")

                    # 2. ã€æ–°å¢ã€‘å°åº•ä¿é™©é€»è¾‘
                    if reported_total_count <= emby_limit:
                        reported_total_count = actual_count
                        logger.debug(f"ä¿®æ­£åçš„æ€»æ•°å°äºåˆ†é¡µé™åˆ¶ï¼Œå¼ºåˆ¶å¯¹é½ TotalRecordCount = {actual_count} ä»¥æ¶ˆé™¤ç°å—")

                return Response(json.dumps({"Items": final_items, "TotalRecordCount": reported_total_count}), mimetype='application/json')

    except Exception as e:
        logger.error(f"å¤„ç†è™šæ‹Ÿåº“ '{mimicked_id}' å¤±è´¥: {e}", exc_info=True)
        return Response(json.dumps({"Items": [], "TotalRecordCount": 0}), mimetype='application/json')

def handle_get_latest_items(user_id, params):
    """
    è·å–æœ€æ–°é¡¹ç›®ã€‚
    åˆ©ç”¨ queries_db çš„æ’åºèƒ½åŠ›ï¼Œå¿«é€Ÿè¿”å›ç»“æœã€‚
    ã€ä¿®å¤ç‰ˆã€‘å¢åŠ å¯¹æ¦œå•(list)å’ŒAIåˆé›†çš„ç±»å‹åˆ¤æ–­ï¼Œé˜²æ­¢æ— è§„åˆ™åˆé›†æ³„éœ²å…¨å±€æœ€æ–°æ•°æ®ã€‚
    """
    try:
        base_url, api_key = _get_real_emby_url_and_key()
        virtual_library_id = params.get('ParentId') or params.get('customViewId')
        limit = int(params.get('Limit', 20))
        fields = params.get('Fields', "PrimaryImageAspectRatio,BasicSyncInfo,DateCreated,UserData")

        # --- è¾…åŠ©å‡½æ•°ï¼šè·å–åˆé›†çš„è¿‡æ»¤ ID ---
        def get_collection_filter_ids(coll_data):
            c_type = coll_data.get('type')
            # 1. æ¦œå•ç±»ï¼šå¿…é¡»é™åˆ¶åœ¨æ¦œå•åŒ…å«çš„ TMDb ID èŒƒå›´å†…
            if c_type == 'list':
                raw_json = coll_data.get('generated_media_info_json')
                raw_list = json.loads(raw_json) if isinstance(raw_json, str) else (raw_json or [])
                return [str(i.get('tmdb_id')) for i in raw_list if i.get('tmdb_id')]
            # 2. AI æ¨èç±»ï¼šæš‚ä¸æ”¯æŒâ€œæœ€æ–°â€è§†å›¾ (å› ä¸ºæ˜¯åŠ¨æ€ç”Ÿæˆçš„)ï¼Œè¿”å›ä¸€ä¸ªä¸å­˜åœ¨çš„ ID é˜²æ­¢æ³„éœ²
            elif c_type in ['ai_recommendation', 'ai_recommendation_global']:
                return ["-1"] 
            # 3. è§„åˆ™ç±»ï¼šè¿”å› Noneï¼Œè¡¨ç¤ºä¸é™åˆ¶ IDï¼Œåªèµ° Rules
            return None

        # åœºæ™¯ä¸€ï¼šå•ä¸ªè™šæ‹Ÿåº“çš„æœ€æ–°
        if virtual_library_id and is_mimicked_id(virtual_library_id):
            real_db_id = from_mimicked_id(virtual_library_id)
            collection_info = custom_collection_db.get_custom_collection_by_id(real_db_id)
            if not collection_info: return Response(json.dumps([]), mimetype='application/json')

            definition = collection_info.get('definition_json') or {}
            if isinstance(definition, str): definition = json.loads(definition)
            
            if not definition.get('show_in_latest', True):
                return Response(json.dumps([]), mimetype='application/json')

            # --- ä¿®å¤æ ¸å¿ƒï¼šè·å– ID è¿‡æ»¤å™¨ ---
            tmdb_ids_filter = get_collection_filter_ids(collection_info)
            # å¦‚æœæ˜¯ AI åˆé›†è¿”å›äº† ["-1"]ï¼Œæˆ–è€…æ¦œå•ä¸ºç©ºï¼Œç›´æ¥è¿”å›ç©ºç»“æœ
            if tmdb_ids_filter is not None and (len(tmdb_ids_filter) == 0 or tmdb_ids_filter == ["-1"]):
                 return Response(json.dumps([]), mimetype='application/json')

            # ç¡®å®šæ’åº
            item_types = definition.get('item_type', ['Movie'])
            is_series_only = isinstance(item_types, list) and len(item_types) == 1 and item_types[0] == 'Series'
            sort_by = 'DateLastContentAdded,DateCreated' if is_series_only else 'DateCreated'

            # SQL è¿‡æ»¤æƒé™å’Œè§„åˆ™
            items, total_count = queries_db.query_virtual_library_items(
                rules=definition.get('rules', []), logic=definition.get('logic', 'AND'),
                user_id=user_id, limit=500, offset=0,
                sort_by='DateCreated', sort_order='Descending',
                item_types=item_types, target_library_ids=definition.get('target_library_ids', []),
                tmdb_ids=tmdb_ids_filter  # <--- ä¼ å…¥ TMDb ID é™åˆ¶
            )
            
            if not items: return Response(json.dumps([]), mimetype='application/json')
            final_emby_ids = [i['Id'] for i in items]

            # ç»Ÿä¸€è°ƒç”¨ä»£ç†æ’åº
            sorted_data = _fetch_sorted_items_via_emby_proxy(
                user_id, final_emby_ids, sort_by, 'Descending', limit, 0, fields, len(final_emby_ids)
            )
            return Response(json.dumps(sorted_data.get("Items", [])), mimetype='application/json')

        # åœºæ™¯äºŒï¼šå…¨å±€æœ€æ–° (æ‰€æœ‰å¯è§åˆé›†çš„èšåˆ)
        elif not virtual_library_id:
            # è·å–æ‰€æœ‰å¼€å¯äº†â€œæ˜¾ç¤ºæœ€æ–°â€çš„åˆé›† ID
            included_collection_ids = custom_collection_db.get_active_collection_ids_for_latest_view()
            if not included_collection_ids:
                return Response(json.dumps([]), mimetype='application/json')
            
            all_latest = []
            for coll_id in included_collection_ids:
                coll = custom_collection_db.get_custom_collection_by_id(coll_id)
                if not coll: continue
                
                # æ£€æŸ¥æƒé™
                allowed_users = coll.get('allowed_user_ids')
                if allowed_users and user_id not in allowed_users: continue

                # --- ä¿®å¤æ ¸å¿ƒï¼šè·å– ID è¿‡æ»¤å™¨ ---
                tmdb_ids_filter = get_collection_filter_ids(coll)
                if tmdb_ids_filter is not None and (len(tmdb_ids_filter) == 0 or tmdb_ids_filter == ["-1"]):
                    continue

                definition = coll.get('definition_json')
                items, _ = queries_db.query_virtual_library_items(
                    rules=definition.get('rules', []),
                    logic=definition.get('logic', 'AND'),
                    user_id=user_id,
                    limit=limit, 
                    offset=0,
                    sort_by='DateCreated',
                    sort_order='Descending',
                    item_types=definition.get('item_type', ['Movie']),
                    target_library_ids=definition.get('target_library_ids', []),
                    tmdb_ids=tmdb_ids_filter # <--- ä¼ å…¥ TMDb ID é™åˆ¶
                )
                all_latest.extend(items)
            
            # å»é‡å¹¶è·å–è¯¦æƒ…
            unique_ids = list({i['Id'] for i in all_latest})
            if not unique_ids: return Response(json.dumps([]), mimetype='application/json')
            
            # æ‰¹é‡è·å–è¯¦æƒ…
            items_details = _fetch_items_in_chunks(base_url, api_key, user_id, unique_ids, "DateCreated")
            # å†…å­˜æ’åº
            items_details.sort(key=lambda x: x.get('DateCreated', ''), reverse=True)
            # æˆªå–
            latest_ids = [i['Id'] for i in items_details[:limit]]

        else:
            # åŸç”Ÿåº“è¯·æ±‚ï¼Œç›´æ¥è½¬å‘
            target_url = f"{base_url}/{request.path.lstrip('/')}"
            forward_headers = {k: v for k, v in request.headers if k.lower() not in ['host', 'accept-encoding']}
            forward_headers['Host'] = urlparse(base_url).netloc
            forward_params = request.args.copy()
            forward_params['api_key'] = api_key
            resp = requests.request(method=request.method, url=target_url, headers=forward_headers, params=forward_params, data=request.get_data(), stream=True, timeout=30.0)
            excluded_resp_headers = ['content-encoding', 'content-length', 'transfer-encoding', 'connection']
            response_headers = [(name, value) for name, value in resp.raw.headers.items() if name.lower() not in excluded_resp_headers]
            return Response(resp.iter_content(chunk_size=8192), resp.status_code, response_headers)

        if not latest_ids:
            return Response(json.dumps([]), mimetype='application/json')

        # è·å–æœ€ç»ˆè¯¦æƒ…
        items_from_emby = _fetch_items_in_chunks(base_url, api_key, user_id, latest_ids, fields)
        items_map = {item['Id']: item for item in items_from_emby}
        final_items = [items_map[id] for id in latest_ids if id in items_map]
        
        return Response(json.dumps(final_items), mimetype='application/json')

    except Exception as e:
        logger.error(f"  âœ å¤„ç†æœ€æ–°åª’ä½“æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}", exc_info=True)
        return Response(json.dumps([]), mimetype='application/json')

proxy_app = Flask(__name__)

@proxy_app.route('/', defaults={'path': ''})
@proxy_app.route('/<path:path>', methods=['GET', 'POST', 'PUT', 'DELETE', 'HEAD', 'OPTIONS'])
def proxy_all(path):
    # --- 1. WebSocket ä»£ç†é€»è¾‘ ---
    if 'Upgrade' in request.headers and request.headers.get('Upgrade', '').lower() == 'websocket':
        ws_client = request.environ.get('wsgi.websocket')
        if not ws_client: return "WebSocket upgrade failed", 400

        try:
            base_url, _ = _get_real_emby_url_and_key()
            parsed_url = urlparse(base_url)
            ws_scheme = 'wss' if parsed_url.scheme == 'https' else 'ws'
            target_ws_url = urlunparse((ws_scheme, parsed_url.netloc, f'/{path}', '', request.query_string.decode(), ''))
            
            headers_to_server = {k: v for k, v in request.headers.items() if k.lower() not in ['host', 'upgrade', 'connection', 'sec-websocket-key', 'sec-websocket-version']}
            ws_server = create_connection(target_ws_url, header=headers_to_server, timeout=10)

            def forward_to_server():
                try:
                    while not ws_client.closed and ws_server.connected:
                        message = ws_client.receive()
                        if message is not None: ws_server.send(message)
                        else: break
                except: pass
                finally: ws_server.close()

            def forward_to_client():
                try:
                    while ws_server.connected and not ws_client.closed:
                        message = ws_server.recv()
                        if message is not None: ws_client.send(message)
                        else: break
                except: pass
                finally: ws_client.close()
            
            greenlets = [spawn(forward_to_server), spawn(forward_to_client)]
            joinall(greenlets)
            
            # WebSocket ç»“æŸåè¿”å›ç©ºå“åº”
            return Response()

        except Exception as e:
            logger.error(f"WebSocket ä»£ç†é”™è¯¯: {e}")
            return Response(status=500)

    # --- 2. HTTP ä»£ç†é€»è¾‘ ---
    try:
        full_path = f'/{path}'

        # ====================================================================
        # â˜…â˜…â˜… ç»ˆææ‹¦æˆª A+ï¼šå…¨ç›˜æ¥ç®¡è§†é¢‘æµ 302 ç›´é“¾è§£æ (å¤åˆ» CMS æ ¸å¿ƒé€»è¾‘) â˜…â˜…â˜…
        # å½“å®¢æˆ·ç«¯è¯·æ±‚è§†é¢‘æµæ—¶ï¼Œåä»£å±‚ä¸»åŠ¨æŸ¥è¯¢æ–‡ä»¶è·¯å¾„å¹¶å‰¥ç¦» 115 ç›´é“¾ï¼
        # ====================================================================
        if '/videos/' in full_path and ('/stream' in full_path or '/original' in full_path or 'PlaybackInfo' in full_path):
            try:
                # 1. æŠ“å–è¯·æ±‚æµçš„é¡¹ç›® ID
                item_id_match = re.search(r'/Items/([^/]+)/', full_path) or re.search(r'/videos/([^/]+)/', full_path)
                if item_id_match:
                    item_id = item_id_match.group(1)
                    base_url, api_key = _get_real_emby_url_and_key()
                    user_id = request.args.get('UserId') or request.args.get('api_key') or "admin"
                    
                    # 2. å‘å±€åŸŸç½‘å†…çš„ Emby æ‰“å¬è¿™ä¸ªè§†é¢‘çš„å®é™…ç‰©ç†è·¯å¾„
                    details_url = f"{base_url}/emby/Items/{item_id}"
                    resp = requests.get(details_url, params={'api_key': api_key, 'UserId': user_id}, timeout=3)
                    
                    if resp.status_code == 200:
                        item_data = resp.json()
                        file_path = item_data.get('Path', '')
                        
                        # 3. æ ¸å¿ƒåˆ¤æ–­ï¼šæ˜¯ .strm æ–‡ä»¶å—ï¼Ÿæœ¬åœ°èƒ½è¯»åˆ°å—ï¼Ÿ
                        if file_path and file_path.endswith('.strm') and os.path.exists(file_path):
                            with open(file_path, 'r', encoding='utf-8') as f:
                                strm_content = f.read().strip()
                                
                            # 4. ä»å±€åŸŸç½‘é“¾æ¥ä¸­æå–æå–ç  (pick_code)
                            # strm æ ¼å¼: http://192.168.X.X:5257/api/p115/play/abc1234
                            if '/api/p115/play/' in strm_content:
                                pick_code = strm_content.split('/play/')[-1].split('?')[0].strip()
                                
                                # 5. â˜… å†³æˆ˜ 115ï¼šè·å–ç›´é“¾å¹¶ç›´æ¥è¿”å› 302ï¼
                                # æ³¨æ„ï¼šå¿…é¡»ä½¿ç”¨å½“å‰å‘èµ·è¯·æ±‚çš„å®¢æˆ·ç«¯çš„ User-Agentï¼Œå¦åˆ™ 115 CDN æŠ¥ 403
                                player_ua = request.headers.get('User-Agent', 'Mozilla/5.0')
                                client_ip = request.headers.get('X-Real-IP', request.remote_addr)
                                
                                # è°ƒç”¨å†…å­˜ç¼“å­˜ç‰ˆçš„ç›´é“¾è·å–å™¨
                                real_url = _get_cached_115_url(pick_code, player_ua, client_ip)
                                
                                if real_url:
                                    logger.info(f"  ğŸ¬ [åä»£åŠ«æŒ] æˆåŠŸæ‹¦æˆª Emby æµè¯·æ±‚ï¼Œä¸‹å‘ 115 CDN ç›´é“¾ï¼")
                                    from flask import redirect
                                    
                                    # å¦‚æœæ˜¯ PlaybackInfo è¯·æ±‚ï¼Œè¿”å›ç»™å®¢æˆ·ç«¯çš„ Path åº”è¯¥æ˜¯åŸå§‹çš„ strm åœ°å€æˆ–è€…å†æ¬¡ç»è¿‡æˆ‘ä»¬ä»£ç†çš„åœ°å€
                                    if 'PlaybackInfo' in full_path:
                                        fake_info = {
                                            "MediaSources": [{
                                                "Id": item_id,
                                                "Path": real_url,
                                                "Protocol": "Http",
                                                "IsInfiniteStream": False,
                                                "SupportsDirectPlay": True,
                                                "SupportsDirectStream": True,
                                                "SupportsTranscoding": False,
                                                "Container": "mp4", # æ˜¾å¼å‘ŠçŸ¥å®¹å™¨æ ¼å¼
                                                "ReadAtNativeFramerate": False,
                                                "Type": "Default"
                                            }],
                                            "PlaySessionId": f"etk_proxy_{int(time.time())}"
                                        }
                                        # å¼ºåˆ¶æŒ‡å®šå†…å®¹é•¿åº¦å’Œç¼–ç ï¼Œé˜²æ­¢æŸäº›å®¢æˆ·ç«¯è§£æ 500
                                        json_data = json.dumps(fake_info)
                                        return Response(
                                            json_data, 
                                            mimetype='application/json',
                                            headers={'Content-Length': str(len(json_data))}
                                        )
                                    
                                    # çœŸæ­£çš„è§†é¢‘æµè¯·æ±‚ï¼Œç›´æ¥ 302 ç”©å‡ºå»
                                    return redirect(real_url, code=302)
            except Exception as e:
                logger.error(f"  âŒ åä»£æ‹¦æˆªè§£æç›´é“¾å‡ºé”™ï¼Œå›é€€åŸç”Ÿå¤„ç†: {e}")

        # --- æ‹¦æˆª A: è™šæ‹Ÿé¡¹ç›®æµ·æŠ¥å›¾ç‰‡ ---
        if path.startswith('emby/Items/') and '/Images/Primary' in path:
            item_id = path.split('/')[2]
            if is_missing_item_id(item_id):
                combined_id = parse_missing_item_id(item_id)
                real_tmdb_id = combined_id.split('_S_')[0] if '_S_' in combined_id else combined_id
                meta = queries_db.get_best_metadata_by_tmdb_id(real_tmdb_id)
                db_status = meta.get('subscription_status', 'WANTED')
                current_status = db_status if db_status in ['WANTED', 'SUBSCRIBED', 'PENDING_RELEASE', 'PAUSED', 'IGNORED'] else 'WANTED'
                
                from handler.poster_generator import get_missing_poster
                img_file_path = get_missing_poster(
                    tmdb_id=real_tmdb_id, 
                    status=current_status,
                    poster_path=meta.get('poster_path')
                )
                
                if img_file_path and os.path.exists(img_file_path):
                    resp = send_file(img_file_path, mimetype='image/jpeg')
                    resp.headers['Cache-Control'] = 'no-cache, no-store, must-revalidate'
                    return resp

        # --- æ‹¦æˆª B: è§†å›¾åˆ—è¡¨ (Views) ---
        if path.endswith('/Views') and path.startswith('emby/Users/'):
            return handle_get_views()

        # --- æ‹¦æˆª C: æœ€æ–°é¡¹ç›® (Latest) ---
        if path.endswith('/Items/Latest'):
            user_id_match = re.search(r'/emby/Users/([^/]+)/', full_path)
            if user_id_match:
                return handle_get_latest_items(user_id_match.group(1), request.args)

        # --- æ‹¦æˆª D: è™šæ‹Ÿåº“è¯¦æƒ… ---
        details_match = MIMICKED_ITEM_DETAILS_RE.search(full_path)
        if details_match:
            user_id = details_match.group(1)
            mimicked_id = details_match.group(2)
            return handle_get_mimicked_library_details(user_id, mimicked_id)

        # --- æ‹¦æˆª E: è™šæ‹Ÿåº“å›¾ç‰‡ ---
        if path.startswith('emby/Items/') and '/Images/' in path:
            item_id = path.split('/')[2]
            if is_mimicked_id(item_id):
                return handle_get_mimicked_library_image(path)
        
        # --- æ‹¦æˆª F: è™šæ‹Ÿåº“å†…å®¹æµè§ˆ (Items) ---
        parent_id = request.args.get("ParentId")
        if parent_id and is_mimicked_id(parent_id):
            # å¤„ç†å…ƒæ•°æ®è¯·æ±‚
            if any(path.endswith(endpoint) for endpoint in UNSUPPORTED_METADATA_ENDPOINTS + ['/Items/Prefixes', '/Genres', '/Studios', '/Tags', '/OfficialRatings', '/Years']):
                return handle_mimicked_library_metadata_endpoint(path, parent_id, request.args)
            
            # å¤„ç†å†…å®¹åˆ—è¡¨è¯·æ±‚
            user_id_match = re.search(r'emby/Users/([^/]+)/Items', path)
            if user_id_match:
                user_id = user_id_match.group(1)
                return handle_get_mimicked_library_items(user_id, parent_id, request.args)

        # å…œåº•é€»è¾‘
        base_url, api_key = _get_real_emby_url_and_key()
        target_url = f"{base_url}/{path.lstrip('/')}"
        
        forward_headers = {k: v for k, v in request.headers if k.lower() not in ['host', 'accept-encoding']}
        forward_headers['Host'] = urlparse(base_url).netloc
        
        forward_params = request.args.copy()
        forward_params['api_key'] = api_key
        
        resp = requests.request(
            method=request.method,
            url=target_url,
            headers=forward_headers,
            params=forward_params,
            data=request.get_data(),
            stream=True,
            timeout=30.0
        )
        
        excluded_resp_headers = ['content-encoding', 'content-length', 'transfer-encoding', 'connection']
        response_headers = [(name, value) for name, value in resp.raw.headers.items() if name.lower() not in excluded_resp_headers]
        
        return Response(resp.iter_content(chunk_size=8192), resp.status_code, response_headers)
        
    except Exception as e:
        logger.error(f"[PROXY] HTTP ä»£ç†æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}", exc_info=True)
        return "Internal Server Error", 500