# core_processor.py

import os
import json
import time
import re
import random
from typing import Dict, List, Optional, Any, Tuple
from collections import defaultdict
import threading
from datetime import datetime, timezone
import time as time_module
import psycopg2
# ç¡®ä¿æ‰€æœ‰ä¾èµ–éƒ½å·²æ­£ç¡®å¯¼å…¥
from handler.custom_collection import RecommendationEngine
import config_manager
from database.connection import get_db_connection
from database import media_db, maintenance_db, settings_db
import handler.emby as emby
import handler.tmdb as tmdb
from tasks.helpers import parse_full_asset_details, calculate_ancestor_ids, construct_metadata_payload, translate_tmdb_metadata_recursively
import utils
import constants
import logging
import actor_utils
from database.actor_db import ActorDBManager
from database.log_db import LogDBManager
from database.connection import get_db_connection as get_central_db_connection
from cachetools import TTLCache
from ai_translator import AITranslator
from watchlist_processor import WatchlistProcessor
from handler.douban import DoubanApi

logger = logging.getLogger(__name__)
try:
    from handler.douban import DoubanApi
    DOUBAN_API_AVAILABLE = True
except ImportError:
    DOUBAN_API_AVAILABLE = False
    class DoubanApi:
        def __init__(self, *args, **kwargs): pass
        def get_acting(self, *args, **kwargs): return {}
        def close(self): pass

def extract_tag_names(item_data):
    """
    å…¼å®¹æ–°æ—§ç‰ˆ Emby API æå–æ ‡ç­¾åã€‚
    """
    tags_set = set()
    # 1. TagItems
    tag_items = item_data.get('TagItems')
    if isinstance(tag_items, list):
        for t in tag_items:
            if isinstance(t, dict):
                name = t.get('Name')
                if name: tags_set.add(name)
            elif isinstance(t, str) and t:
                tags_set.add(t)
    # 2. Tags
    tags = item_data.get('Tags')
    if isinstance(tags, list):
        for t in tags:
            if t: tags_set.add(str(t))
    return list(tags_set)

def _read_local_json(file_path: str) -> Optional[Dict[str, Any]]:
    if not os.path.exists(file_path):
        logger.warning(f"æœ¬åœ°å…ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return None
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        logger.error(f"è¯»å–æœ¬åœ°JSONæ–‡ä»¶å¤±è´¥: {file_path}, é”™è¯¯: {e}")
        return None

def _aggregate_series_cast_from_tmdb_data(series_data: Dict[str, Any], all_episodes_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    ã€æ–°ã€‘ä»å†…å­˜ä¸­çš„TMDBæ•°æ®èšåˆä¸€ä¸ªå‰§é›†çš„æ‰€æœ‰æ¼”å‘˜ã€‚
    """
    logger.debug(f"ã€æ¼”å‘˜èšåˆã€‘å¼€å§‹ä¸º '{series_data.get('name')}' ä»å†…å­˜ä¸­çš„TMDBæ•°æ®èšåˆæ¼”å‘˜...")
    aggregated_cast_map = {}

    # 1. ä¼˜å…ˆå¤„ç†ä¸»å‰§é›†çš„æ¼”å‘˜åˆ—è¡¨
    main_cast = series_data.get("credits", {}).get("cast", [])
    for actor in main_cast:
        actor_id = actor.get("id")
        if actor_id:
            aggregated_cast_map[actor_id] = actor
    logger.debug(f"  âœ ä»ä¸»å‰§é›†æ•°æ®ä¸­åŠ è½½äº† {len(aggregated_cast_map)} ä½ä¸»æ¼”å‘˜ã€‚")

    # 2. èšåˆæ‰€æœ‰åˆ†é›†çš„æ¼”å‘˜å’Œå®¢ä¸²æ¼”å‘˜
    for episode_data in all_episodes_data:
        credits_data = episode_data.get("credits", {})
        actors_to_process = credits_data.get("cast", []) + credits_data.get("guest_stars", [])
        
        for actor in actors_to_process:
            actor_id = actor.get("id")
            if actor_id and actor_id not in aggregated_cast_map:
                if 'order' not in actor:
                    actor['order'] = 999  # ä¸ºå®¢ä¸²æ¼”å‘˜è®¾ç½®é«˜orderå€¼
                aggregated_cast_map[actor_id] = actor

    full_aggregated_cast = list(aggregated_cast_map.values())
    full_aggregated_cast.sort(key=lambda x: x.get('order', 999))
    
    logger.info(f"  âœ å…±ä¸º '{series_data.get('name')}' èšåˆäº† {len(full_aggregated_cast)} ä½ç‹¬ç«‹æ¼”å‘˜ã€‚")
    return full_aggregated_cast
class MediaProcessor:
    def __init__(self, config: Dict[str, Any]):
        # â˜…â˜…â˜… ç„¶åï¼Œä»è¿™ä¸ª config å­—å…¸é‡Œï¼Œè§£æå‡ºæ‰€æœ‰éœ€è¦çš„å±æ€§ â˜…â˜…â˜…
        self.config = config

        # åˆå§‹åŒ–æˆ‘ä»¬çš„æ•°æ®åº“ç®¡ç†å‘˜
        self.actor_db_manager = ActorDBManager()
        self.log_db_manager = LogDBManager()

        # ä» config ä¸­è·å–æ‰€æœ‰å…¶ä»–é…ç½®
        self.douban_api = None
        if getattr(constants, 'DOUBAN_API_AVAILABLE', False):
            try:
                # --- âœ¨âœ¨âœ¨ æ ¸å¿ƒä¿®æ”¹åŒºåŸŸ START âœ¨âœ¨âœ¨ ---

                # 1. ä»é…ç½®ä¸­è·å–å†·å´æ—¶é—´ 
                douban_cooldown = self.config.get(constants.CONFIG_OPTION_DOUBAN_DEFAULT_COOLDOWN, 2.0)
                
                # 2. ä»é…ç½®ä¸­è·å– Cookieï¼Œä½¿ç”¨æˆ‘ä»¬åˆšåˆšåœ¨ constants.py ä¸­å®šä¹‰çš„å¸¸é‡
                douban_cookie = self.config.get(constants.CONFIG_OPTION_DOUBAN_COOKIE, "")
                
                # 3. æ·»åŠ ä¸€ä¸ªæ—¥å¿—ï¼Œæ–¹ä¾¿è°ƒè¯•
                if not douban_cookie:
                    logger.debug(f"é…ç½®æ–‡ä»¶ä¸­æœªæ‰¾åˆ°æˆ–æœªè®¾ç½® '{constants.CONFIG_OPTION_DOUBAN_COOKIE}'ã€‚å¦‚æœè±†ç“£APIè¿”å›'need_login'é”™è¯¯ï¼Œè¯·é…ç½®è±†ç“£cookieã€‚")
                else:
                    logger.debug("å·²ä»é…ç½®ä¸­åŠ è½½è±†ç“£ Cookieã€‚")

                # 4. å°†æ‰€æœ‰å‚æ•°ä¼ é€’ç»™ DoubanApi çš„æ„é€ å‡½æ•°
                self.douban_api = DoubanApi(
                    cooldown_seconds=douban_cooldown,
                    user_cookie=douban_cookie  # <--- å°† cookie ä¼ è¿›å»
                )
                logger.trace("DoubanApi å®ä¾‹å·²åœ¨ MediaProcessorAPI ä¸­åˆ›å»ºã€‚")
                
                # --- âœ¨âœ¨âœ¨ æ ¸å¿ƒä¿®æ”¹åŒºåŸŸ END âœ¨âœ¨âœ¨ ---

            except Exception as e:
                logger.error(f"MediaProcessorAPI åˆå§‹åŒ– DoubanApi å¤±è´¥: {e}", exc_info=True)
        else:
            logger.warning("DoubanApi å¸¸é‡æŒ‡ç¤ºä¸å¯ç”¨ï¼Œå°†ä¸ä½¿ç”¨è±†ç“£åŠŸèƒ½ã€‚")
        self.emby_url = self.config.get("emby_server_url")
        self.emby_api_key = self.config.get("emby_api_key")
        self.emby_user_id = self.config.get("emby_user_id")
        self.tmdb_api_key = self.config.get("tmdb_api_key", "")
        self.local_data_path = self.config.get("local_data_path", "").strip()

        self.ai_enabled = any([
            self.config.get("ai_translate_actor_role", False),
            self.config.get(constants.CONFIG_OPTION_AI_TRANSLATE_TITLE, False),    
            self.config.get(constants.CONFIG_OPTION_AI_TRANSLATE_OVERVIEW, False), 
            self.config.get("ai_translate_episode_overview", False),
            self.config.get("ai_vector", False),
        ])
        self.ai_translator = AITranslator(self.config) if self.ai_enabled else None
        
        self._stop_event = threading.Event()
        self.processed_items_cache = self._load_processed_log_from_db()
        self.manual_edit_cache = TTLCache(maxsize=10, ttl=600)
        self._global_lib_guid_map = {}
        self._last_lib_map_update = 0
        logger.trace("æ ¸å¿ƒå¤„ç†å™¨åˆå§‹åŒ–å®Œæˆã€‚")

    # --- [ä¼˜åŒ–ç‰ˆ] å®æ—¶ç›‘æ§æ–‡ä»¶é€»è¾‘ (å¢åŠ ç¼“å­˜è·³è¿‡ & æ”¯æŒæ‰¹é‡å»¶è¿Ÿåˆ·æ–°) ---
    def process_file_actively(self, file_path: str, skip_refresh: bool = False) -> Optional[str]:
        """
        å®æ—¶ç›‘æ§ï¼ˆä¼˜åŒ–ç‰ˆï¼‰ï¼š
        1. è¯†åˆ« TMDb IDã€‚
        2. åŒå‘æ£€æŸ¥æ•°æ®åº“å’Œæœ¬åœ°ç¼“å­˜ï¼Œäº’è¡¥ç¼ºå¤±æ•°æ®ã€‚
        3. ç”Ÿæˆæœ¬åœ°è¦†ç›–ç¼“å­˜æ–‡ä»¶ (Override Cache)ã€‚
        4. (å¯é€‰) é€šçŸ¥ Emby åˆ·æ–°ã€‚
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            skip_refresh: æ˜¯å¦è·³è¿‡ Emby åˆ·æ–°æ­¥éª¤ (ç”¨äºæ‰¹é‡å¤„ç†æ—¶æœ€åç»Ÿä¸€åˆ·æ–°)
            
        Returns:
            str: è¯¥æ–‡ä»¶æ‰€å±çš„çˆ¶ç›®å½•è·¯å¾„ (å¦‚æœå¤„ç†æˆåŠŸ)ï¼Œå¦åˆ™è¿”å› None
        """
        folder_path = os.path.dirname(file_path)
        try:
            filename = os.path.basename(file_path)
            folder_name = os.path.basename(folder_path)
            grandparent_path = os.path.dirname(folder_path)
            grandparent_name = os.path.basename(grandparent_path)
            
            # =========================================================
            # æ­¥éª¤ 1: è¯†åˆ«ä¿¡æ¯
            # =========================================================
            tmdb_id = None
            search_query = None
            search_year = None

            tmdb_regex = r'(?:tmdb|tmdbid)[-_=\s]*(\d+)'
            match = re.search(tmdb_regex, folder_name, re.IGNORECASE)
            if not match:
                match = re.search(tmdb_regex, grandparent_name, re.IGNORECASE)
            if not match:
                match = re.search(tmdb_regex, filename, re.IGNORECASE)

            if match:
                tmdb_id = match.group(1)
                logger.info(f"  âœ [å®æ—¶ç›‘æ§] æˆåŠŸæå– TMDb ID: {tmdb_id}")
            else:
                # ä¼˜åŒ–ï¼šå…ˆå°è¯•ä»ç›®å½•åæå–æœç´¢ä¿¡æ¯
                def is_season_folder(name: str) -> bool:
                    return bool(re.match(r'^(Season|S)\s*\d+|Specials', name, re.IGNORECASE))
                def extract_title_year(text: str):
                    year_regex = r'\b(19|20)\d{2}\b'
                    season_episode_regex = r'[sS](\d{1,2})[eE](\d{1,2})'
                    year_matches = list(re.finditer(year_regex, text))
                    se_match = re.search(season_episode_regex, text)
                    if year_matches:
                        last_year_match = year_matches[-1]
                        year = last_year_match.group(0)
                        raw_title = text[:last_year_match.start()]
                    elif se_match:
                        year = None
                        raw_title = text[:se_match.start()]
                    else:
                        year = None
                        raw_title = text
                    query = raw_title.replace('.', ' ').replace('_', ' ').strip(' -[]()')
                    return query, year

                # é¦–å…ˆå°è¯•folder_nameï¼Œä½†å¦‚æœæ˜¯å­£ç›®å½•åï¼Œåˆ™æ¢ç”¨grandparent_name
                if is_season_folder(folder_name):
                    search_query, search_year = extract_title_year(grandparent_name)
                else:
                    search_query, search_year = extract_title_year(folder_name)

                # å¦‚æœç›®å½•åéƒ½æ²¡æå–åˆ°æœ‰æ•ˆæ ‡é¢˜ï¼Œå†ç”¨filename
                if not search_query or search_query == '':
                    search_query, search_year = extract_title_year(os.path.splitext(filename)[0])

                logger.info(f"  âœ [å®æ—¶ç›‘æ§] æœªæ‰¾åˆ°IDï¼Œæå–æœç´¢ä¿¡æ¯: æ ‡é¢˜='{search_query}', å¹´ä»½='{search_year}'")

            # =========================================================
            # æ­¥éª¤ 2: è·å– TMDb æ•°æ® (å¦‚æœåªæœ‰æ ‡é¢˜åˆ™æœç´¢)
            # =========================================================
            if not tmdb_id and search_query:
                is_series_guess = bool(re.search(r'S\d+E\d+', filename, re.IGNORECASE))
                search_type = 'tv' if is_series_guess else 'movie'
                results = tmdb.search_media(search_query, self.tmdb_api_key, item_type=search_type, year=search_year)
                if results:
                    tmdb_id = str(results[0].get('id'))
                    logger.info(f"  âœ [å®æ—¶ç›‘æ§] æœç´¢åŒ¹é…æˆåŠŸ: {results[0].get('title') or results[0].get('name')} (ID: {tmdb_id})")
                else:
                    logger.warning(f"  âœ [å®æ—¶ç›‘æ§] æœç´¢å¤±è´¥ï¼Œæ— æ³•å¤„ç†: {search_query}")
                    return None

            if not tmdb_id: return None

            # ç¡®å®šç±»å‹
            is_series = bool(re.search(r'S\d+E\d+', filename, re.IGNORECASE))
            item_type = "Series" if is_series else "Movie"

            # =========================================================
            # æé€ŸæŸ¥é‡ (åˆ©ç”¨æ–‡ä»¶åæ¯”å¯¹)
            # =========================================================
            try:
                # è·å–è¯¥ TMDb ID ä¸‹æ‰€æœ‰å·²å…¥åº“çš„æ–‡ä»¶å (å«ç”µå½±å’Œæ‰€æœ‰åˆ†é›†)
                known_files = media_db.get_known_filenames_by_tmdb_id(tmdb_id)
                current_filename = os.path.basename(file_path)
                
                if current_filename in known_files:
                    logger.info(f"  âœ [å®æ—¶ç›‘æ§] æ–‡ä»¶å·²å®Œç¾å…¥åº“ ({current_filename})ï¼Œç›´æ¥è·³è¿‡ã€‚")
                    return folder_path # å³ä½¿è·³è¿‡å¤„ç†ï¼Œä¹Ÿè¿”å›è·¯å¾„ä»¥ä¾¿åç»­åˆ·æ–°æ£€æŸ¥
            except Exception as e:
                logger.warning(f"  âœ [å®æ—¶ç›‘æ§] æŸ¥é‡å¤±è´¥ï¼Œå°†ç»§ç»­å¸¸è§„æµç¨‹: {e}")

            # =========================================================
            # â˜…â˜…â˜… æ ¸å¿ƒå‡çº§ï¼šæ•°æ®åº“ä¸ç¼“å­˜åŒå‘äº’è¡¥æ£€æŸ¥ â˜…â˜…â˜…
            # =========================================================
            should_skip_full_processing = False
            
            # 1. è·¯å¾„å‡†å¤‡
            cache_folder_name = "tmdb-movies2" if item_type == "Movie" else "tmdb-tv"
            base_override_dir = os.path.join(self.local_data_path, "override", cache_folder_name, str(tmdb_id))
            main_json_filename = "all.json" if item_type == "Movie" else "series.json"
            main_json_path = os.path.join(base_override_dir, main_json_filename)
            file_exists = os.path.exists(main_json_path)

            # 2. æ•°æ®åº“æŸ¥è¯¢ (è·å–å®Œæ•´å…ƒæ•°æ® + æ¼”å‘˜è¡¨)
            db_record = None
            db_actors = []
            
            with get_central_db_connection() as conn:
                cursor = conn.cursor()
                # A. æŸ¥ä¸»è¡¨
                cursor.execute(f"SELECT * FROM media_metadata WHERE tmdb_id = %s AND item_type = %s", (str(tmdb_id), item_type))
                row = cursor.fetchone()
                if row:
                    db_record = dict(row)
                    # B. æŸ¥æ¼”å‘˜ (å¦‚æœä¸»è¡¨å­˜åœ¨)
                    if db_record.get('actors_json'):
                        try:
                            raw_actors = db_record['actors_json']
                            # â˜…â˜…â˜… ä¿®å¤ï¼šå…¼å®¹ list å’Œ str ä¸¤ç§ç±»å‹ â˜…â˜…â˜…
                            if isinstance(raw_actors, str):
                                actors_link = json.loads(raw_actors)
                            else:
                                actors_link = raw_actors

                            # æå– tmdb_id åˆ—è¡¨
                            actor_tmdb_ids = [a['tmdb_id'] for a in actors_link if 'tmdb_id' in a]
                            if actor_tmdb_ids:
                                # æ‰¹é‡æŸ¥è¯¢æ¼”å‘˜è¯¦æƒ…
                                placeholders = ','.join(['%s'] * len(actor_tmdb_ids))
                                sql = f"""
                                    SELECT am.*, pim.primary_name as name
                                    FROM actor_metadata am
                                    LEFT JOIN person_identity_map pim ON am.tmdb_id = pim.tmdb_person_id
                                    WHERE am.tmdb_id IN ({placeholders})
                                """
                                cursor.execute(sql, tuple(actor_tmdb_ids))
                                
                                actor_rows = cursor.fetchall()
                                actor_map = {r['tmdb_id']: dict(r) for r in actor_rows}
                                
                                # ç»„è£…å›æœ‰åºåˆ—è¡¨
                                for link in actors_link:
                                    tid = link.get('tmdb_id')
                                    if tid in actor_map:
                                        full_actor = actor_map[tid].copy()
                                        full_actor['character'] = link.get('character') # ä½¿ç”¨å…³ç³»è¡¨é‡Œçš„è§’è‰²å
                                        full_actor['order'] = link.get('order')
                                        db_actors.append(full_actor)
                                        
                                # æŒ‰ order æ’åº
                                db_actors.sort(key=lambda x: x.get('order', 999))
                        except Exception as e:
                            logger.warning(f"  âœ [å®æ—¶ç›‘æ§] ä»æ•°æ®åº“è§£ææ¼”å‘˜å¤±è´¥: {e}")

            # 3. å†³ç­–é€»è¾‘åˆ†æ”¯
            
            # --- åˆ†æ”¯ A: æ•°æ®åº“æœ‰ï¼Œæ–‡ä»¶æ²¡æœ‰ -> ç”Ÿæˆæ–‡ä»¶ (çº¸è´¨å­˜æ¡£ç¼ºå¤±) ---
            if db_record and not file_exists and db_actors:
                logger.info(f"  âœ [å®æ—¶ç›‘æ§] å‘½ä¸­æ•°æ®åº“ç¼“å­˜ (ID:{tmdb_id})ï¼Œä½†è¦†ç›–ç¼“å­˜ç¼ºå¤±ã€‚æ­£åœ¨ä»æ•°æ®åº“ç”Ÿæˆè¦†ç›–ç¼“å­˜æ–‡ä»¶...")
                try:
                    # 1. ç”Ÿæˆä¸» payload
                    from tasks.helpers import reconstruct_metadata_from_db
                    payload = reconstruct_metadata_from_db(db_record, db_actors)

                    # â˜…â˜…â˜… æ–°å¢ï¼šå¦‚æœæ˜¯å‰§é›†ï¼Œéœ€è¦æŸ¥è¯¢å¹¶æ³¨å…¥åˆ†å­£/åˆ†é›†æ•°æ® â˜…â˜…â˜…
                    if item_type == "Series":
                        with get_central_db_connection() as conn:
                            cursor = conn.cursor()
                            
                            # A. æŸ¥åˆ†å­£
                            cursor.execute("SELECT * FROM media_metadata WHERE parent_series_tmdb_id = %s AND item_type = 'Season'", (str(tmdb_id),))
                            seasons_rows = cursor.fetchall()
                            seasons_data = []
                            for s_row in seasons_rows:
                                s_data = {
                                    "id": int(s_row['tmdb_id']),
                                    "name": s_row['title'],
                                    "overview": s_row['overview'],
                                    "season_number": s_row['season_number'],
                                    "air_date": str(s_row['release_date']) if s_row['release_date'] else None,
                                    "poster_path": s_row['poster_path']
                                }
                                seasons_data.append(s_data)
                            
                            # B. æŸ¥åˆ†é›†
                            cursor.execute("SELECT * FROM media_metadata WHERE parent_series_tmdb_id = %s AND item_type = 'Episode'", (str(tmdb_id),))
                            episodes_rows = cursor.fetchall()
                            episodes_data = {} # å­—å…¸æ ¼å¼ S1E1: data
                            
                            for e_row in episodes_rows:
                                s_num = e_row['season_number']
                                e_num = e_row['episode_number']
                                key = f"S{s_num}E{e_num}"
                                e_data = {
                                    "id": int(e_row['tmdb_id']),
                                    "name": e_row['title'],
                                    "overview": e_row['overview'],
                                    "season_number": s_num,
                                    "episode_number": e_num,
                                    "air_date": str(e_row['release_date']) if e_row['release_date'] else None,
                                    "vote_average": e_row['rating'],
                                }
                                episodes_data[key] = e_data

                            # C. æ³¨å…¥ payload
                            if seasons_data:
                                payload['seasons_details'] = seasons_data
                            if episodes_data:
                                payload['episodes_details'] = episodes_data
                                
                            logger.info(f"  âœ [å®æ—¶ç›‘æ§] å·²ä»æ•°æ®åº“æ¢å¤ {len(seasons_data)} ä¸ªå­£å’Œ {len(episodes_data)} ä¸ªåˆ†é›†çš„æ•°æ®ã€‚")
                    
                    # 2. æ„é€ ä¸Šä¸‹æ–‡å¯¹è±¡
                    fake_item_details = {
                        "Id": "pending", 
                        "Name": db_record.get('title'), 
                        "Type": item_type, 
                        "ProviderIds": {"Tmdb": tmdb_id}
                    }
                    
                    # 3. å†™å…¥æ–‡ä»¶
                    self.sync_item_metadata(
                        item_details=fake_item_details,
                        tmdb_id=str(tmdb_id),
                        metadata_override=payload
                    )
                    should_skip_full_processing = True
                    logger.info(f"  âœ [å®æ—¶ç›‘æ§] è¦†ç›–æ–‡ä»¶å·²æ¢å¤ã€‚è·³è¿‡åœ¨çº¿åˆ®å‰Šã€‚")
                except Exception as e:
                    logger.error(f"  âœ [å®æ—¶ç›‘æ§] ä»æ•°æ®åº“æ¢å¤æ–‡ä»¶å¤±è´¥: {e}ï¼Œå°†å›é€€åˆ°åœ¨çº¿åˆ®å‰Šã€‚")

            # --- åˆ†æ”¯ B: æ–‡ä»¶æœ‰ï¼Œæ•°æ®åº“æ²¡æœ‰ -> åå“ºæ•°æ®åº“ (æ•°å­—å­˜æ¡£ç¼ºå¤±) ---
            elif not db_record and file_exists:
                logger.info(f"  âœ [å®æ—¶ç›‘æ§] å‘½ä¸­æœ¬åœ°è¦†ç›–æ–‡ä»¶ (ID:{tmdb_id})ï¼Œä½†æ•°æ®åº“è®°å½•ç¼ºå¤±ã€‚æ­£åœ¨åå“ºæ•°æ®åº“...")
                try:
                    override_data = _read_local_json(main_json_path)
                    if override_data:
                        # æå–æ¼”å‘˜
                        cast_data = (override_data.get('casts', {}) or override_data.get('credits', {})).get('cast', [])
                        
                        # æ„é€ ä¼ªé€ çš„ Emby å¯¹è±¡ç”¨äº upsert
                        fake_item_details = {
                            "Id": "pending", 
                            "Name": override_data.get('title') or override_data.get('name'), 
                            "Type": item_type, 
                            "ProviderIds": {"Tmdb": tmdb_id},
                            "DateCreated": datetime.now(timezone.utc)
                        }
                        
                        # å†™å…¥æ•°æ®åº“
                        with get_central_db_connection() as conn:
                            cursor = conn.cursor()
                            self._upsert_media_metadata(
                                cursor=cursor,
                                item_type=item_type,
                                final_processed_cast=cast_data, # ç›´æ¥ä½¿ç”¨æ–‡ä»¶é‡Œçš„æ¼”å‘˜
                                source_data_package=override_data,
                                item_details_from_emby=fake_item_details
                            )
                            conn.commit()
                        
                        should_skip_full_processing = True
                        logger.info(f"  âœ [å®æ—¶ç›‘æ§] æ•°æ®åº“è®°å½•å·²è¡¥å…¨ã€‚è·³è¿‡åœ¨çº¿åˆ®å‰Šã€‚")
                except Exception as e:
                    logger.error(f"  âœ [å®æ—¶ç›‘æ§] åå“ºæ•°æ®åº“å¤±è´¥: {e}ï¼Œå°†å›é€€åˆ°åœ¨çº¿åˆ®å‰Šã€‚")

            # --- åˆ†æ”¯ C: éƒ½æœ‰ -> å®Œç¾çŠ¶æ€ ---
            elif db_record and file_exists:
                # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–° in_library çŠ¶æ€ (å¦‚æœæ˜¯æ–°æ–‡ä»¶å…¥åº“)
                if db_record.get('in_library') is False:
                     logger.info(f"  âœ [å®æ—¶ç›‘æ§] æ•°æ®åŒå…¨ (ID:{tmdb_id})ï¼Œä½†æ•°æ®åº“æ ‡è®°ä¸ºç¦»çº¿ã€‚æ— éœ€å¤„ç†å…ƒæ•°æ®ï¼Œä»…é€šçŸ¥ Emby åˆ·æ–°ã€‚")
                else:
                     logger.info(f"  âœ [å®æ—¶ç›‘æ§] æ•°æ®åŒå…¨ä¸”åœ¨çº¿ (ID:{tmdb_id})ã€‚å¯èƒ½æ˜¯æ´—ç‰ˆ/è¿½æ›´ï¼Œè·³è¿‡å…ƒæ•°æ®å¤„ç†ï¼Œä»…é€šçŸ¥ Emby åˆ·æ–°ã€‚")
                
                should_skip_full_processing = True

            # --- åˆ†æ”¯ D: éƒ½æ²¡æœ‰ -> ç»§ç»­åç»­çš„ TMDb åœ¨çº¿æµç¨‹ ---
            else:
                logger.info(f"  âœ [å®æ—¶ç›‘æ§] æœ¬åœ°æ— ç¼“å­˜ (ID:{tmdb_id})ï¼Œå‡†å¤‡æ‰§è¡Œ TMDb åœ¨çº¿åˆ®å‰Š...")

            # =========================================================
            # æ­¥éª¤ 3: è·å–å®Œæ•´è¯¦æƒ… & å‡†å¤‡æ ¸å¿ƒå¤„ç†
            # =========================================================
            details = None
            aggregated_tmdb_data = None
            final_processed_cast = None

            if not should_skip_full_processing:
                time.sleep(random.uniform(0.5, 2.0))
                logger.info(f"  âœ [å®æ—¶ç›‘æ§] æ­£åœ¨è·å– TMDb è¯¦æƒ…å¹¶æ‰§è¡Œæ ¸å¿ƒå¤„ç† (ID: {tmdb_id})...")
                
                if item_type == "Movie":
                    details = tmdb.get_movie_details(int(tmdb_id), self.tmdb_api_key)
                else:
                    aggregated_tmdb_data = tmdb.aggregate_full_series_data_from_tmdb(int(tmdb_id), self.tmdb_api_key)
                    details = aggregated_tmdb_data.get('series_details') if aggregated_tmdb_data else None
                    
                if not details:
                    logger.error("  âœ [å®æ—¶ç›‘æ§] æ— æ³•è·å– TMDb è¯¦æƒ…ï¼Œä¸­æ­¢å¤„ç†ã€‚")
                    return None
                
                # --- æ ‡é¢˜ä¸ç®€ä»‹ AI ç¿»è¯‘ ---
                if self.ai_translator:
                    
                    # ====== 1. ç®€ä»‹ç¿»è¯‘æ¨¡å— ======
                    if self.config.get(constants.CONFIG_OPTION_AI_TRANSLATE_OVERVIEW, False):
                        current_overview = details.get("overview", "")
                        item_title = details.get("title") or details.get("name")

                        # ä¼˜å…ˆæ£€æŸ¥æœ¬åœ°æ•°æ®åº“ç¼“å­˜ (ç®€ä»‹)
                        local_trans = media_db.get_local_translation_info(str(tmdb_id), item_type)
                        if local_trans and local_trans.get('overview') and utils.contains_chinese(local_trans['overview']):
                            details["overview"] = local_trans['overview']
                            if aggregated_tmdb_data and "series_details" in aggregated_tmdb_data:
                                aggregated_tmdb_data["series_details"]["overview"] = local_trans['overview']
                            logger.info(f"  âœ [å®æ—¶ç›‘æ§] å‘½ä¸­æœ¬åœ°ä¸­æ–‡ç®€ä»‹ç¼“å­˜ï¼Œè·³è¿‡AIç¿»è¯‘ã€‚")
                        
                        else:
                            # åˆ¤æ–­æ˜¯å¦éœ€è¦ç¿»è¯‘ï¼šç®€ä»‹ä¸ºç©º æˆ– ä¸åŒ…å«ä¸­æ–‡
                            needs_translation = False
                            if not current_overview:
                                needs_translation = True
                            elif not utils.contains_chinese(current_overview):
                                needs_translation = True
                            
                            if needs_translation:
                                logger.info(f"  âœ [å®æ—¶ç›‘æ§] æ£€æµ‹åˆ°ç®€ä»‹ç¼ºå¤±æˆ–éä¸­æ–‡ï¼Œå‡†å¤‡è¿›è¡Œ AI ç¿»è¯‘...")
                                english_overview = ""
                                
                                # 1. å°è¯•ä½¿ç”¨ç°æœ‰çš„è‹±æ–‡ç®€ä»‹
                                if current_overview and len(current_overview) > 10:
                                    english_overview = current_overview
                                
                                # 2. å¦‚æœç°æœ‰ç®€ä»‹ä¸ºç©ºï¼Œå°è¯•è¯·æ±‚è‹±æ–‡ç‰ˆæ•°æ®
                                else:
                                    try:
                                        if item_type == "Movie":
                                            en_data = tmdb.get_movie_details(int(tmdb_id), self.tmdb_api_key, language="en-US")
                                            english_overview = en_data.get("overview")
                                        elif item_type == "Series":
                                            en_data = tmdb.get_tv_details(int(tmdb_id), self.tmdb_api_key, language="en-US")
                                            english_overview = en_data.get("overview")
                                    except Exception as e_en:
                                        logger.warning(f"  âœ [å®æ—¶ç›‘æ§] è·å–è‹±æ–‡æºæ•°æ®å¤±è´¥: {e_en}")

                                # 3. è°ƒç”¨ AI ç¿»è¯‘
                                if english_overview:
                                    translated_overview = self.ai_translator.translate_overview(english_overview, title=item_title)
                                    if translated_overview:
                                        details["overview"] = translated_overview
                                        logger.info(f"  âœ [å®æ—¶ç›‘æ§] ç®€ä»‹ç¿»è¯‘æˆåŠŸï¼Œå·²æ›´æ–°å†…å­˜æ•°æ®ã€‚")
                                        if aggregated_tmdb_data and "series_details" in aggregated_tmdb_data:
                                            aggregated_tmdb_data["series_details"]["overview"] = translated_overview
                                    else:
                                        logger.warning(f"  âœ [å®æ—¶ç›‘æ§] AI ç¿»è¯‘æœªè¿”å›ç»“æœã€‚")
                                else:
                                    logger.info(f"  âœ [å®æ—¶ç›‘æ§] æœªèƒ½è·å–åˆ°æœ‰æ•ˆçš„è‹±æ–‡ç®€ä»‹ï¼Œè·³è¿‡ç¿»è¯‘ã€‚")

                    # ====== 2. æ ‡é¢˜ç¿»è¯‘æ¨¡å— ======
                    if self.config.get(constants.CONFIG_OPTION_AI_TRANSLATE_TITLE, False):
                        # è·å–å½“å‰æ ‡é¢˜
                        current_title = details.get("title") if item_type == "Movie" else details.get("name")
                        
                        # ä¼˜å…ˆæ£€æŸ¥æœ¬åœ°æ•°æ®åº“ç¼“å­˜ (æ ‡é¢˜)
                        local_trans = media_db.get_local_translation_info(str(tmdb_id), item_type)
                        if local_trans and local_trans.get('title') and utils.contains_chinese(local_trans['title']):
                            current_title = local_trans['title']
                            if item_type == "Movie":
                                details["title"] = current_title
                            else:
                                details["name"] = current_title
                                if aggregated_tmdb_data and "series_details" in aggregated_tmdb_data:
                                    aggregated_tmdb_data["series_details"]["name"] = current_title
                            logger.info(f"  âœ [å®æ—¶ç›‘æ§] å‘½ä¸­æœ¬åœ°ä¸­æ–‡æ ‡é¢˜ç¼“å­˜ï¼Œè·³è¿‡AIç¿»è¯‘ã€‚")
                        
                        # å¦‚æœæ ‡é¢˜å­˜åœ¨ä¸”ä¸åŒ…å«ä¸­æ–‡ï¼Œåˆ™å°è¯•ç¿»è¯‘
                        elif current_title and not utils.contains_chinese(current_title):
                            logger.info(f"  âœ [å®æ—¶ç›‘æ§] æ£€æµ‹åˆ°æ ‡é¢˜ä¸ºçº¯å¤–æ–‡ ('{current_title}')ï¼Œå‡†å¤‡è¿›è¡Œ AI ç¿»è¯‘...")
                            
                            release_date = details.get("release_date") if item_type == "Movie" else details.get("first_air_date")
                            year_str = release_date[:4] if release_date else ""
                            
                            translated_title = self.ai_translator.translate_title(
                                current_title, 
                                media_type=item_type, 
                                year=year_str
                            )
                            
                            if translated_title and utils.contains_chinese(translated_title):
                                logger.info(f"  âœ [å®æ—¶ç›‘æ§] æ ‡é¢˜ç¿»è¯‘æˆåŠŸ: '{current_title}' -> '{translated_title}'")
                                if item_type == "Movie":
                                    details["title"] = translated_title
                                else:
                                    details["name"] = translated_title
                                    if aggregated_tmdb_data and "series_details" in aggregated_tmdb_data:
                                        aggregated_tmdb_data["series_details"]["name"] = translated_title
                            else:
                                logger.warning(f"  âœ [å®æ—¶ç›‘æ§] æ ‡é¢˜ç¿»è¯‘ç»“æœä»ä¸ºå¤–æ–‡æˆ–ä¸ºç©ºï¼Œä¸¢å¼ƒ: {translated_title}")

                # å‡†å¤‡æ¼”å‘˜æºæ•°æ®
                authoritative_cast_source = []
                if item_type == "Movie":
                    credits_source = details.get('credits') or details.get('casts') or {}
                    authoritative_cast_source = credits_source.get('cast', [])
                elif item_type == "Series":
                    if aggregated_tmdb_data:
                        all_episodes = list(aggregated_tmdb_data.get("episodes_details", {}).values())
                        authoritative_cast_source = _aggregate_series_cast_from_tmdb_data(details, all_episodes)
                    else:
                        credits_source = details.get('aggregate_credits') or details.get('credits') or {}
                        authoritative_cast_source = credits_source.get('cast', [])

                with get_db_connection() as conn:
                    cursor = conn.cursor()
                    dummy_emby_item = {
                        "Id": "pending",
                        "Name": details.get('title') or details.get('name'),
                        "OriginalTitle": details.get('original_title') or details.get('original_name'),
                        "People": [],
                        "Genres": details.get('genres', [])
                    }
                    logger.info(f"  âœ [å®æ—¶ç›‘æ§] å¯åŠ¨æ¼”å‘˜è¡¨æ ¸å¿ƒå¤„ç† (AIç¿»è¯‘/å»é‡/å¤´åƒæ£€æŸ¥)...")
                    final_processed_cast = self._process_cast_list(
                        tmdb_cast_people=authoritative_cast_source,
                        emby_cast_people=[],
                        douban_cast_list=[],
                        item_details_from_emby=dummy_emby_item,
                        cursor=cursor,
                        tmdb_api_key=self.tmdb_api_key,
                        stop_event=None
                    )
                    conn.commit()

                if not final_processed_cast:
                    logger.warning("  âœ [å®æ—¶ç›‘æ§] æ¼”å‘˜å¤„ç†æœªèƒ½è¿”å›ç»“æœï¼Œå°†ä½¿ç”¨åŸå§‹æ•°æ®ã€‚")
                    final_processed_cast = authoritative_cast_source
            
            # =========================================================
            # æ­¥éª¤ 4 & 5: ç”Ÿæˆæœ¬åœ° override å…ƒæ•°æ®æ–‡ä»¶ & å†™å…¥æ•°æ®åº“
            # =========================================================
            if not should_skip_full_processing:
                # 1. å‡†å¤‡ä¼ªé€ çš„ Emby å¯¹è±¡
                fake_item_details = {
                    "Id": "pending",
                    "Name": details.get('title') or details.get('name'),
                    "Type": item_type,
                    "ProviderIds": {"Tmdb": tmdb_id}
                }

                logger.info(f"  âœ [å®æ—¶ç›‘æ§] æ­£åœ¨æŒ‰ç…§éª¨æ¶æ¨¡æ¿æ ¼å¼åŒ–å…ƒæ•°æ®...")

                # 2. åˆå§‹åŒ–éª¨æ¶
                formatted_metadata = construct_metadata_payload(
                    item_type=item_type,
                    tmdb_data=details,
                    aggregated_tmdb_data=aggregated_tmdb_data
                )

                # 3. å†™å…¥æœ¬åœ°æ–‡ä»¶
                logger.info(f"  âœ [å®æ—¶ç›‘æ§] æ­£åœ¨å†™å…¥æœ¬åœ°å…ƒæ•°æ®æ–‡ä»¶...")
                self.sync_item_metadata(
                    item_details=fake_item_details,
                    tmdb_id=tmdb_id,
                    final_cast_override=final_processed_cast,
                    metadata_override=formatted_metadata 
                )

                # 4. å†™å…¥æ•°æ®åº“ (å ä½è®°å½•)
                logger.info(f"  âœ [å®æ—¶ç›‘æ§] æ­£åœ¨å°†å…ƒæ•°æ®å†™å…¥æ•°æ®åº“ ...")
                with get_db_connection() as conn:
                    cursor = conn.cursor()
                    self._upsert_media_metadata(
                        cursor=cursor,
                        item_type=item_type,
                        final_processed_cast=final_processed_cast,
                        source_data_package=formatted_metadata, # ä½¿ç”¨æ ¼å¼åŒ–åçš„æ•°æ®
                        item_details_from_emby=fake_item_details # Id="pending"
                    )
                    conn.commit()

                # 5: ä¸‹è½½å›¾ç‰‡
                self.download_images_from_tmdb(
                    tmdb_id=tmdb_id,
                    item_type=item_type
                )

            else:
                logger.info(f"  âœ [å®æ—¶ç›‘æ§] å·²è·³è¿‡åœ¨çº¿åˆ®å‰Šå’Œå…ƒæ•°æ®å†™å…¥ (æ•°æ®å·²é€šè¿‡ç¼“å­˜æ¢å¤)ã€‚")

            # =========================================================
            # æ­¥éª¤ 6: é€šçŸ¥ Emby åˆ·æ–° (å¯é€‰)
            # =========================================================
            
            # â˜…â˜…â˜… æ™ºèƒ½è®¡ç®—éœ€è¦åˆ·æ–°çš„æ ¹è·¯å¾„ â˜…â˜…â˜…
            # é»˜è®¤åˆ·æ–°å½“å‰æ–‡ä»¶æ‰€åœ¨çš„çˆ¶ç›®å½• (é€‚ç”¨äºç”µå½± / å¹³é“ºçš„å‰§é›†)
            path_to_refresh = folder_path
            
            # å¦‚æœæ˜¯å‰§é›†ï¼Œä¸”çˆ¶ç›®å½•çœ‹èµ·æ¥åƒ "Season X"ï¼Œåˆ™å‘ä¸Šå–ä¸€çº§ï¼Œåˆ·æ–°å‰§é›†æ ¹ç›®å½•
            # è¿™æ ·å¯ä»¥åˆå¹¶åŒä¸€éƒ¨å‰§ä¸åŒå­£çš„åˆ·æ–°è¯·æ±‚
            if item_type == "Series":
                folder_name = os.path.basename(folder_path)
                # åŒ¹é… Season 1, S01, Specials ç­‰å¸¸è§å­£ç›®å½•å
                if re.match(r'^(Season|S)\s*\d+|Specials', folder_name, re.IGNORECASE):
                    path_to_refresh = os.path.dirname(folder_path)
                    logger.debug(f"  âœ [å®æ—¶ç›‘æ§] è¯†åˆ«ä¸ºå‰§é›†å­£ç›®å½•ï¼Œå°†åˆ·æ–°èŒƒå›´æ‰©å¤§è‡³å‰§é›†æ ¹ç›®å½•: {os.path.basename(path_to_refresh)}")

            if not skip_refresh:
                logger.info(f"  âœ [å®æ—¶ç›‘æ§] é€šçŸ¥ Emby åˆ·æ–°ç›®å½•: {path_to_refresh}")
                emby.refresh_library_by_path(path_to_refresh, self.emby_url, self.emby_api_key)
                logger.info(f"  âœ… [å®æ—¶ç›‘æ§] é¢„å¤„ç†å®Œæˆï¼Œç­‰å¾…Embyå…¥åº“æ›´æ–°åª’ä½“èµ„äº§æ•°æ®...")
            else:
                logger.info(f"  âœ [å®æ—¶ç›‘æ§] ç¼“å­˜å·²ç”Ÿæˆï¼Œç­‰å¾…ç»Ÿä¸€åˆ·æ–°...")
            
            # è¿”å›è®¡ç®—å‡ºçš„æœ€ä¼˜åˆ·æ–°è·¯å¾„
            return path_to_refresh

        except Exception as e:
            logger.error(f"  âœ [å®æ—¶ç›‘æ§] å¤„ç†æ–‡ä»¶ {file_path} æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
            return None

    # --- æ‰¹é‡å®æ—¶ç›‘æ§å¤„ç† ---
    def process_file_actively_batch(self, file_paths: List[str]):
        """
        å®æ—¶ç›‘æ§ï¼ˆæ‰¹é‡ç‰ˆï¼‰ï¼š
        é’ˆå¯¹çŸ­æ—¶é—´å†…æ¶Œå…¥çš„å¤šä¸ªæ–‡ä»¶ï¼Œå…ˆé€ä¸ªç”Ÿæˆè¦†ç›–ç¼“å­˜ï¼Œæœ€åç»Ÿä¸€åˆ·æ–° Embyã€‚
        â˜… ä¼˜åŒ–ï¼šå°†æ‰€æœ‰è·¯å¾„è§£æä¸º Emby é”šç‚¹ ID åå†å»é‡åˆ·æ–°ï¼Œé¿å…åŒä¸€åº“é‡å¤åˆ·æ–°ã€‚
        """
        if not file_paths:
            return

        logger.info(f"  ğŸ“¥ [å®æ—¶ç›‘æ§] æ”¶åˆ° {len(file_paths)} ä¸ªæ–°ä»»åŠ¡ï¼Œå¼€å§‹æ‰¹é‡é¢„å¤„ç†...")
        
        folders_to_check = set()
        
        # 1. å¾ªç¯å¤„ç†æ¯ä¸ªæ–‡ä»¶ (åªç”Ÿæˆç¼“å­˜ï¼Œä¸åˆ·æ–°)
        for i, file_path in enumerate(file_paths):
            try:
                logger.info(f"  âœ [å®æ—¶ç›‘æ§] ({i+1}/{len(file_paths)}) æ­£åœ¨å¤„ç†: {os.path.basename(file_path)}")
                # process_file_actively è¿”å›çš„æ˜¯å»ºè®®åˆ·æ–°çš„çˆ¶ç›®å½•è·¯å¾„
                folder = self.process_file_actively(file_path, skip_refresh=True)
                if folder:
                    folders_to_check.add(folder)
            except Exception as e:
                logger.error(f"  ğŸš« [å®æ—¶ç›‘æ§] å¤„ç†æ–‡ä»¶ '{file_path}' å¤±è´¥: {e}")

        # 2. â˜…â˜…â˜… ID çº§åˆ«å»é‡ä¸åˆ·æ–° â˜…â˜…â˜…
        if folders_to_check:
            logger.info(f"  ğŸ” [å®æ—¶ç›‘æ§] é¢„å¤„ç†å®Œæˆã€‚æ­£åœ¨è§£æ {len(folders_to_check)} ä¸ªè·¯å¾„å¯¹åº”çš„ Emby é”šç‚¹...")
            
            unique_anchor_map = {} # ID -> Name
            fallback_paths = []

            # A. è§£æè·¯å¾„åˆ° ID
            for folder_path in folders_to_check:
                anchor_id, anchor_name = emby.find_nearest_library_anchor(folder_path, self.emby_url, self.emby_api_key)
                if anchor_id:
                    if anchor_id not in unique_anchor_map:
                        unique_anchor_map[anchor_id] = anchor_name
                        logger.debug(f"    â”œâ”€ è·¯å¾„ '{os.path.basename(folder_path)}' -> é”šç‚¹ '{anchor_name}' (ID: {anchor_id})")
                else:
                    fallback_paths.append(folder_path)

            # B. åˆ·æ–°å”¯ä¸€çš„ ID
            if unique_anchor_map:
                logger.info(f"  ğŸš€ [å®æ—¶ç›‘æ§] èšåˆå®Œæˆï¼Œæ­£åœ¨åˆ·æ–° {len(unique_anchor_map)} ä¸ª Emby é”šç‚¹...")
                for anchor_id, anchor_name in unique_anchor_map.items():
                    logger.info(f"  âœ æ­£åœ¨åˆ·æ–°: '{anchor_name}' (ID: {anchor_id})")
                    emby.refresh_item_by_id(anchor_id, self.emby_url, self.emby_api_key)
                    time.sleep(0.2) # ç¨å¾®é—´éš”

            # C. å¤„ç†æ— æ³•è§£æ ID çš„è·¯å¾„ (å›é€€åˆ°æ—§æ–¹æ³•)
            if fallback_paths:
                logger.warning(f"  âš ï¸ [å®æ—¶ç›‘æ§] æœ‰ {len(fallback_paths)} ä¸ªè·¯å¾„æ— æ³•è§£æé”šç‚¹ï¼Œä½¿ç”¨å›é€€åˆ·æ–°...")
                for path in fallback_paths:
                    emby.refresh_library_by_path(path, self.emby_url, self.emby_api_key)

            logger.info(f"  âœ… [å®æ—¶ç›‘æ§] æ‰¹é‡é¢„å¤„ç†å®Œæˆï¼Œç­‰å¾…Embyå…¥åº“æ›´æ–°åª’ä½“èµ„äº§æ•°æ®...")
        else:
            logger.warning(f"  âš ï¸ [å®æ—¶ç›‘æ§] æœªæ”¶é›†åˆ°æœ‰æ•ˆçš„åˆ·æ–°ç›®å½•ï¼Œä»»åŠ¡ç»“æŸã€‚")

    # --- å†…éƒ¨ç§æœ‰æ–¹æ³•ï¼šå•æ–‡ä»¶æ•°æ®åº“æ¸…ç†é€»è¾‘ ---
    def _cleanup_local_db_for_deleted_file(self, filename: str) -> bool:
        """
        æ ¹æ®æ–‡ä»¶åæ‰§è¡Œæœ¬åœ°æ•°æ®åº“çš„æ¸…ç†å·¥ä½œï¼ˆä¸å« Emby åˆ·æ–°ï¼‰ã€‚
        è¿”å› True è¡¨ç¤ºæ‰§è¡Œäº†æ¸…ç†ï¼ŒFalse è¡¨ç¤ºæœªæ‰¾åˆ°è®°å½•ã€‚
        """
        # 1. ç²¾ç¡®åæŸ¥ (è·å– target_emby_id)
        media_info = media_db.get_media_info_by_filename(filename)
        
        if media_info:
            tmdb_id = media_info.get('tmdb_id')
            item_type = media_info.get('item_type')
            item_name = media_info.get('title', filename)
            target_emby_id = media_info.get('target_emby_id')
            
            # æå–çˆ¶å‰§é›† ID (å¦‚æœæ˜¯åˆ†é›†)
            parent_series_tmdb_id = media_info.get('parent_series_tmdb_id')
            
            # å…œåº•ï¼šå¦‚æœèµ„äº§é‡Œæ²¡è®° IDï¼Œå°è¯•å– emby_item_ids_json çš„ç¬¬ä¸€ä¸ª
            if not target_emby_id:
                all_ids = media_info.get('emby_item_ids_json')
                if all_ids and len(all_ids) > 0:
                    target_emby_id = all_ids[0]

            if target_emby_id:
                logger.info(f"  âœ [æ–‡ä»¶åˆ é™¤] æ•°æ®åº“å‘½ä¸­: '{item_name}' (TMDB:{tmdb_id}) -> å¯¹åº” EmbyID: {target_emby_id}")
                
                # 2. æ¸…ç†æ•°æ®åº“ (maintenance_db ä¼šè‡ªåŠ¨å¤„ç†çˆ¶å‰§é›†çš„çŠ¶æ€æ ‡è®°)
                cascaded_info = maintenance_db.cleanup_deleted_media_item(
                    item_id=target_emby_id,
                    item_name=item_name,
                    item_type=item_type,
                    series_id_from_webhook=None 
                )
                
                # 3. æ™ºèƒ½æ¸…ç†æ—¥å¿—å’Œç¼“å­˜
                ids_to_clean = set()

                if cascaded_info:
                    # æƒ…å†µ A: è§¦å‘äº†çº§è”ä¸‹æ¶ (Series æˆ– Movie)
                    # æˆ‘ä»¬æ¸…ç†è¯¥é¡¶å±‚åª’ä½“å…³è”çš„æ‰€æœ‰ Emby ID
                    if cascaded_info.get('emby_ids'):
                        ids_to_clean.update(cascaded_info['emby_ids'])
                        logger.info(f"  ğŸ§¹ [çº§è”æ¸…ç†] é¡¶å±‚åª’ä½“ {cascaded_info.get('item_name', 'æœªçŸ¥')} (TMDB:{cascaded_info['tmdb_id']}) å·²ç¦»çº¿ï¼Œå‡†å¤‡æ¸…ç† {len(ids_to_clean)} æ¡å…³è”æ—¥å¿—ã€‚")
                    
                    # å¦‚æœæ˜¯ç”µå½±ï¼Œtarget_emby_id æœ¬èº«å°±æ˜¯é¡¶å±‚ IDï¼Œç¡®ä¿å®ƒè¢«åŒ…å«
                    if item_type == 'Movie':
                        ids_to_clean.add(target_emby_id)
                
                else:
                    # æƒ…å†µ B: åªæ˜¯åˆ äº†ä¸ªåˆ†é›†ï¼Œå‰§è¿˜åœ¨
                    # å¦‚æœæ˜¯ç”µå½±ï¼ˆè™½ç„¶ä¸Šé¢è¦†ç›–äº†ï¼‰ï¼Œè¿˜æ˜¯åˆ ä¸€ä¸‹æ¯”è¾ƒå¥½
                    if item_type == 'Movie':
                        ids_to_clean.add(target_emby_id)

                # ç»Ÿä¸€æ‰§è¡Œæ¸…ç†
                if ids_to_clean:
                    try:
                        with get_central_db_connection() as conn:
                            cursor = conn.cursor()
                            for clean_id in ids_to_clean:
                                # 1. åˆ æ•°æ®åº“æ—¥å¿—
                                self.log_db_manager.remove_from_processed_log(cursor, clean_id)
                                # 2. åˆ å†…å­˜ç¼“å­˜
                                if clean_id in self.processed_items_cache:
                                    del self.processed_items_cache[clean_id]
                            conn.commit()
                        logger.info(f"  âœ [æ–‡ä»¶åˆ é™¤] å·²æ¸…ç† {len(ids_to_clean)} æ¡ç›¸å…³çš„å·²å¤„ç†è®°å½•/ç¼“å­˜ã€‚")
                    except Exception as e:
                        logger.warning(f"  âœ [æ–‡ä»¶åˆ é™¤] æ¸…ç†æ—¥å¿—æ—¶é‡åˆ°è½»å¾®é”™è¯¯: {e}")

                return True
            else:
                logger.warning(f"  âœ [æ–‡ä»¶åˆ é™¤] æ•°æ®åº“è®°å½•å­˜åœ¨ä½†æ— æ³•å®šä½ Emby IDï¼Œè·³è¿‡æœ¬åœ°æ¸…ç†: {filename}")
        
        return False

    # --- å®æ—¶ç›‘æ§ï¼šå¤„ç†æ–‡ä»¶åˆ é™¤ (å•æ–‡ä»¶ç‰ˆ) ---
    def process_file_deletion(self, file_path: str):
        """
        å®æ—¶ç›‘æ§ï¼šå¤„ç†å•ä¸ªæ–‡ä»¶åˆ é™¤äº‹ä»¶ã€‚
        """
        try:
            filename = os.path.basename(file_path)
            folder_path = os.path.dirname(file_path)
            
            logger.info(f"  ğŸ—‘ï¸ [æ–‡ä»¶åˆ é™¤] æ£€æµ‹åˆ°æ–‡ä»¶ç§»é™¤: {filename}")
            
            # 1. æ‰§è¡Œæ•°æ®åº“æ¸…ç†
            cleaned = self._cleanup_local_db_for_deleted_file(filename)
            
            # 2. åˆ·æ–°å‘é‡ç¼“å­˜ (å¦‚æœæœ‰æ¸…ç†åŠ¨ä½œä¸”å¼€å¯äº†æ¨è)
            if cleaned and config_manager.APP_CONFIG.get(constants.CONFIG_OPTION_PROXY_ENABLED) and config_manager.APP_CONFIG.get(constants.CONFIG_OPTION_AI_VECTOR):
                try:
                    threading.Thread(target=RecommendationEngine.refresh_cache).start()
                except: pass

            # 3. é€šçŸ¥ Emby åˆ·æ–° (æ— è®ºæœ¬åœ°æ˜¯å¦æ¸…ç†ï¼Œéƒ½è¦é€šçŸ¥ Emby åŒæ­¥)
            logger.info(f"  âœ [æ–‡ä»¶åˆ é™¤] é€šçŸ¥ Emby åˆ·æ–°çˆ¶ç›®å½•: {folder_path}")
            emby.refresh_library_by_path(folder_path, self.emby_url, self.emby_api_key)

        except Exception as e:
            logger.error(f"  ğŸš« [æ–‡ä»¶åˆ é™¤] å¤„ç†å¤±è´¥: {e}", exc_info=True)

    # --- å®æ—¶ç›‘æ§ï¼šå¤„ç†æ–‡ä»¶åˆ é™¤ (æ‰¹é‡ç‰ˆ) ---
    def process_file_deletion_batch(self, file_paths: List[str]):
        """
        å®æ—¶ç›‘æ§ï¼šæ‰¹é‡å¤„ç†æ–‡ä»¶åˆ é™¤äº‹ä»¶ã€‚
        â˜… ä¼˜åŒ–ï¼šID çº§åˆ«å»é‡åˆ·æ–°ã€‚
        """
        if not file_paths:
            return

        logger.info(f"  ğŸ—‘ï¸ [æ‰¹é‡åˆ é™¤] å¼€å§‹å¤„ç† {len(file_paths)} ä¸ªæ–‡ä»¶çš„åˆ é™¤äº‹ä»¶...")
        
        folders_to_check = set()
        cleaned_count = 0
        
        # 1. å¾ªç¯æ¸…ç†æ•°æ®åº“
        for file_path in file_paths:
            try:
                filename = os.path.basename(file_path)
                folder_path = os.path.dirname(file_path)
                folders_to_check.add(folder_path)
                
                if self._cleanup_local_db_for_deleted_file(filename):
                    cleaned_count += 1
            except Exception as e:
                logger.error(f"  ğŸš« [æ‰¹é‡åˆ é™¤] å¤„ç†æ–‡ä»¶ '{file_path}' æ—¶å‡ºé”™: {e}")

        # 2. åˆ·æ–°å‘é‡ç¼“å­˜
        if cleaned_count > 0 and config_manager.APP_CONFIG.get(constants.CONFIG_OPTION_PROXY_ENABLED) and config_manager.APP_CONFIG.get(constants.CONFIG_OPTION_AI_VECTOR):
            try:
                threading.Thread(target=RecommendationEngine.refresh_cache).start()
            except: pass

        # 3. â˜…â˜…â˜… ID çº§åˆ«å»é‡ä¸åˆ·æ–° â˜…â˜…â˜…
        logger.info(f"  ğŸ” [æ‰¹é‡åˆ é™¤] æ•°æ®åº“æ¸…ç†å®Œæˆ ({cleaned_count}/{len(file_paths)})ï¼Œæ­£åœ¨è§£æ {len(folders_to_check)} ä¸ªè·¯å¾„å¯¹åº”çš„ Emby é”šç‚¹...")
        
        unique_anchor_map = {}
        fallback_paths = []

        for folder_path in folders_to_check:
            anchor_id, anchor_name = emby.find_nearest_library_anchor(folder_path, self.emby_url, self.emby_api_key)
            if anchor_id:
                unique_anchor_map[anchor_id] = anchor_name
            else:
                fallback_paths.append(folder_path)

        if unique_anchor_map:
            logger.info(f"  ğŸš€ [æ‰¹é‡åˆ é™¤] èšåˆå®Œæˆï¼Œæ­£åœ¨åˆ·æ–° {len(unique_anchor_map)} ä¸ª Emby é”šç‚¹...")
            for anchor_id, anchor_name in unique_anchor_map.items():
                logger.info(f"  âœ æ­£åœ¨åˆ·æ–°: '{anchor_name}' (ID: {anchor_id})")
                emby.refresh_item_by_id(anchor_id, self.emby_url, self.emby_api_key)
                time.sleep(0.2)

        if fallback_paths:
            for path in fallback_paths:
                emby.refresh_library_by_path(path, self.emby_url, self.emby_api_key)

    def _refresh_lib_guid_map(self):
        """ä» Emby å®æ—¶è·å–æ‰€æœ‰åª’ä½“åº“çš„ ID åˆ° GUID æ˜ å°„"""
        try:
            # è°ƒç”¨ emby.py ä¸­çš„å‡½æ•°
            libs_data = emby.get_all_libraries_with_paths(self.emby_url, self.emby_api_key)
            new_map = {}
            for lib in libs_data:
                info = lib.get('info', {})
                l_id = str(info.get('Id'))
                l_guid = str(info.get('Guid'))
                if l_id and l_guid:
                    new_map[l_id] = l_guid
            
            self._global_lib_guid_map = new_map
            self._last_lib_map_update = time.time()
            logger.debug(f"  âœ å·²åˆ·æ–°åª’ä½“åº“ GUID æ˜ å°„è¡¨ï¼Œå…±åŠ è½½ {len(new_map)} ä¸ªåº“ã€‚")
        except Exception as e:
            logger.error(f"åˆ·æ–°åª’ä½“åº“ GUID æ˜ å°„å¤±è´¥: {e}")

    # --- å®æ—¶è·å–é¡¹ç›®çš„ç¥–å…ˆåœ°å›¾å’Œåº“ GUID ---
    def _get_realtime_ancestor_context(self, item_id: str, source_lib_id: str) -> Tuple[Dict[str, str], Optional[str]]:
        """
        å®æ—¶è·å–é¡¹ç›®çš„ç¥–å…ˆåœ°å›¾å’Œåº“ GUIDã€‚
        """
        id_to_parent_map = {}
        # 1. è·å– GUID æ˜ å°„ (ä¿æŒä¸å˜)
        if not self._global_lib_guid_map or (time.time() - self._last_lib_map_update > 3600):
            self._refresh_lib_guid_map()
        lib_guid = self._global_lib_guid_map.get(str(source_lib_id))

        # 3. å‘ä¸Šçˆ¬æ ‘æ„å»ºçˆ¶å­å…³ç³»ï¼ˆç”¨äºè®¡ç®— ancestor_idsï¼‰
        try:
            curr_id = item_id
            for _ in range(10):
                # å®æ—¶å…¥åº“åªéœ€è¦ ParentId å³å¯ï¼Œä¸éœ€è¦å†è¯·æ±‚ Guid å­—æ®µ
                details = emby.get_emby_item_details(
                    curr_id, 
                    self.emby_url, 
                    self.emby_api_key, 
                    self.emby_user_id,
                    fields="ParentId",
                    silent_404=True
                )
                if not details: break
                
                p_id = details.get('ParentId')
                if p_id == str(source_lib_id) and lib_guid:
                    # æ„é€  Emby ç‰¹æœ‰çš„å¤åˆ ID: GUID_æ•°å­—ID
                    composite_id = f"{lib_guid}_{p_id}"
                    id_to_parent_map[curr_id] = composite_id
                    # å¤åˆ ID çš„çˆ¶çº§æ˜¯ç³»ç»Ÿæ ¹èŠ‚ç‚¹ "1"
                    id_to_parent_map[composite_id] = "1"
                    break 
                
                if p_id and p_id != '1':
                    id_to_parent_map[str(curr_id)] = p_id
                    curr_id = p_id
                else:
                    break
        except Exception as e:
            logger.error(f"å®æ—¶æ„å»ºçˆ¬æ ‘åœ°å›¾å¤±è´¥: {e}")

        return id_to_parent_map, lib_guid

    # --- æ›´æ–°åª’ä½“å…ƒæ•°æ®ç¼“å­˜ ---
    def _upsert_media_metadata(
        self,
        cursor: psycopg2.extensions.cursor,
        item_type: str,
        final_processed_cast: List[Dict[str, Any]],
        source_data_package: Optional[Dict[str, Any]],
        item_details_from_emby: Optional[Dict[str, Any]] = None
    ):
        """
        - å®æ—¶å…ƒæ•°æ®å†™å…¥ (ç»ˆæç¨³å¥ç‰ˆ)ã€‚
        - å…¼å®¹ 'pending' é¢„å¤„ç†æ¨¡å¼å’Œ 'webhook' å›æµæ¨¡å¼ã€‚
        - ä¿®å¤äº† ID=0 çš„è„æ•°æ®é—®é¢˜ã€‚
        - ä¿®å¤äº†å›æµæ—¶å› ç±»å‹ä¸åŒ¹é…å¯¼è‡´æ— æ³•æ ‡è®°å…¥åº“çš„é—®é¢˜ã€‚
        - ã€ä¿®æ”¹ã€‘åˆ†é›†å¤„ç†é€»è¾‘ç®€åŒ–ï¼šåªå†™å…¥ä¸»ç‰ˆæœ¬æ•°æ®ï¼Œä¸å†èšåˆå¤šç‰ˆæœ¬ï¼Œé˜²æ­¢å®æ—¶å¤„ç†æ±¡æŸ“æ•°æ®ã€‚
        """
        # =========================================================
        # âœ¨âœ¨âœ¨ [é­”æ³•æ—¥å¿—] START âœ¨âœ¨âœ¨
        # =========================================================
        # try:
        #     if item_details_from_emby:
        #         # ä½¿ç”¨ default=str å¤„ç† datetime ç­‰æ— æ³• JSON åºåˆ—åŒ–çš„å¯¹è±¡
        #         debug_json = json.dumps(item_details_from_emby, ensure_ascii=False, indent=2, default=str)
        #         logger.info(f"\nğŸ”®ğŸ”®ğŸ”® [Magic Log] è¿›å…¥ _upsert_media_metadata ğŸ”®ğŸ”®ğŸ”®\n"
        #                     f"Item Type: {item_type}\n"
        #                     f"Content of item_details_from_emby:\n{debug_json}\n"
        #                     f"ğŸ”®ğŸ”®ğŸ”® [Magic Log End] ğŸ”®ğŸ”®ğŸ”®")
        #     else:
        #         logger.info(f"\nğŸ”®ğŸ”®ğŸ”® [Magic Log] è¿›å…¥ _upsert_media_metadata ğŸ”®ğŸ”®ğŸ”®\n"
        #                     f"Item Type: {item_type}\n"
        #                     f"âš ï¸ item_details_from_emby IS NONE OR EMPTY\n"
        #                     f"ğŸ”®ğŸ”®ğŸ”® [Magic Log End] ğŸ”®ğŸ”®ğŸ”®")
        # except Exception as e:
        #     logger.error(f"ğŸ”® [Magic Log] åºåˆ—åŒ–æ—¥å¿—æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        # =========================================================
        # âœ¨âœ¨âœ¨ [é­”æ³•æ—¥å¿—] END âœ¨âœ¨âœ¨
        # =========================================================
        if not item_details_from_emby:
            logger.error("  âœ å†™å…¥å…ƒæ•°æ®ç¼“å­˜å¤±è´¥ï¼šç¼ºå°‘ Emby è¯¦æƒ…æ•°æ®ã€‚")
            return
            
        item_id = str(item_details_from_emby.get('Id'))
        # æ ¸å¿ƒåˆ¤æ–­ï¼šæ˜¯å¦ä¸ºé¢„å¤„ç†/ä¸»åŠ¨ç›‘æ§æ¨¡å¼
        is_pending = (item_id == 'pending')

        # åˆå§‹åŒ–å˜é‡
        source_lib_id = ""
        id_to_parent_map = {}
        lib_guid = None
        
        # åªæœ‰åœ¨ä¸æ˜¯ pending çŠ¶æ€ä¸‹ï¼Œæ‰å»è®¡ç®—ç¥–å…ˆé“¾å’Œåº“ä¿¡æ¯
        if not is_pending:
            source_lib_id = str(item_details_from_emby.get('_SourceLibraryId') or "")
            id_to_parent_map, lib_guid = self._get_realtime_ancestor_context(item_id, source_lib_id)

        def get_representative_runtime(emby_items, tmdb_runtime):
            if not emby_items: return tmdb_runtime
            runtimes = [round(item['RunTimeTicks'] / 600000000) for item in emby_items if item.get('RunTimeTicks')]
            return max(runtimes) if runtimes else tmdb_runtime
        
        def _extract_common_json_fields(details: Dict[str, Any], m_type: str):
            # 1. Genres (ç±»å‹)
            genres_raw = details.get('genres', [])
            genres_list = []
            for g in genres_raw:
                if isinstance(g, dict): 
                    # TMDb æ•°æ®ï¼Œæœ‰ ID
                    name = g.get('name')
                    if name in utils.GENRE_TRANSLATION_PATCH:
                        name = utils.GENRE_TRANSLATION_PATCH[name]
                    genres_list.append({"id": g.get('id', 0), "name": name})
                elif isinstance(g, str): 
                    # Emby æ•°æ®ï¼Œæ—  IDï¼Œé»˜è®¤ä¸º 0
                    name = g
                    if name in utils.GENRE_TRANSLATION_PATCH:
                        name = utils.GENRE_TRANSLATION_PATCH[name]
                    genres_list.append({"id": 0, "name": name})
            
            genres_json = json.dumps(genres_list, ensure_ascii=False)

            # 2. Studios (å·¥ä½œå®¤/åˆ¶ä½œå…¬å¸/ç”µè§†ç½‘)
            # å‰§é›†åªå– networksï¼Œç”µå½±åªå– production_companies 
            raw_studios = []
            if m_type == 'Series':
                # å‰§é›†ï¼šåªè¦æ’­å‡ºå¹³å° (Networks)ï¼Œä¸è¦åˆ¶ä½œå…¬å¸
                raw_studios = details.get('networks') or []
            else:
                # ç”µå½±ï¼šä¿ç•™åˆ¶ä½œå…¬å¸
                raw_studios = details.get('production_companies') or []
            
            if isinstance(raw_studios, list): 
                raw_studios = list(raw_studios)
            else: 
                raw_studios = []
            
            unique_studios_map = {}
            for s in raw_studios:
                if isinstance(s, dict):
                    s_id = s.get('id')
                    s_name = s.get('name')
                    if s_name: unique_studios_map[s_id] = {'id': s_id, 'name': s_name}
                elif isinstance(s, str) and s:
                    unique_studios_map[s] = {'id': None, 'name': s}
            studios_json = json.dumps(list(unique_studios_map.values()), ensure_ascii=False)

            # 3. Keywords (å…³é”®è¯)
            keywords_data = details.get('keywords') or details.get('tags') or []
            raw_k_list = []
            if isinstance(keywords_data, dict):
                if m_type == 'Series': raw_k_list = keywords_data.get('results')
                else: raw_k_list = keywords_data.get('keywords')
                if not raw_k_list: raw_k_list = keywords_data.get('results') or keywords_data.get('keywords') or []
            elif isinstance(keywords_data, list):
                raw_k_list = keywords_data
            
            keywords = []
            for k in raw_k_list:
                if isinstance(k, dict) and k.get('name'): keywords.append({'id': k.get('id'), 'name': k.get('name')})
                elif isinstance(k, str) and k: keywords.append({'id': None, 'name': k})
            keywords_json = json.dumps(keywords, ensure_ascii=False)

            # 4. Countries (å›½å®¶)
            countries_raw = details.get('production_countries') or details.get('origin_country') or []
            country_codes = []
            for c in countries_raw:
                if isinstance(c, dict): 
                    code = c.get('iso_3166_1')
                    if code: country_codes.append(code)
                elif isinstance(c, str) and c: country_codes.append(c)
            countries_json = json.dumps(country_codes, ensure_ascii=False)
            return genres_json, studios_json, keywords_json, countries_json

        try:
            from psycopg2.extras import execute_batch
            
            if not source_data_package:
                logger.warning("  âœ å…ƒæ•°æ®å†™å…¥è·³è¿‡ï¼šæœªæä¾›æºæ•°æ®åŒ…ã€‚")
                return

            records_to_upsert = []

            # ç”Ÿæˆå‘é‡é€»è¾‘
            overview_embedding_json = None
            if item_type in ["Movie", "Series"] and self.ai_translator and self.config.get(constants.CONFIG_OPTION_AI_VECTOR, False):
                overview_text = source_data_package.get('overview') or item_details_from_emby.get('Overview')
                if overview_text:
                    try:
                        embedding = self.ai_translator.generate_embedding(overview_text)
                        if embedding: overview_embedding_json = json.dumps(embedding)
                    except Exception as e_embed:
                        logger.warning(f"  âœ ç”Ÿæˆå‘é‡å¤±è´¥: {e_embed}")
            
            # ==================================================================
            # å¤„ç†ç”µå½± (Movie)
            # ==================================================================
            if item_type == "Movie":
                movie_record = source_data_package.copy()
                movie_record['item_type'] = 'Movie'
                movie_record['tmdb_id'] = str(movie_record.get('id'))
                movie_record['runtime_minutes'] = get_representative_runtime([item_details_from_emby], movie_record.get('runtime'))
                movie_record['rating'] = movie_record.get('vote_average')
                
                # â˜… èµ„äº§ä¿¡æ¯å¤„ç†
                if is_pending:
                    movie_record['asset_details_json'] = '[]'
                    movie_record['emby_item_ids_json'] = '[]'
                    movie_record['in_library'] = False
                else:
                    asset_details = parse_full_asset_details(
                        item_details_from_emby, 
                        id_to_parent_map=id_to_parent_map, 
                        library_guid=lib_guid
                    )
                    asset_details['source_library_id'] = source_lib_id
                    movie_record['asset_details_json'] = json.dumps([asset_details], ensure_ascii=False)
                    movie_record['emby_item_ids_json'] = json.dumps([item_id])
                    movie_record['in_library'] = True

                movie_record['actors_json'] = json.dumps([{"tmdb_id": int(p.get("id")), "character": p.get("character"), "order": p.get("order")} for p in final_processed_cast if p.get("id")], ensure_ascii=False)
                movie_record['subscription_status'] = 'NONE'
                movie_record['date_added'] = item_details_from_emby.get("DateCreated") or datetime.now(timezone.utc)
                movie_record['overview_embedding'] = overview_embedding_json

                # é€šç”¨å­—æ®µ
                g_json, s_json, k_json, c_json = _extract_common_json_fields(source_data_package, 'Movie')
                movie_record['genres_json'] = g_json
                movie_record['studios_json'] = s_json
                movie_record['keywords_json'] = k_json
                movie_record['countries_json'] = c_json

                # åˆ†çº§å¤„ç†
                raw_ratings_map = {}
                results = source_data_package.get('release_dates', {}).get('results', [])
                if results:
                    for r in results:
                        country = r.get('iso_3166_1')
                        if not country: continue
                        cert = None
                        for release in r.get('release_dates', []):
                            if release.get('certification'):
                                cert = release.get('certification')
                                break
                        if cert: raw_ratings_map[country] = cert
                
                releases = source_data_package.get('releases', {}).get('countries', [])
                for r in releases:
                    country = r.get('iso_3166_1')
                    cert = r.get('certification')
                    if country and cert: raw_ratings_map[country] = cert
                
                movie_record['official_rating_json'] = json.dumps(raw_ratings_map, ensure_ascii=False)
                
                credits_data = source_data_package.get("credits") or source_data_package.get("casts") or {}
                crew = credits_data.get('crew', [])
                movie_record['directors_json'] = json.dumps([{'id': p.get('id'), 'name': p.get('name')} for p in crew if p.get('job') == 'Director'], ensure_ascii=False)

                records_to_upsert.append(movie_record)

            # ==================================================================
            # å¤„ç†å‰§é›† (Series)
            # ==================================================================
            elif item_type == "Series":
                series_details = source_data_package.get("series_details", source_data_package)
                seasons_details = source_data_package.get("seasons_details", series_details.get("seasons", []))
                
                series_asset_details = []
                # â˜… Pending æ¨¡å¼ä¸‹ä¸å¤„ç†èµ„äº§è·¯å¾„
                if not is_pending:
                    series_path = item_details_from_emby.get('Path')
                    if series_path:
                        series_asset = {
                            "path": series_path,
                            "source_library_id": source_lib_id,
                            "ancestor_ids": calculate_ancestor_ids(item_id, id_to_parent_map, lib_guid)
                        }
                        series_asset_details.append(series_asset)

                # æ„å»º Series è®°å½•
                series_record = {
                    "item_type": "Series", "tmdb_id": str(series_details.get('id')), "title": series_details.get('name'),
                    "original_title": series_details.get('original_name'), "overview": series_details.get('overview'),
                    "release_date": series_details.get('first_air_date'), "poster_path": series_details.get('poster_path'),
                    "rating": series_details.get('vote_average'),
                    "total_episodes": series_details.get('number_of_episodes', 0),
                    "watchlist_tmdb_status": series_details.get('status'),
                    "asset_details_json": json.dumps(series_asset_details, ensure_ascii=False),
                    "overview_embedding": overview_embedding_json
                }
                
                # â˜… çŠ¶æ€æ ‡è®°
                if is_pending:
                    series_record['in_library'] = False
                    series_record['emby_item_ids_json'] = '[]'
                else:
                    series_record['in_library'] = True
                    series_record['emby_item_ids_json'] = json.dumps([item_id])

                actors_relation = [{"tmdb_id": int(p.get("id")), "character": p.get("character"), "order": p.get("order")} for p in final_processed_cast if p.get("id")]
                series_record['actors_json'] = json.dumps(actors_relation, ensure_ascii=False)
                
                # åˆ†çº§
                raw_ratings_map = {}
                results = series_details.get('content_ratings', {}).get('results', [])
                for r in results:
                    country = r.get('iso_3166_1')
                    rating = r.get('rating')
                    if country and rating: raw_ratings_map[country] = rating
                series_record['official_rating_json'] = json.dumps(raw_ratings_map, ensure_ascii=False)

                # é€šç”¨å­—æ®µ
                g_json, s_json, k_json, c_json = _extract_common_json_fields(series_details, 'Series')
                series_record['genres_json'] = g_json
                series_record['studios_json'] = s_json
                series_record['keywords_json'] = k_json
                series_record['countries_json'] = c_json
                
                series_record['directors_json'] = json.dumps([{'id': c.get('id'), 'name': c.get('name')} for c in series_details.get('created_by', [])], ensure_ascii=False)
                
                languages_list = series_details.get('languages', [])
                series_record['original_language'] = series_details.get('original_language') or (languages_list[0] if languages_list else None)
                series_record['subscription_status'] = 'NONE'
                series_record['date_added'] = item_details_from_emby.get("DateCreated") or datetime.now(timezone.utc)
                series_record['ignore_reason'] = None
                records_to_upsert.append(series_record)

                # â˜…â˜…â˜… 3. å¤„ç†å­£ (Season) â˜…â˜…â˜…
                emby_season_versions = []
                # â˜… Pending æ¨¡å¼ä¸‹è·³è¿‡ Emby æŸ¥è¯¢
                if not is_pending:
                    emby_season_versions = emby.get_series_seasons(
                        series_id=item_details_from_emby.get('Id'),
                        base_url=self.emby_url,
                        api_key=self.emby_api_key,
                        user_id=self.emby_user_id,
                        series_name_for_log=series_details.get('name')
                    ) or []
                
                seasons_grouped_by_number = defaultdict(list)
                for s_ver in emby_season_versions:
                    # å¼ºåˆ¶è½¬ intï¼Œé˜²æ­¢ç±»å‹ä¸åŒ¹é…
                    idx = s_ver.get("IndexNumber")
                    if idx is not None:
                        try: seasons_grouped_by_number[int(idx)].append(s_ver)
                        except: pass

                for season in seasons_details:
                    if not isinstance(season, dict): continue
                    
                    # â˜…â˜…â˜… æ ¸å¿ƒä¿®å¤ï¼šä¸¥é˜²æ­»å®ˆ ID=0 â˜…â˜…â˜…
                    s_tmdb_id = season.get('id')
                    if not s_tmdb_id or str(s_tmdb_id) in ['0', 'None', '']:
                        continue

                    s_num = season.get('season_number')
                    if s_num is None: continue 
                    try: s_num_int = int(s_num)
                    except ValueError: continue

                    season_poster = season.get('poster_path') or series_details.get('poster_path')
                    matched_emby_seasons = seasons_grouped_by_number.get(s_num_int, [])

                    # â˜…â˜…â˜… ä¿®æ”¹ï¼šå­£ä¹Ÿåªå–ç¬¬ä¸€ä¸ªç‰ˆæœ¬ï¼Œä¿æŒé€»è¾‘ä¸€è‡´æ€§ â˜…â˜…â˜…
                    primary_season_id = matched_emby_seasons[0]['Id'] if matched_emby_seasons else None
                    
                    records_to_upsert.append({
                        "tmdb_id": str(s_tmdb_id), "item_type": "Season", 
                        "parent_series_tmdb_id": str(series_details.get('id')), 
                        "title": season.get('name'), "overview": season.get('overview'), 
                        "release_date": season.get('air_date'), "poster_path": season_poster, 
                        "season_number": s_num,
                        "total_episodes": season.get('episode_count', 0),
                        "in_library": bool(matched_emby_seasons) if not is_pending else False,
                        "emby_item_ids_json": json.dumps([primary_season_id]) if primary_season_id else '[]'
                    })
                
                # â˜…â˜…â˜… 4. å¤„ç†åˆ†é›† (Episode) â˜…â˜…â˜…
                raw_episodes = source_data_package.get("episodes_details", {})
                # å…¼å®¹å­—å…¸(S1E1: data)å’Œåˆ—è¡¨ä¸¤ç§æ ¼å¼
                episodes_details = list(raw_episodes.values()) if isinstance(raw_episodes, dict) else (raw_episodes if isinstance(raw_episodes, list) else [])
                
                emby_episode_versions = []
                if not is_pending:
                    emby_episode_versions = emby.get_all_library_versions(
                        base_url=self.emby_url, api_key=self.emby_api_key, user_id=self.emby_user_id,
                        media_type_filter="Episode", parent_id=item_details_from_emby.get('Id'),
                        fields="Id,Type,ParentIndexNumber,IndexNumber,MediaStreams,Container,Size,Path,ProviderIds,RunTimeTicks,DateCreated,_SourceLibraryId"
                    ) or []
                
                episodes_grouped_by_number = defaultdict(list)
                for ep_version in emby_episode_versions:
                    s_num = ep_version.get("ParentIndexNumber")
                    e_num = ep_version.get("IndexNumber")
                    if s_num is not None and e_num is not None:
                        try: episodes_grouped_by_number[(int(s_num), int(e_num))].append(ep_version)
                        except: pass

                for episode in episodes_details:
                    # â˜…â˜…â˜… æ ¸å¿ƒä¿®å¤ï¼šä¸¥é˜²æ­»å®ˆï¼Œåªè®¤ TMDb æ•°å­— ID â˜…â˜…â˜…
                    e_tmdb_id = episode.get('id')
                    
                    # 1. å¿…é¡»æœ‰ ID
                    if not e_tmdb_id: 
                        continue
                    
                    # 2. ID å¿…é¡»æ˜¯æ•°å­—å­—ç¬¦ä¸²ï¼Œä¸”ä¸èƒ½æ˜¯ '0'
                    e_tmdb_id_str = str(e_tmdb_id)
                    if e_tmdb_id_str in ['0', 'None', ''] or not e_tmdb_id_str.isdigit():
                        continue

                    # 3. å¿…é¡»æœ‰å­£å·å’Œé›†å·
                    if episode.get('episode_number') is None: continue
                    try:
                        s_num = int(episode.get('season_number'))
                        e_num = int(episode.get('episode_number'))
                    except (ValueError, TypeError): continue

                    versions_of_episode = episodes_grouped_by_number.get((s_num, e_num))
                    final_runtime = get_representative_runtime(versions_of_episode, episode.get('runtime'))

                    episode_record = {
                        "tmdb_id": e_tmdb_id_str, 
                        "item_type": "Episode", 
                        "parent_series_tmdb_id": str(series_details.get('id')), 
                        "title": episode.get('name'), "overview": episode.get('overview'), 
                        "release_date": episode.get('air_date'), 
                        "season_number": s_num, "episode_number": e_num,
                        "runtime_minutes": final_runtime
                    }
                    
                    if not is_pending and versions_of_episode:
                        # â˜…â˜…â˜… æ ¸å¿ƒä¿®æ”¹ï¼šåªå–ç¬¬ä¸€ä¸ªç‰ˆæœ¬ï¼ˆä¸»ç‰ˆæœ¬ï¼‰ï¼Œæ”¾å¼ƒå¤šç‰ˆæœ¬èšåˆ â˜…â˜…â˜…
                        primary_version = versions_of_episode[0]
                        
                        details = parse_full_asset_details(primary_version)
                        details['source_library_id'] = item_details_from_emby.get('_SourceLibraryId')
                        
                        episode_record['asset_details_json'] = json.dumps([details], ensure_ascii=False)
                        episode_record['emby_item_ids_json'] = json.dumps([primary_version.get('Id')])
                        episode_record['in_library'] = True
                    else:
                        episode_record['in_library'] = False
                        episode_record['emby_item_ids_json'] = '[]'
                        episode_record['asset_details_json'] = '[]'
                        
                    records_to_upsert.append(episode_record)

            if not records_to_upsert:
                return
            
            # ==================================================================
            # æ‰¹é‡å†™å…¥æ•°æ®åº“
            # ==================================================================
            all_possible_columns = [
                "tmdb_id", "item_type", "title", "original_title", "overview", "release_date", "release_year",
                "original_language",
                "poster_path", "rating", "actors_json", "parent_series_tmdb_id", "season_number", "episode_number",
                "in_library", "subscription_status", "subscription_sources_json", "emby_item_ids_json", "date_added",
                "official_rating_json",
                "genres_json", "directors_json", "studios_json", "countries_json", "keywords_json", "ignore_reason",
                "asset_details_json",
                "runtime_minutes",
                "overview_embedding",
                "total_episodes",
                "watchlist_tmdb_status"
            ]
            data_for_batch = []
            for record in records_to_upsert:
                # å†æ¬¡æ£€æŸ¥ IDï¼Œé˜²æ­¢æ¼ç½‘ä¹‹é±¼
                if not record.get('tmdb_id') or str(record.get('tmdb_id')) == '0':
                    continue

                db_row_complete = {col: record.get(col) for col in all_possible_columns}
                
                if db_row_complete['in_library'] is None: db_row_complete['in_library'] = False
                if db_row_complete['subscription_status'] is None: db_row_complete['subscription_status'] = 'NONE'
                if db_row_complete['subscription_sources_json'] is None: db_row_complete['subscription_sources_json'] = '[]'
                if db_row_complete['emby_item_ids_json'] is None: db_row_complete['emby_item_ids_json'] = '[]'

                r_date = db_row_complete.get('release_date')
                if not r_date: db_row_complete['release_date'] = None
                
                final_date_val = db_row_complete.get('release_date')
                if final_date_val and isinstance(final_date_val, str) and len(final_date_val) >= 4:
                    try: db_row_complete['release_year'] = int(final_date_val[:4])
                    except (ValueError, TypeError): pass
                
                data_for_batch.append(db_row_complete)

            if not data_for_batch:
                return

            cols_str = ", ".join(all_possible_columns)
            placeholders_str = ", ".join([f"%({col})s" for col in all_possible_columns])
            cols_to_update = [col for col in all_possible_columns if col not in ['tmdb_id', 'item_type', 'custom_rating']]
            
            cols_to_protect = ['subscription_sources_json']
            timestamp_field = "last_synced_at"
            
            for col in cols_to_protect:
                if col in cols_to_update: cols_to_update.remove(col)

            update_clauses = []
            for col in cols_to_update:
                # é’ˆå¯¹ total_episodes å­—æ®µï¼Œæ£€æŸ¥é”å®šçŠ¶æ€
                # é€»è¾‘ï¼šå¦‚æœ total_episodes_locked ä¸º TRUEï¼Œåˆ™ä¿æŒåŸå€¼ï¼›å¦åˆ™ä½¿ç”¨æ–°å€¼ (EXCLUDED.total_episodes)
                if col == 'total_episodes':
                    update_clauses.append(
                        "total_episodes = CASE WHEN media_metadata.total_episodes_locked IS TRUE THEN media_metadata.total_episodes ELSE EXCLUDED.total_episodes END"
                    )
                else:
                    # å…¶ä»–å­—æ®µæ­£å¸¸æ›´æ–°
                    update_clauses.append(f"{col} = EXCLUDED.{col}")

            update_clauses.append(f"{timestamp_field} = NOW()")

            sql = f"""
                INSERT INTO media_metadata ({cols_str})
                VALUES ({placeholders_str})
                ON CONFLICT (tmdb_id, item_type) DO UPDATE SET {', '.join(update_clauses)};
            """
            
            execute_batch(cursor, sql, data_for_batch)
            logger.info(f"  âœ æˆåŠŸå°† {len(data_for_batch)} æ¡å±‚çº§å…ƒæ•°æ®è®°å½•æ‰¹é‡å†™å…¥æ•°æ®åº“ã€‚")

        except Exception as e:
            logger.error(f"æ‰¹é‡å†™å…¥å±‚çº§å…ƒæ•°æ®åˆ°æ•°æ®åº“æ—¶å¤±è´¥: {e}", exc_info=True)
            raise

    # --- æ ‡è®°ä¸ºå·²å¤„ç† ---
    def _mark_item_as_processed(self, cursor: psycopg2.extensions.cursor, item_id: str, item_name: str, score: float = 10.0):
        """
        ã€é‡æ„ã€‘å°†ä¸€ä¸ªé¡¹ç›®æ ‡è®°ä¸ºâ€œå·²å¤„ç†â€çš„å”¯ä¸€å®˜æ–¹æ–¹æ³•ã€‚
        å®ƒä¼šåŒæ—¶æ›´æ–°æ•°æ®åº“å’Œå†…å­˜ç¼“å­˜ï¼Œç¡®ä¿æ•°æ®ä¸€è‡´æ€§ã€‚
        """
        # 1. æ›´æ–°æ•°æ®åº“
        self.log_db_manager.save_to_processed_log(cursor, item_id, item_name, score=score)
        
        # 2. å®æ—¶æ›´æ–°å†…å­˜ç¼“å­˜
        self.processed_items_cache[item_id] = item_name
        
        logger.debug(f"  âœ å·²å°† '{item_name}' æ ‡è®°ä¸ºå·²å¤„ç† (æ•°æ®åº“ & å†…å­˜)ã€‚")
    # --- æ¸…é™¤å·²å¤„ç†è®°å½• ---
    def clear_processed_log(self):
        """
        ã€å·²æ”¹é€ ã€‘æ¸…é™¤æ•°æ®åº“å’Œå†…å­˜ä¸­çš„å·²å¤„ç†è®°å½•ã€‚
        ä½¿ç”¨ä¸­å¤®æ•°æ®åº“è¿æ¥å‡½æ•°ã€‚
        """
        try:
            # 1. â˜…â˜…â˜… è°ƒç”¨ä¸­å¤®å‡½æ•° â˜…â˜…â˜…
            with get_central_db_connection() as conn:
                cursor = conn.cursor()
                
                logger.debug("æ­£åœ¨ä»æ•°æ®åº“åˆ é™¤ processed_log è¡¨ä¸­çš„æ‰€æœ‰è®°å½•...")
                cursor.execute("DELETE FROM processed_log")
                # with è¯­å¥ä¼šè‡ªåŠ¨å¤„ç† conn.commit()
            
            logger.info("  âœ æ•°æ®åº“ä¸­çš„å·²å¤„ç†è®°å½•å·²æ¸…é™¤ã€‚")

            # 2. æ¸…ç©ºå†…å­˜ç¼“å­˜
            self.processed_items_cache.clear()
            logger.info("  âœ å†…å­˜ä¸­çš„å·²å¤„ç†è®°å½•ç¼“å­˜å·²æ¸…é™¤ã€‚")

        except Exception as e:
            logger.error(f"æ¸…é™¤æ•°æ®åº“æˆ–å†…å­˜å·²å¤„ç†è®°å½•æ—¶å¤±è´¥: {e}", exc_info=True)
            # 3. â˜…â˜…â˜… é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œé€šçŸ¥ä¸Šæ¸¸è°ƒç”¨è€…æ“ä½œå¤±è´¥ â˜…â˜…â˜…
            raise
    
    # å…¬å¼€çš„ã€ç‹¬ç«‹çš„è¿½å‰§åˆ¤æ–­æ–¹æ³•
    def check_and_add_to_watchlist(self, item_details: Dict[str, Any]):
        """
        æ£€æŸ¥ä¸€ä¸ªåª’ä½“é¡¹ç›®æ˜¯å¦ä¸ºå‰§é›†ï¼Œå¦‚æœæ˜¯ï¼Œåˆ™æ‰§è¡Œæ™ºèƒ½è¿½å‰§åˆ¤æ–­å¹¶æ·»åŠ åˆ°å¾…çœ‹åˆ—è¡¨ã€‚
        æ­¤æ–¹æ³•è¢«è®¾è®¡ä¸ºç”±å¤–éƒ¨äº‹ä»¶ï¼ˆå¦‚Webhookï¼‰æ˜¾å¼è°ƒç”¨ã€‚
        """
        item_name_for_log = item_details.get("Name", f"æœªçŸ¥é¡¹ç›®(ID:{item_details.get('Id')})")
        
        if item_details.get("Type") != "Series":
            # å¦‚æœä¸æ˜¯å‰§é›†ï¼Œç›´æ¥è¿”å›ï¼Œä¸æ‰“å°éå¿…è¦çš„æ—¥å¿—
            return

        logger.info(f"  âœ å¼€å§‹ä¸ºæ–°å…¥åº“å‰§é›† '{item_name_for_log}' è¿›è¡Œè¿½å‰§çŠ¶æ€åˆ¤æ–­...")
        try:
            # å®ä¾‹åŒ– WatchlistProcessor å¹¶æ‰§è¡Œæ·»åŠ æ“ä½œ
            watchlist_proc = WatchlistProcessor(self.config)
            watchlist_proc.add_series_to_watchlist(item_details)
        except Exception as e_watchlist:
            logger.error(f"  âœ åœ¨è‡ªåŠ¨æ·»åŠ  '{item_name_for_log}' åˆ°è¿½å‰§åˆ—è¡¨æ—¶å‘ç”Ÿé”™è¯¯: {e_watchlist}", exc_info=True)

    def signal_stop(self):
        self._stop_event.set()

    def clear_stop_signal(self):
        self._stop_event.clear()

    def get_stop_event(self) -> threading.Event:
        """è¿”å›å†…éƒ¨çš„åœæ­¢äº‹ä»¶å¯¹è±¡ï¼Œä»¥ä¾¿ä¼ é€’ç»™å…¶ä»–å‡½æ•°ã€‚"""
        return self._stop_event

    def is_stop_requested(self) -> bool:
        return self._stop_event.is_set()

    def _load_processed_log_from_db(self) -> Dict[str, str]:
        log_dict = {}
        try:
            # 1. â˜…â˜…â˜… ä½¿ç”¨ with è¯­å¥å’Œä¸­å¤®å‡½æ•° â˜…â˜…â˜…
            with get_central_db_connection() as conn:
                cursor = conn.cursor()
                
                # 2. æ‰§è¡ŒæŸ¥è¯¢
                cursor.execute("SELECT item_id, item_name FROM processed_log")
                rows = cursor.fetchall()
                
                # 3. å¤„ç†ç»“æœ
                for row in rows:
                    if row['item_id'] and row['item_name']:
                        log_dict[row['item_id']] = row['item_name']
            
            # 4. with è¯­å¥ä¼šè‡ªåŠ¨å¤„ç†æ‰€æœ‰äº‹æƒ…ï¼Œä»£ç å¹²å‡€åˆ©è½ï¼

        except Exception as e:
            # 5. â˜…â˜…â˜… è®°å½•æ›´è¯¦ç»†çš„å¼‚å¸¸ä¿¡æ¯ â˜…â˜…â˜…
            logger.error(f"ä»æ•°æ®åº“è¯»å–å·²å¤„ç†è®°å½•å¤±è´¥: {e}", exc_info=True)
        return log_dict

    # åœ¨æœ¬åœ°ç¼“å­˜ä¸­æŸ¥æ‰¾è±†ç“£JSONæ–‡ä»¶
    def _find_local_douban_json(self, imdb_id: Optional[str], douban_id: Optional[str], douban_cache_dir: str) -> Optional[str]:
        """æ ¹æ® IMDb ID æˆ– è±†ç“£ ID åœ¨æœ¬åœ°ç¼“å­˜ç›®å½•ä¸­æŸ¥æ‰¾å¯¹åº”çš„è±†ç“£JSONæ–‡ä»¶ã€‚"""
        if not os.path.exists(douban_cache_dir):
            return None
        
        # ä¼˜å…ˆä½¿ç”¨ IMDb ID åŒ¹é…ï¼Œæ›´å‡†ç¡®
        if imdb_id:
            for dirname in os.listdir(douban_cache_dir):
                if dirname.startswith('0_'): continue
                if imdb_id in dirname:
                    dir_path = os.path.join(douban_cache_dir, dirname)
                    for filename in os.listdir(dir_path):
                        if filename.endswith('.json'):
                            return os.path.join(dir_path, filename)
                            
        # å…¶æ¬¡ä½¿ç”¨è±†ç“£ ID åŒ¹é…
        if douban_id:
            for dirname in os.listdir(douban_cache_dir):
                if dirname.startswith(f"{douban_id}_"):
                    dir_path = os.path.join(douban_cache_dir, dirname)
                    for filename in os.listdir(dir_path):
                        if filename.endswith('.json'):
                            return os.path.join(dir_path, filename)
        return None

    # âœ¨ å°è£…äº†â€œä¼˜å…ˆæœ¬åœ°ç¼“å­˜ï¼Œå¤±è´¥åˆ™åœ¨çº¿è·å–â€çš„é€»è¾‘
    def _get_douban_data_with_local_cache(self, media_info: Dict[str, Any]) -> Tuple[List[Dict[str, Any]], Optional[float]]:
        """
        ã€V3 - æœ€ç»ˆç‰ˆã€‘è·å–è±†ç“£æ•°æ®ï¼ˆæ¼”å‘˜+è¯„åˆ†ï¼‰ã€‚ä¼˜å…ˆæœ¬åœ°ç¼“å­˜ï¼Œå¤±è´¥åˆ™å›é€€åˆ°åŠŸèƒ½å®Œæ•´çš„åœ¨çº¿APIè·¯å¾„ã€‚
        è¿”å›: (æ¼”å‘˜åˆ—è¡¨, è±†ç“£è¯„åˆ†) çš„å…ƒç»„ã€‚
        """
        # 1. å‡†å¤‡æŸ¥æ‰¾æ‰€éœ€çš„ä¿¡æ¯
        provider_ids = media_info.get("ProviderIds", {})
        item_name = media_info.get("Name", "")
        imdb_id = provider_ids.get("Imdb")
        douban_id_from_provider = provider_ids.get("Douban")
        item_type = media_info.get("Type")
        item_year = str(media_info.get("ProductionYear", ""))

        # 2. å°è¯•ä»æœ¬åœ°ç¼“å­˜æŸ¥æ‰¾
        douban_cache_dir_name = "douban-movies" if item_type == "Movie" else "douban-tv"
        douban_cache_path = os.path.join(self.local_data_path, "cache", douban_cache_dir_name)
        local_json_path = self._find_local_douban_json(imdb_id, douban_id_from_provider, douban_cache_path)

        if local_json_path:
            logger.debug(f"  âœ å‘ç°æœ¬åœ°è±†ç“£ç¼“å­˜æ–‡ä»¶ï¼Œå°†ç›´æ¥ä½¿ç”¨: {local_json_path}")
            douban_data = _read_local_json(local_json_path)
            if douban_data:
                cast = douban_data.get('actors', [])
                rating_str = douban_data.get("rating", {}).get("value")
                rating_float = None
                if rating_str:
                    try: rating_float = float(rating_str)
                    except (ValueError, TypeError): pass
                return cast, rating_float
            else:
                logger.warning(f"æœ¬åœ°è±†ç“£ç¼“å­˜æ–‡ä»¶ '{local_json_path}' æ— æ•ˆï¼Œå°†å›é€€åˆ°åœ¨çº¿APIã€‚")

        # æ£€æŸ¥æ˜¯å¦å¯ç”¨äº†åœ¨çº¿API 
        if not self.config.get(constants.CONFIG_OPTION_DOUBAN_ENABLE_ONLINE_API, True):
            logger.info("  âœ æœªæ‰¾åˆ°æœ¬åœ°è±†ç“£ç¼“å­˜ï¼Œä¸”åœ¨çº¿è±†ç“£APIå·²ç¦ç”¨ï¼Œè·³è¿‡è±†ç“£æ•°æ®è·å–ã€‚")
            return [], None
        
        # 3. å¦‚æœæœ¬åœ°æœªæ‰¾åˆ°ï¼Œå›é€€åˆ°åŠŸèƒ½å®Œæ•´çš„åœ¨çº¿APIè·¯å¾„
        logger.info("  âœ æœªæ‰¾åˆ°æœ¬åœ°è±†ç“£ç¼“å­˜ï¼Œå°†é€šè¿‡åœ¨çº¿APIè·å–æ¼”å‘˜ä¿¡æ¯ã€‚")

        # 3.1 åŒ¹é…è±†ç“£IDå’Œç±»å‹ã€‚ç°åœ¨ match_info è¿”å›çš„ç»“æœæ˜¯å®Œå…¨å¯ä¿¡çš„ã€‚
        match_info_result = self.douban_api.match_info(
            name=item_name, imdbid=imdb_id, mtype=item_type, year=item_year
        )

        if match_info_result.get("error") or not match_info_result.get("id"):
            logger.warning(f"  âŒ åœ¨çº¿åŒ¹é…è±†ç“£IDå¤±è´¥ for '{item_name}': {match_info_result.get('message', 'æœªæ‰¾åˆ°ID')}")
            return [], None

        douban_id = match_info_result["id"]
        # âœ¨âœ¨âœ¨ ç›´æ¥ä¿¡ä»»ä» douban.py è¿”å›çš„ç±»å‹ âœ¨âœ¨âœ¨
        douban_type = match_info_result.get("type")

        if not douban_type:
            logger.error(f"  ğŸš« ä»è±†ç“£åŒ¹é…ç»“æœä¸­æœªèƒ½è·å–åˆ°åª’ä½“ç±»å‹ for ID {douban_id}ã€‚å¤„ç†ä¸­æ­¢ã€‚")
            return [], None

        # 3.2 è·å–æ¼”èŒå‘˜ (ä½¿ç”¨å®Œå…¨å¯ä¿¡çš„ç±»å‹)
        cast_data = self.douban_api.get_acting(
            name=item_name, 
            douban_id_override=douban_id, 
            mtype=douban_type
        )
        douban_cast_raw = cast_data.get("cast", [])

        return douban_cast_raw, None
    
    # --- é€šè¿‡TmdbIDæŸ¥æ‰¾æ˜ å°„è¡¨ ---
    def _find_person_in_map_by_tmdb_id(self, tmdb_id: str, cursor: psycopg2.extensions.cursor) -> Optional[Dict[str, Any]]:
        """
        æ ¹æ® TMDB ID åœ¨ person_identity_map è¡¨ä¸­æŸ¥æ‰¾å¯¹åº”çš„è®°å½•ã€‚
        """
        if not tmdb_id:
            return None
        try:
            cursor.execute(
                "SELECT * FROM person_identity_map WHERE tmdb_person_id = %s",
                (tmdb_id,)
            )
            return cursor.fetchone()
        except psycopg2.Error as e:
            logger.error(f"é€šè¿‡ TMDB ID '{tmdb_id}' æŸ¥è¯¢ person_identity_map æ—¶å‡ºé”™: {e}")
            return None
    
    # --- é€šè¿‡ API æ›´æ–° Emby ä¸­æ¼”å‘˜åå­— ---
    def _update_emby_person_names_from_final_cast(self, final_cast: List[Dict[str, Any]], item_name_for_log: str):
        """
        æ ¹æ®æœ€ç»ˆå¤„ç†å¥½çš„æ¼”å‘˜åˆ—è¡¨ï¼Œé€šè¿‡ API æ›´æ–° Emby ä¸­â€œæ¼”å‘˜â€é¡¹ç›®çš„åå­—ã€‚
        """
        actors_to_update = [
            actor for actor in final_cast 
            if actor.get("emby_person_id") and utils.contains_chinese(actor.get("name"))
        ]

        if not actors_to_update:
            logger.info(f"  âœ æ— éœ€é€šè¿‡ API æ›´æ–°æ¼”å‘˜åå­— (æ²¡æœ‰æ‰¾åˆ°éœ€è¦ç¿»è¯‘çš„ Emby æ¼”å‘˜)ã€‚")
            return

        logger.info(f"  âœ å¼€å§‹ä¸ºã€Š{item_name_for_log}ã€‹çš„ {len(actors_to_update)} ä½æ¼”å‘˜é€šè¿‡ API æ›´æ–°åå­—...")
        
        # æ‰¹é‡è·å–è¿™äº›æ¼”å‘˜åœ¨ Emby ä¸­çš„å½“å‰ä¿¡æ¯ï¼Œä»¥å‡å°‘ API è¯·æ±‚
        person_ids = [actor["emby_person_id"] for actor in actors_to_update]
        current_person_details = emby.get_emby_items_by_id(
            base_url=self.emby_url,
            api_key=self.emby_api_key,
            user_id=self.emby_user_id,
            item_ids=person_ids,
            fields="Name"
        )
        
        current_names_map = {p["Id"]: p.get("Name") for p in current_person_details} if current_person_details else {}

        updated_count = 0
        for actor in actors_to_update:
            person_id = actor["emby_person_id"]
            new_name = actor["name"]
            current_name = current_names_map.get(person_id)

            # åªæœ‰å½“æ–°åå­—å’Œå½“å‰åå­—ä¸åŒæ—¶ï¼Œæ‰æ‰§è¡Œæ›´æ–°
            if new_name != current_name:
                emby.update_person_details(
                    person_id=person_id,
                    new_data={"Name": new_name},
                    emby_server_url=self.emby_url,
                    emby_api_key=self.emby_api_key,
                    user_id=self.emby_user_id
                )
                updated_count += 1
                # åŠ ä¸ªå°å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«
                time.sleep(0.2) 

        logger.info(f"  âœ æˆåŠŸé€šè¿‡ API æ›´æ–°äº† {updated_count} ä½æ¼”å‘˜çš„åå­—ã€‚")
    
    # --- å…¨é‡å¤„ç†çš„å…¥å£ ---
    def process_full_library(self, update_status_callback: Optional[callable] = None, force_full_update: bool = False):
        """
        å…¨é‡å¤„ç†çš„å…¥å£ã€‚
        """
        self.clear_stop_signal()
        
        logger.trace(f"è¿›å…¥æ ¸å¿ƒæ‰§è¡Œå±‚: process_full_library, æ¥æ”¶åˆ°çš„ force_full_update = {force_full_update}")

        if force_full_update:
            logger.info("  âœ æ£€æµ‹åˆ°â€œæ·±åº¦æ›´æ–°â€æ¨¡å¼ï¼Œæ­£åœ¨æ¸…ç©ºå·²å¤„ç†æ—¥å¿—...")
            try:
                self.clear_processed_log()
            except Exception as e:
                logger.error(f"åœ¨ process_full_library ä¸­æ¸…ç©ºæ—¥å¿—å¤±è´¥: {e}", exc_info=True)
                if update_status_callback: update_status_callback(-1, "æ¸…ç©ºæ—¥å¿—å¤±è´¥")
                return

        libs_to_process_ids = self.config.get("libraries_to_process", [])
        if not libs_to_process_ids:
            logger.warning("  âœ æœªåœ¨é…ç½®ä¸­æŒ‡å®šè¦å¤„ç†çš„åª’ä½“åº“ã€‚")
            return

        logger.info("  âœ æ­£åœ¨å°è¯•ä»Embyè·å–åª’ä½“é¡¹ç›®...")
        all_emby_libraries = emby.get_emby_libraries(self.emby_url, self.emby_api_key, self.emby_user_id) or []
        library_name_map = {lib.get('Id'): lib.get('Name', 'æœªçŸ¥åº“å') for lib in all_emby_libraries}
        
        movies = emby.get_emby_library_items(self.emby_url, self.emby_api_key, "Movie", self.emby_user_id, libs_to_process_ids, library_name_map=library_name_map) or []
        series = emby.get_emby_library_items(self.emby_url, self.emby_api_key, "Series", self.emby_user_id, libs_to_process_ids, library_name_map=library_name_map) or []
        
        if movies:
            source_movie_lib_names = sorted(list({library_name_map.get(item.get('_SourceLibraryId')) for item in movies if item.get('_SourceLibraryId')}))
            logger.info(f"  âœ ä»åª’ä½“åº“ã€{', '.join(source_movie_lib_names)}ã€‘è·å–åˆ° {len(movies)} ä¸ªç”µå½±é¡¹ç›®ã€‚")

        if series:
            source_series_lib_names = sorted(list({library_name_map.get(item.get('_SourceLibraryId')) for item in series if item.get('_SourceLibraryId')}))
            logger.info(f"  âœ ä»åª’ä½“åº“ã€{', '.join(source_series_lib_names)}ã€‘è·å–åˆ° {len(series)} ä¸ªç”µè§†å‰§é¡¹ç›®ã€‚")

        all_items = movies + series
        total = len(all_items)
        
        if total == 0:
            logger.info("  âœ åœ¨æ‰€æœ‰é€‰å®šçš„åº“ä¸­æœªæ‰¾åˆ°ä»»ä½•å¯å¤„ç†çš„é¡¹ç›®ã€‚")
            if update_status_callback: update_status_callback(100, "æœªæ‰¾åˆ°å¯å¤„ç†çš„é¡¹ç›®ã€‚")
            return

        # --- æ¸…ç†å·²åˆ é™¤çš„åª’ä½“é¡¹ ---
        if update_status_callback: update_status_callback(20, "æ­£åœ¨æ£€æŸ¥å¹¶æ¸…ç†å·²åˆ é™¤çš„åª’ä½“é¡¹...")
        
        with get_central_db_connection() as conn:
            cursor = conn.cursor()
            cursor.execute("SELECT item_id, item_name FROM processed_log")
            processed_log_entries = cursor.fetchall()
            
            processed_ids_in_db = {entry['item_id'] for entry in processed_log_entries}
            emby_ids_in_library = {item.get('Id') for item in all_items if item.get('Id')}
            
            # æ‰¾å‡ºåœ¨ processed_log ä¸­ä½†ä¸åœ¨ Emby åª’ä½“åº“ä¸­çš„é¡¹ç›®
            deleted_items_to_clean = processed_ids_in_db - emby_ids_in_library
            
            if deleted_items_to_clean:
                logger.info(f"  âœ å‘ç° {len(deleted_items_to_clean)} ä¸ªå·²ä» Emby åª’ä½“åº“åˆ é™¤çš„é¡¹ç›®ï¼Œæ­£åœ¨ä» 'å·²å¤„ç†' ä¸­ç§»é™¤...")
                for deleted_item_id in deleted_items_to_clean:
                    self.log_db_manager.remove_from_processed_log(cursor, deleted_item_id)
                    # åŒæ—¶ä»å†…å­˜ç¼“å­˜ä¸­ç§»é™¤
                    if deleted_item_id in self.processed_items_cache:
                        del self.processed_items_cache[deleted_item_id]
                    logger.debug(f"  âœ å·²ä» 'å·²å¤„ç†' ä¸­ç§»é™¤ ItemID: {deleted_item_id}")
                conn.commit()
                logger.info("  âœ å·²åˆ é™¤åª’ä½“é¡¹çš„æ¸…ç†å·¥ä½œå®Œæˆã€‚")
            else:
                logger.info("  âœ æœªå‘ç°éœ€è¦ä» 'å·²å¤„ç†' ä¸­æ¸…ç†çš„å·²åˆ é™¤åª’ä½“é¡¹ã€‚")
        
        if update_status_callback: update_status_callback(30, "å·²åˆ é™¤åª’ä½“é¡¹æ¸…ç†å®Œæˆï¼Œå¼€å§‹å¤„ç†ç°æœ‰åª’ä½“...")

        # --- ç°æœ‰åª’ä½“é¡¹å¤„ç†å¾ªç¯ ---
        for i, item in enumerate(all_items):
            if self.is_stop_requested():
                logger.warning("  ğŸš« å…¨åº“æ‰«æä»»åŠ¡å·²è¢«ç”¨æˆ·ä¸­æ­¢ã€‚")
                break # ä½¿ç”¨ break ä¼˜é›…åœ°é€€å‡ºå¾ªç¯
            
            item_id = item.get('Id')
            item_name = item.get('Name', f"ID:{item_id}")

            if not force_full_update and item_id in self.processed_items_cache:
                logger.info(f"  âœ æ­£åœ¨è·³è¿‡å·²å¤„ç†çš„é¡¹ç›®: {item_name}")
                if update_status_callback:
                    # è°ƒæ•´è¿›åº¦æ¡çš„èµ·å§‹ç‚¹ï¼Œä½¿å…¶åœ¨æ¸…ç†åä» 30% å¼€å§‹
                    progress_after_cleanup = 30
                    current_progress = progress_after_cleanup + int(((i + 1) / total) * (100 - progress_after_cleanup))
                    update_status_callback(current_progress, f"è·³è¿‡: {item_name}")
                continue

            if update_status_callback:
                progress_after_cleanup = 30
                current_progress = progress_after_cleanup + int(((i + 1) / total) * (100 - progress_after_cleanup))
                update_status_callback(current_progress, f"å¤„ç†ä¸­ ({i+1}/{total}): {item_name}")
            
            self.process_single_item(
                item_id, 
                force_full_update=force_full_update
            )
            
            time_module.sleep(float(self.config.get("delay_between_items_sec", 0.5)))
        
        if not self.is_stop_requested() and update_status_callback:
            update_status_callback(100, "å…¨é‡å¤„ç†å®Œæˆ")
    
    # --- æ ¸å¿ƒå¤„ç†æ€»ç®¡ ---
    def process_single_item(self, emby_item_id: str, force_full_update: bool = False, specific_episode_ids: Optional[List[str]] = None):
        """
        å…¥å£å‡½æ•°ï¼Œå®ƒä¼šå…ˆæ£€æŸ¥æ˜¯å¦éœ€è¦è·³è¿‡å·²å¤„ç†çš„é¡¹ç›®ã€‚
        """
        # 1. é™¤éå¼ºåˆ¶ï¼Œå¦åˆ™è·³è¿‡å·²å¤„ç†çš„
        if not force_full_update and not specific_episode_ids and emby_item_id in self.processed_items_cache:
            item_name_from_cache = self.processed_items_cache.get(emby_item_id, f"ID:{emby_item_id}")
            logger.info(f"åª’ä½“ '{item_name_from_cache}' è·³è¿‡å·²å¤„ç†è®°å½•ã€‚")
            return True

        # 2. æ£€æŸ¥åœæ­¢ä¿¡å·
        if self.is_stop_requested():
            return False

        # 3. è·å–Embyè¯¦æƒ…ï¼Œè¿™æ˜¯åç»­æ‰€æœ‰æ“ä½œçš„åŸºç¡€
        item_details = emby.get_emby_item_details(
            emby_item_id, self.emby_url, self.emby_api_key, self.emby_user_id
        )
        
        if not item_details:
            logger.error(f"process_single_item: æ— æ³•è·å– Emby é¡¹ç›® {emby_item_id} çš„è¯¦æƒ…ã€‚")
            return False
        
        # è¡¥å…¨ _SourceLibraryIdï¼šå› ä¸ºå•é¡¹è·å–æ¥å£ä¸åŒ…å«æ­¤å­—æ®µï¼Œéœ€é€šè¿‡è·¯å¾„åæŸ¥
        if not item_details.get('_SourceLibraryId'):
            lib_info = emby.get_library_root_for_item(
                item_id=emby_item_id,
                base_url=self.emby_url,
                api_key=self.emby_api_key,
                user_id=self.emby_user_id
            )
            if lib_info and lib_info.get('Id'):
                item_details['_SourceLibraryId'] = lib_info['Id']
                logger.debug(f"  âœ å·²ä¸º '{item_details.get('Name')}' è¡¥å…¨åª’ä½“åº“ID: {lib_info['Id']}")
            else:
                logger.warning(f"  âœ æ— æ³•ç¡®å®š '{item_details.get('Name')}' æ‰€å±çš„åª’ä½“åº“IDã€‚")

        # 4. å°†ä»»åŠ¡äº¤ç»™æ ¸å¿ƒå¤„ç†å‡½æ•°
        return self._process_item_core_logic(
            item_details_from_emby=item_details,
            force_full_update=force_full_update,
            specific_episode_ids=specific_episode_ids
        )

    # ---æ ¸å¿ƒå¤„ç†æµç¨‹ ---
    def _process_item_core_logic(self, item_details_from_emby: Dict[str, Any], force_full_update: bool = False, specific_episode_ids: Optional[List[str]] = None):
        """
        æœ¬å‡½æ•°ä½œä¸ºâ€œè®¾è®¡å¸ˆâ€ï¼Œåªè´Ÿè´£è®¡ç®—å’Œæ€è€ƒï¼Œäº§å‡ºâ€œè®¾è®¡å›¾â€å’Œâ€œç‰©æ–™æ¸…å•â€ï¼Œç„¶åå…¨æƒå§”æ‰˜ç»™æ–½å·¥é˜Ÿã€‚
        """
        # ======================================================================
        # é˜¶æ®µ 1: å‡†å¤‡å·¥ä½œ
        # ======================================================================
        item_id = item_details_from_emby.get("Id")
        item_name_for_log = item_details_from_emby.get("Name", f"æœªçŸ¥é¡¹ç›®(ID:{item_id})")
        tmdb_id = item_details_from_emby.get("ProviderIds", {}).get("Tmdb")
        item_type = item_details_from_emby.get("Type")

        logger.trace(f"--- å¼€å§‹å¤„ç† '{item_name_for_log}' (TMDb ID: {tmdb_id}) ---")

        all_emby_people_for_count = item_details_from_emby.get("People", [])
        original_emby_actor_count = len([p for p in all_emby_people_for_count if p.get("Type") == "Actor"])

        if not tmdb_id:
            logger.error(f"  âœ '{item_name_for_log}' ç¼ºå°‘ TMDb IDï¼Œæ— æ³•å¤„ç†ã€‚")
            return False
        if not self.local_data_path:
            logger.error(f"  âœ '{item_name_for_log}' å¤„ç†å¤±è´¥ï¼šæœªåœ¨é…ç½®ä¸­è®¾ç½®â€œæœ¬åœ°æ•°æ®æºè·¯å¾„â€ã€‚")
            return False
        
        try:
            authoritative_cast_source = []
            tmdb_details_for_extra = None # ç”¨äºå†…éƒ¨ç¼“å­˜

            # =========================================================
            # â˜…â˜…â˜… æ­¥éª¤ 1: ç¡®å®šå…ƒæ•°æ®éª¨æ¶ â˜…â˜…â˜…
            # =========================================================
            logger.info(f"  âœ æ­£åœ¨æ„å»ºæ ‡å‡†å…ƒæ•°æ®éª¨æ¶...")
            
            # 1. åˆå§‹åŒ–éª¨æ¶
            if item_type == "Movie":
                tmdb_details_for_extra = json.loads(json.dumps(utils.MOVIE_SKELETON_TEMPLATE))
            elif item_type == "Series":
                tmdb_details_for_extra = json.loads(json.dumps(utils.SERIES_SKELETON_TEMPLATE))
            
            # 2. è·å–æ•°æ®æº (TMDb API æˆ– æœ¬åœ°ç¼“å­˜)
            fresh_data = None
            aggregated_tmdb_data = None # ä¸“é—¨ç”¨äºå‰§é›†

            if self.tmdb_api_key:
                try:
                    if item_type == "Movie":
                        fresh_data = tmdb.get_movie_details(tmdb_id, self.tmdb_api_key)
                        if fresh_data: logger.info(f"  âœ æˆåŠŸä» TMDb API è·å–åˆ°æœ€æ–°ç”µå½±å…ƒæ•°æ®ã€‚")

                    elif item_type == "Series":
                        aggregated_tmdb_data = tmdb.aggregate_full_series_data_from_tmdb(int(tmdb_id), self.tmdb_api_key)
                        if aggregated_tmdb_data:
                            fresh_data = aggregated_tmdb_data.get("series_details")
                            logger.info(f"  âœ æˆåŠŸä» TMDb API è·å–åˆ°æœ€æ–°å‰§é›†èšåˆæ•°æ®ã€‚")

                except Exception as e:
                    logger.warning(f"  âœ ä» TMDb API è·å–æ•°æ®å¤±è´¥: {e}")

            # 4. å¡«å……éª¨æ¶ (Data Mapping)
            if fresh_data:
                # --- A. åŸºç¡€å­—æ®µç›´æ¥è¦†ç›– (é€šç”¨) ---
                tmdb_details_for_extra = construct_metadata_payload(
                    item_type=item_type,
                    tmdb_data=fresh_data,
                    aggregated_tmdb_data=aggregated_tmdb_data,
                    emby_data_fallback=item_details_from_emby
                )
                
                #  å¦‚æœ Emby å°šæœªæœ‰ç±»å‹æ•°æ®ï¼Œä½¿ç”¨ TMDb æ•°æ®è¡¥å…¨ï¼Œç¡®ä¿åç»­åŠ¨ç”»åˆ¤æ–­å‡†ç¡® 
                if not item_details_from_emby.get("Genres") and fresh_data.get("genres"):
                    item_details_from_emby["Genres"] = fresh_data.get("genres")
                    logger.debug(f"  âœ æ£€æµ‹åˆ° Emby ç¼ºå°‘ç±»å‹æ•°æ®ï¼Œå·²ä½¿ç”¨ TMDb æ•°æ®è¡¥å…¨ Genres: {len(fresh_data.get('genres'))} ä¸ª")

                # --- é‡æ–°æå– authoritative_cast_source (ä¸ºäº†åç»­æµç¨‹) ---
                if item_type == "Movie":
                    credits_source = fresh_data.get('credits') or fresh_data.get('casts') or {}
                    authoritative_cast_source = credits_source.get('cast', [])
                elif item_type == "Series":
                    if aggregated_tmdb_data:
                        all_episodes = list(aggregated_tmdb_data.get("episodes_details", {}).values())
                        authoritative_cast_source = _aggregate_series_cast_from_tmdb_data(fresh_data, all_episodes)
                    else:
                        credits_source = fresh_data.get('aggregate_credits') or fresh_data.get('credits') or {}
                        authoritative_cast_source = credits_source.get('cast', [])

            # =========================================================
            # â˜…â˜…â˜… æ­¥éª¤ 2: ç§»é™¤æ— å¤´åƒæ¼”å‘˜ â˜…â˜…â˜…
            # =========================================================
            if self.config.get(constants.CONFIG_OPTION_REMOVE_ACTORS_WITHOUT_AVATARS, True) and authoritative_cast_source:
                original_count = len(authoritative_cast_source)
                
                # ä½¿ç”¨ 'profile_path' ä½œä¸ºåˆ¤æ–­ä¾æ®
                actors_with_avatars = [
                    actor for actor in authoritative_cast_source if actor.get("profile_path")
                ]
                
                if len(actors_with_avatars) < original_count:
                    removed_count = original_count - len(actors_with_avatars)
                    logger.info(f"  âœ åœ¨æ ¸å¿ƒå¤„ç†å‰ï¼Œå·²ä»æºæ•°æ®ä¸­ç§»é™¤ {removed_count} ä½æ— å¤´åƒçš„æ¼”å‘˜ã€‚")
                    # ç”¨ç­›é€‰åçš„åˆ—è¡¨è¦†ç›–åŸå§‹åˆ—è¡¨
                    authoritative_cast_source = actors_with_avatars
                else:
                    logger.debug("  âœ (é¢„æ£€æŸ¥) æ‰€æœ‰æºæ•°æ®ä¸­çš„æ¼”å‘˜å‡æœ‰å¤´åƒï¼Œæ— éœ€é¢„å…ˆç§»é™¤ã€‚")
                
            # =========================================================
            # â˜…â˜…â˜… æ­¥éª¤ 3:  æ•°æ®æ¥æº â˜…â˜…â˜…
            # =========================================================
            final_processed_cast = None
            cache_row = None 
            # 1.å¿«é€Ÿæ¨¡å¼
            if not force_full_update:
                # --- è·¯å¾„å‡†å¤‡ ---
                cache_folder_name = "tmdb-movies2" if item_type == "Movie" else "tmdb-tv"
                target_override_dir = os.path.join(self.local_data_path, "override", cache_folder_name, tmdb_id)
                main_json_filename = "all.json" if item_type == "Movie" else "series.json"
                override_json_path = os.path.join(target_override_dir, main_json_filename)
                
                if os.path.exists(override_json_path):
                    logger.info(f"  âœ [å¿«é€Ÿæ¨¡å¼] å‘ç°æœ¬åœ°è¦†ç›–æ–‡ä»¶ï¼Œä¼˜å…ˆåŠ è½½: {override_json_path}")
                    try:
                        override_data = _read_local_json(override_json_path)
                        if override_data:
                            cast_data = (override_data.get('casts', {}) or override_data.get('credits', {})).get('cast', [])
                            if cast_data:
                                logger.info(f"  âœ [å¿«é€Ÿæ¨¡å¼] æˆåŠŸä»æ–‡ä»¶åŠ è½½ {len(cast_data)} ä½æ¼”å‘˜å’Œå…ƒæ•°æ®...")
                                final_processed_cast = cast_data
                                
                                # å…³é”®è®¾ç½® 1: ä»¥æ­¤ä¸ºæºæ›´æ–°æ•°æ®åº“
                                tmdb_details_for_extra = override_data 
                                
                                # å¦‚æœæ˜¯å‰§é›†ï¼Œå¿…é¡»æŠŠåˆ†é›†æ–‡ä»¶ä¹Ÿè¯»è¿›æ¥ï¼ 
                                if item_type == "Series":
                                    logger.info("  âœ [å¿«é€Ÿæ¨¡å¼] æ£€æµ‹åˆ°å‰§é›†ï¼Œæ­£åœ¨èšåˆæœ¬åœ°åˆ†é›†å…ƒæ•°æ®...")
                                    episodes_details_map = {}
                                    seasons_details_list = [] 
                                    
                                    try:
                                        # 1. å…ˆè¯» Override (ä¿ç•™ä½ çš„æ‰‹åŠ¨ä¿®æ”¹)
                                        if os.path.exists(target_override_dir):
                                            for fname in os.listdir(target_override_dir):
                                                full_path = os.path.join(target_override_dir, fname)
                                                if fname.startswith("season-") and fname.endswith(".json"):
                                                    try:
                                                        data = _read_local_json(full_path)
                                                        if data:
                                                            if "-episode-" in fname:
                                                                key = f"S{data.get('season_number')}E{data.get('episode_number')}"
                                                                episodes_details_map[key] = data
                                                            else:
                                                                seasons_details_list.append(data)
                                                    except: pass
                                        
                                        # 2. æŸ¥æ¼è¡¥ç¼ºï¼šä» TMDb è·å–æ–°åˆ†é›† ID
                                        if self.tmdb_api_key:
                                            # ç›´æ¥å¤ç”¨å‡½æ•°å¼€å¤´å·²ç»è·å–åˆ°çš„æ•°æ®ï¼Œä¸å†é‡å¤è¯·æ±‚
                                            fresh_agg_data = aggregated_tmdb_data
                                            
                                            #ä»¥æ­¤ä¸ºé˜²çº¿ï¼šä¸‡ä¸€å¼€å¤´æ²¡è·å–åˆ°ï¼ˆè™½ç„¶ä¸å¤ªå¯èƒ½ï¼‰ï¼Œå†å°è¯•è·å–ä¸€æ¬¡
                                            if not fresh_agg_data:
                                                # logger.debug("  âœ [å¿«é€Ÿæ¨¡å¼] é¦–æ¬¡èšåˆæœªå‘½ä¸­ï¼Œæ­£åœ¨è¡¥å……è¯·æ±‚ TMDb æ•°æ®...")
                                                fresh_agg_data = tmdb.aggregate_full_series_data_from_tmdb(int(tmdb_id), self.tmdb_api_key)
                                            
                                            if fresh_agg_data:
                                                fresh_eps = fresh_agg_data.get("episodes_details", {})
                                                added_count = 0
                                                
                                                for k, v in fresh_eps.items():
                                                    # åªæœ‰æœ¬åœ°æ²¡æœ‰çš„ï¼Œæ‰åŠ è¿›å» 
                                                    if k not in episodes_details_map:
                                                        # âœ¨ åˆ é™¤ TMDb è‡ªå¸¦çš„åŸå§‹æ¼”å‘˜è¡¨ï¼Œå¼ºåˆ¶ä½¿ç”¨æœ¬åœ°ç²¾ä¿®è¡¨ âœ¨
                                                        if 'credits' in v:
                                                            del v['credits']
                                                            
                                                        episodes_details_map[k] = v
                                                        added_count += 1
                                                        
                                                if added_count > 0:
                                                    logger.info(f"  âœ [å¿«é€Ÿæ¨¡å¼] æˆåŠŸè¡¥å…¨äº† {added_count} ä¸ªæœ¬åœ°ç¼ºå¤±çš„è¿½æ›´åˆ†é›†æ•°æ® (å†…å­˜è¡¥å…¨)ã€‚")

                                                # é¡ºä¾¿è¡¥å…¨ä¸€ä¸‹ç¼ºå¤±çš„å­£ä¿¡æ¯ (Seasons)
                                                fresh_seasons = fresh_agg_data.get("seasons_details", [])
                                                existing_season_nums = {s.get('season_number') for s in seasons_details_list}
                                                for fs in fresh_seasons:
                                                    if fs.get('season_number') not in existing_season_nums:
                                                        seasons_details_list.append(fs)

                                        # 3. å¡å›éª¨æ¶
                                        if episodes_details_map:
                                            tmdb_details_for_extra['episodes_details'] = episodes_details_map
                                        if seasons_details_list:
                                            seasons_details_list.sort(key=lambda x: x.get('season_number', 0))
                                            tmdb_details_for_extra['seasons_details'] = seasons_details_list

                                    except Exception as e_ep:
                                        logger.warning(f"  âœ [å¿«é€Ÿæ¨¡å¼] èšåˆåˆ†é›†/å­£æ•°æ®æ—¶å‘ç”Ÿå¼‚å¸¸: {e_ep}")

                                # å…³é”®è®¾ç½® 2: æ ‡è®°æºä¸ºæ–‡ä»¶
                                cache_row = {'source': 'override_file'} 

                                # è¡¥å……ï¼šç®€å•çš„ ID æ˜ å°„
                                tmdb_to_emby_map = {}
                                for person in item_details_from_emby.get("People", []):
                                    pid = (person.get("ProviderIds") or {}).get("Tmdb")
                                    if pid: tmdb_to_emby_map[str(pid)] = person.get("Id")
                                for actor in final_processed_cast:
                                    aid = str(actor.get('id'))
                                    if aid in tmdb_to_emby_map:
                                        actor['emby_person_id'] = tmdb_to_emby_map[aid]
                    except Exception as e:
                        logger.warning(f"  âœ è¯»å–è¦†ç›–æ–‡ä»¶å¤±è´¥: {e}ï¼Œå°†å°è¯•æ•°æ®åº“ç¼“å­˜ã€‚")

            # 2.å®Œæ•´æ¨¡å¼
            if final_processed_cast is None:
                logger.info(f"  âœ æœªå‘½ä¸­ç¼“å­˜æˆ–å¼ºåˆ¶é‡å¤„ç†ï¼Œå¼€å§‹å¤„ç†æ¼”å‘˜è¡¨...")

                # æ­¤æ—¶å¿…é¡»ä» TMDb æ‹‰å–æœ€æ–°çš„å¯¼æ¼”ã€åˆ†çº§ã€å·¥ä½œå®¤ï¼Œå¦åˆ™ Emby çš„æ•°æ®å¤ªæ®‹ç¼ºã€‚
                if not force_full_update and self.tmdb_api_key:
                    logger.info(f"  âœ æ£€æµ‹åˆ°æœ¬åœ°æ— æœ‰æ•ˆç¼“å­˜ï¼Œæ­£åœ¨ä» TMDb è¡¥å…¨å…ƒæ•°æ®éª¨æ¶(å¯¼æ¼”/åˆ†çº§/å·¥ä½œå®¤)...")
                    try:
                        if item_type == "Movie":
                            fresh_data = tmdb.get_movie_details(tmdb_id, self.tmdb_api_key)
                            if fresh_data:
                                # 1. è¦†ç›–éª¨æ¶ (å¯¼æ¼”ã€åˆ†çº§ã€å·¥ä½œå®¤ã€ç®€ä»‹ç­‰)
                                tmdb_details_for_extra.update(fresh_data)
                                # 2. æ›´æ–°æ¼”å‘˜æº (ç¡®ä¿æ˜¯ TMDb åŸç‰ˆé¡ºåº)
                                if fresh_data.get("credits", {}).get("cast"):
                                    authoritative_cast_source = fresh_data["credits"]["cast"]
                                logger.info(f"  âœ ç”µå½±å…ƒæ•°æ®è¡¥å…¨æˆåŠŸã€‚")

                        elif item_type == "Series":
                            aggregated_tmdb_data = tmdb.aggregate_full_series_data_from_tmdb(int(tmdb_id), self.tmdb_api_key)
                            if aggregated_tmdb_data:
                                series_details = aggregated_tmdb_data.get("series_details", {})
                                # 1. è¦†ç›–éª¨æ¶
                                tmdb_details_for_extra.update(series_details)
                                # 2. æ›´æ–°æ¼”å‘˜æº
                                all_episodes = list(aggregated_tmdb_data.get("episodes_details", {}).values())
                                authoritative_cast_source = _aggregate_series_cast_from_tmdb_data(series_details, all_episodes)
                                logger.info(f"  âœ å‰§é›†å…ƒæ•°æ®è¡¥å…¨æˆåŠŸã€‚")
                    except Exception as e_fetch:
                        logger.warning(f"  âœ å°è¯•è¡¥å…¨å…ƒæ•°æ®æ—¶å¤±è´¥ï¼Œå°†ä½¿ç”¨ Emby åŸå§‹æ•°æ®å…œåº•: {e_fetch}")

                # æ ‡é¢˜å’Œç®€ä»‹ç¿»è¯‘ (ä»…åœ¨å®Œæ•´æ¨¡å¼ä¸‹æ‰§è¡Œ) 
                if self.ai_translator:
                    
                    # ====== 1. ç®€ä»‹ç¿»è¯‘å¤„ç† ======
                    if self.config.get(constants.CONFIG_OPTION_AI_TRANSLATE_OVERVIEW, False):
                        logger.info(f"  âœ [å®Œæ•´æ¨¡å¼] æ­£åœ¨æ£€æŸ¥æ˜¯å¦éœ€è¦ AI ç¿»è¯‘ç®€ä»‹...")
                        
                        # ä¼˜å…ˆæ£€æŸ¥æœ¬åœ°æ•°æ®åº“ç¼“å­˜
                        local_trans = media_db.get_local_translation_info(str(tmdb_id), item_type)
                        
                        # é€»è¾‘ A: å›å¡«ç¼“å­˜
                        if local_trans and local_trans.get('overview') and utils.contains_chinese(local_trans['overview']):
                            tmdb_details_for_extra["overview"] = local_trans['overview']
                            if aggregated_tmdb_data and "series_details" in aggregated_tmdb_data:
                                aggregated_tmdb_data["series_details"]["overview"] = local_trans['overview']
                            logger.info(f"  âœ [å®Œæ•´æ¨¡å¼] å‘½ä¸­æœ¬åœ°ä¸­æ–‡ç®€ä»‹ç¼“å­˜ï¼Œè·³è¿‡AIç¿»è¯‘ã€‚")
                        
                        # é€»è¾‘ B: æ‰§è¡Œç¿»è¯‘
                        else:
                            current_overview = tmdb_details_for_extra.get("overview", "")
                            item_title_for_ai = tmdb_details_for_extra.get("title") or tmdb_details_for_extra.get("name")
                            
                            needs_trans_overview = False
                            if not current_overview: needs_trans_overview = True
                            elif not utils.contains_chinese(current_overview): needs_trans_overview = True
                            
                            if needs_trans_overview:
                                english_overview = ""
                                if current_overview and len(current_overview) > 10:
                                    english_overview = current_overview
                                elif self.tmdb_api_key:
                                    try:
                                        if item_type == "Movie":
                                            en_data = tmdb.get_movie_details(int(tmdb_id), self.tmdb_api_key, language="en-US")
                                            english_overview = en_data.get("overview")
                                        elif item_type == "Series":
                                            en_data = tmdb.get_tv_details(int(tmdb_id), self.tmdb_api_key, language="en-US")
                                            english_overview = en_data.get("overview")
                                    except: pass
                                
                                if english_overview:
                                    logger.info(f"  âœ [å®Œæ•´æ¨¡å¼] æ­£åœ¨è°ƒç”¨ AI ç¿»è¯‘ç®€ä»‹...")
                                    trans_overview = self.ai_translator.translate_overview(english_overview, title=item_title_for_ai)
                                    if trans_overview:
                                        tmdb_details_for_extra["overview"] = trans_overview
                                        if aggregated_tmdb_data and "series_details" in aggregated_tmdb_data:
                                            aggregated_tmdb_data["series_details"]["overview"] = trans_overview

                    # ====== 2. æ ‡é¢˜ç¿»è¯‘å¤„ç† ======
                    if self.config.get(constants.CONFIG_OPTION_AI_TRANSLATE_TITLE, False):
                        logger.info(f"  âœ [å®Œæ•´æ¨¡å¼] æ­£åœ¨æ£€æŸ¥æ˜¯å¦éœ€è¦ AI ç¿»è¯‘æ ‡é¢˜...")
                        
                        # ä¼˜å…ˆæ£€æŸ¥æœ¬åœ°æ•°æ®åº“ç¼“å­˜
                        local_trans = media_db.get_local_translation_info(str(tmdb_id), item_type)
                        
                        # é€»è¾‘ A: å›å¡«ç¼“å­˜
                        if local_trans and local_trans.get('title') and utils.contains_chinese(local_trans['title']):
                            cached_title = local_trans['title']
                            if item_type == "Movie": 
                                tmdb_details_for_extra["title"] = cached_title
                            else: 
                                tmdb_details_for_extra["name"] = cached_title
                                if aggregated_tmdb_data and "series_details" in aggregated_tmdb_data:
                                    aggregated_tmdb_data["series_details"]["name"] = cached_title
                            logger.info(f"  âœ [å®Œæ•´æ¨¡å¼] å‘½ä¸­æœ¬åœ°ä¸­æ–‡æ ‡é¢˜ç¼“å­˜ï¼Œè·³è¿‡AIç¿»è¯‘ã€‚")
                        
                        # é€»è¾‘ B: æ‰§è¡Œç¿»è¯‘
                        else:
                            current_title = tmdb_details_for_extra.get("title") if item_type == "Movie" else tmdb_details_for_extra.get("name")
                            if current_title and not utils.contains_chinese(current_title):
                                logger.info(f"  âœ [å®Œæ•´æ¨¡å¼] æ­£åœ¨è°ƒç”¨ AI ç¿»è¯‘æ ‡é¢˜: {current_title}")
                                release_date = tmdb_details_for_extra.get("release_date") or tmdb_details_for_extra.get("first_air_date")
                                year_str = release_date[:4] if release_date else ""
                                
                                trans_title = self.ai_translator.translate_title(current_title, media_type=item_type, year=year_str)
                                if trans_title and utils.contains_chinese(trans_title):
                                    if item_type == "Movie": tmdb_details_for_extra["title"] = trans_title
                                    else: 
                                        tmdb_details_for_extra["name"] = trans_title
                                        if aggregated_tmdb_data and "series_details" in aggregated_tmdb_data:
                                            aggregated_tmdb_data["series_details"]["name"] = trans_title

                # 3. å‰§é›†åˆ†é›†ç¿»è¯‘
                if item_type == "Series" and aggregated_tmdb_data and self.ai_translator and self.config.get(constants.CONFIG_OPTION_AI_TRANSLATE_EPISODE_OVERVIEW, False):
                    logger.info(f"  âœ [å®Œæ•´æ¨¡å¼] æ­£åœ¨é€’å½’æ£€æŸ¥åˆ†é›†ç®€ä»‹ç¿»è¯‘...")
                    current_series_name = tmdb_details_for_extra.get("name")
                    translate_tmdb_metadata_recursively(
                        item_type='Series',
                        tmdb_data=aggregated_tmdb_data,
                        ai_translator=self.ai_translator,
                        item_name=current_series_name
                    )
                    # ç¿»è¯‘å®Œåï¼Œéœ€è¦æŠŠ aggregated_tmdb_data é‡Œçš„ episodes_details åŒæ­¥å› tmdb_details_for_extra
                    if "episodes_details" in aggregated_tmdb_data:
                        tmdb_details_for_extra["episodes_details"] = aggregated_tmdb_data["episodes_details"]

                with get_central_db_connection() as conn:
                    cursor = conn.cursor()
                    
                    all_emby_people = item_details_from_emby.get("People", [])
                    current_emby_cast_raw = [p for p in all_emby_people if p.get("Type") == "Actor"]
                    emby_config = {"url": self.emby_url, "api_key": self.emby_api_key, "user_id": self.emby_user_id}
                    enriched_emby_cast = self.actor_db_manager.enrich_actors_with_provider_ids(cursor, current_emby_cast_raw, emby_config)
                    douban_cast_raw, _ = self._get_douban_data_with_local_cache(item_details_from_emby)

                    # è°ƒç”¨æ ¸å¿ƒå¤„ç†å™¨å¤„ç†æ¼”å‘˜è¡¨
                    final_processed_cast = self._process_cast_list(
                        tmdb_cast_people=authoritative_cast_source,
                        emby_cast_people=enriched_emby_cast,
                        douban_cast_list=douban_cast_raw,
                        item_details_from_emby=item_details_from_emby,
                        cursor=cursor,
                        tmdb_api_key=self.tmdb_api_key,
                        stop_event=self.get_stop_event()
                    )

            # =========================================================
            # â˜…â˜…â˜… æ­¥éª¤ 5: ç»Ÿä¸€çš„æ”¶å°¾æµç¨‹ â˜…â˜…â˜…
            # =========================================================
            if final_processed_cast is None:
                raise ValueError("æœªèƒ½ç”Ÿæˆæœ‰æ•ˆçš„æœ€ç»ˆæ¼”å‘˜åˆ—è¡¨ã€‚")

            with get_central_db_connection() as conn:
                cursor = conn.cursor()

                is_feedback_mode = (
                    cache_row 
                    and isinstance(cache_row, dict) 
                    and cache_row.get('source') == 'override_file'
                    and not specific_episode_ids  # <--- å…³é”®ï¼šå¦‚æœæœ‰æŒ‡å®šåˆ†é›†(è¿½æ›´)ï¼Œåˆ™å¿…é¡»ä¸º False
                )

                if is_feedback_mode:
                    # --- åˆ†æ”¯ A: çº¯è¯»å–æ¨¡å¼ (æé€Ÿæ¢å¤) ---
                    logger.info(f"  âœ [å¿«é€Ÿæ¨¡å¼] æ£€æµ‹åˆ°å®Œç¾æœ¬åœ°æ•°æ®ï¼Œè·³è¿‡å›¾ç‰‡ä¸‹è½½ã€æ–‡ä»¶å†™å…¥åŠ Emby åˆ·æ–°ã€‚")
                
                else:
                    # --- åˆ†æ”¯ B: æ­£å¸¸å¤„ç†/è¿½æ›´æ¨¡å¼ ---
                    # å†™å…¥ override æ–‡ä»¶
                    # æ³¨æ„ï¼šsync_single_item_assets å†…éƒ¨å·²ç»æœ‰é’ˆå¯¹ episode_ids_to_sync çš„ä¼˜åŒ–ï¼Œ
                    # å®ƒåªä¼šä¸‹è½½æ–°åˆ†é›†çš„å›¾ç‰‡ï¼Œå¹¶å¤åˆ¶æ–°åˆ†é›†çš„ JSONï¼Œä¸ä¼šé‡æ–°ä¸‹è½½å…¨å¥—å›¾ç‰‡ã€‚
                    self.sync_single_item_assets(
                        item_id=item_id,
                        update_description="ä¸»æµç¨‹å¤„ç†å®Œæˆ" if not specific_episode_ids else f"è¿½æ›´: {len(specific_episode_ids)}ä¸ªåˆ†é›†",
                        final_cast_override=final_processed_cast,
                        episode_ids_to_sync=specific_episode_ids,
                        metadata_override=tmdb_details_for_extra 
                    )

                    # é€šè¿‡ API å®æ—¶æ›´æ–° Emby æ¼”å‘˜åº“ä¸­çš„åå­—
                    self._update_emby_person_names_from_final_cast(final_processed_cast, item_name_for_log)

                    # é€šçŸ¥ Emby åˆ·æ–°
                    logger.info(f"  âœ å¤„ç†å®Œæˆï¼Œæ­£åœ¨é€šçŸ¥ Emby åˆ·æ–°...")
                    emby.refresh_emby_item_metadata(
                        item_emby_id=item_id,
                        emby_server_url=self.emby_url,
                        emby_api_key=self.emby_api_key,
                        user_id_for_ops=self.emby_user_id,
                        replace_all_metadata_param=True, 
                        item_name_for_log=item_name_for_log
                    )

                # æ›´æ–°æˆ‘ä»¬è‡ªå·±çš„æ•°æ®åº“ç¼“å­˜
                self._upsert_media_metadata(
                    cursor=cursor,
                    item_type=item_type,
                    item_details_from_emby=item_details_from_emby,
                    final_processed_cast=final_processed_cast,
                    source_data_package=tmdb_details_for_extra
                )
                
                # ç»¼åˆè´¨æ£€ (è§†é¢‘æµæ£€æŸ¥ + æ¼”å‘˜åŒ¹é…åº¦è¯„åˆ†)
                logger.info(f"  âœ æ­£åœ¨è¯„ä¼°ã€Š{item_name_for_log}ã€‹çš„å¤„ç†è´¨é‡...")
                
                # --- 1. è§†é¢‘æµæ•°æ®å®Œæ•´æ€§æ£€æŸ¥ (é‡æ„ç‰ˆ) ---
                stream_check_passed = True
                stream_fail_reason = ""
                
                # å†…éƒ¨è¾…åŠ©ï¼šæ£€æŸ¥å•ä¸ªèµ„äº§åˆ—è¡¨
                def _check_assets_list(assets_list, label_prefix=""):
                    if not assets_list:
                        return False, f"{label_prefix} æ— èµ„äº§æ•°æ®"
                    last_fail = "ç¼ºå¤±åª’ä½“ä¿¡æ¯"
                    # åªè¦æœ‰ä¸€ä¸ªç‰ˆæœ¬æ˜¯å¥½çš„ï¼Œå°±ç®—é€šè¿‡ (å¤šç‰ˆæœ¬æƒ…å†µ)
                    for asset in assets_list:
                        # é€‚é… Emby API ç»“æ„ (MediaStreams) å’Œ DB ç»“æ„ (asset_details_json) çš„å·®å¼‚
                        # DBç»“æ„: ç›´æ¥æ˜¯ dict
                        # Embyç»“æ„: éœ€è¦ä» MediaStreams æå–
                        
                        w, h, c = 0, 0, ""
                        
                        # æƒ…å†µ A: DB ç»“æ„ (asset_details_json)
                        if 'video_codec' in asset or 'width' in asset:
                            w = asset.get('width')
                            h = asset.get('height')
                            c = asset.get('video_codec')
                        
                        # æƒ…å†µ B: Emby API ç»“æ„ (MediaSource)
                        elif 'MediaStreams' in asset:
                            # éœ€éå†æµæ‰¾åˆ° Video
                            found_video = False
                            for stream in asset.get('MediaStreams', []):
                                if stream.get('Type') == 'Video':
                                    w = stream.get('Width')
                                    h = stream.get('Height')
                                    c = stream.get('Codec')
                                    found_video = True
                                    break
                            if not found_video: continue # è¿™ä¸ª Source æ²¡è§†é¢‘æµï¼Œçœ‹ä¸‹ä¸€ä¸ª
                        
                        # è°ƒç”¨é€šç”¨è´¨æ£€å‡½æ•°
                        valid, reason = utils.check_stream_validity(w, h, c)
                        if valid:
                            return True, ""
                        
                        # è®°å½•æœ€åä¸€ä¸ªå¤±è´¥åŸå› 
                        last_fail = reason

                    return False, f"{label_prefix} {last_fail}"

                # --- åˆ†ç±»å¤„ç† ---
                if item_type in ['Movie', 'Episode']:
                    # Emby API è¿”å›çš„æ˜¯ MediaSources åˆ—è¡¨
                    passed, reason = _check_assets_list(item_details_from_emby.get("MediaSources", []), "")
                    if not passed:
                        stream_check_passed = False
                        stream_fail_reason = reason

                elif item_type == 'Series':
                    # å‰§é›†ï¼šæŸ¥åº“é€’å½’æ£€æŸ¥
                    try:
                        # ä½¿ç”¨åˆšåˆšå†™å…¥çš„æœ€æ–°æ•°æ®
                        cursor.execute("""
                            SELECT season_number, episode_number, asset_details_json 
                            FROM media_metadata 
                            WHERE parent_series_tmdb_id = %s AND item_type = 'Episode' AND in_library = TRUE
                            ORDER BY season_number ASC, episode_number ASC
                        """, (tmdb_id,))
                        
                        db_episodes = cursor.fetchall()
                        for db_ep in db_episodes:
                            s_idx = db_ep['season_number']
                            e_idx = db_ep['episode_number']
                            raw_assets = db_ep['asset_details_json']
                            
                            assets = json.loads(raw_assets) if isinstance(raw_assets, str) else (raw_assets if isinstance(raw_assets, list) else [])
                            
                            # è°ƒç”¨è¾…åŠ©å‡½æ•°æ£€æŸ¥
                            passed, reason = _check_assets_list(assets, f"[S{s_idx}E{e_idx}]")
                            
                            if not passed:
                                stream_check_passed = False
                                stream_fail_reason = reason
                                logger.warning(f"  âœ [è´¨æ£€] å‰§é›†ã€Š{item_name_for_log}ã€‹æ£€æµ‹åˆ°ååˆ†é›†: {reason}")
                                break 

                    except Exception as e_db_check:
                        logger.warning(f"  âœ [è´¨æ£€] æ•°æ®åº“éªŒè¯åˆ†é›†æµä¿¡æ¯æ—¶å‡ºé”™: {e_db_check}")

                # æ¼”å‘˜å¤„ç†è´¨é‡è¯„åˆ†
                raw_genres = item_details_from_emby.get("Genres", [])

                # å¦‚æœæ•°æ®æœ¬èº«å°±æ˜¯å­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆå…¼å®¹æ—§æ•°æ®ï¼‰ï¼Œåˆ™ä¿æŒä¸å˜
                if raw_genres and isinstance(raw_genres[0], dict):
                    genres = [g.get('name') for g in raw_genres if g.get('name')]
                else:
                    genres = raw_genres

                is_animation = "Animation" in genres or "åŠ¨ç”»" in genres or "Documentary" in genres or "çºªå½•" in genres or "è®°å½•" in genres
                
                # æ— è®ºæ•°æ®æ¥è‡ª API è¿˜æ˜¯ æœ¬åœ°ç¼“å­˜ï¼Œéƒ½å¿…é¡»æ¥å—è¯„åˆ†ç®—æ³•çš„æ£€éªŒã€‚
                processing_score = actor_utils.evaluate_cast_processing_quality(
                    final_cast=final_processed_cast, 
                    original_cast_count=original_emby_actor_count,
                    expected_final_count=len(final_processed_cast), 
                    is_animation=is_animation
                )

                if cache_row:
                    logger.info(f"  âœ [å¿«é€Ÿæ¨¡å¼] åŸºäºç¼“å­˜æ•°æ®çš„å®æ—¶å¤æ ¸è¯„åˆ†: {processing_score:.2f}")
                
                min_score_for_review = float(self.config.get("min_score_for_review", constants.DEFAULT_MIN_SCORE_FOR_REVIEW))
                
                # æœ€ç»ˆåˆ¤å®šä¸æ—¥å¿—å†™å…¥ ---
                # ç¡®å®šè¦è®°å½•åˆ°æ•°æ®åº“çš„ç›®æ ‡ ID å’Œ åç§°
                # å¦‚æœå½“å‰å¤„ç†çš„æ˜¯åˆ†é›†ï¼Œå¿…é¡»å‘ä¸Šè¿½æº¯åˆ°å‰§é›† ID
                target_log_id = item_id
                target_log_name = item_name_for_log
                target_log_type = item_type

                if item_type == 'Episode':
                    series_id = item_details_from_emby.get('SeriesId')
                    series_name = item_details_from_emby.get('SeriesName')
                    if series_id:
                        target_log_id = str(series_id)
                        target_log_name = series_name or f"å‰§é›†(ID:{series_id})"
                        target_log_type = 'Series'
                        # å¦‚æœæ˜¯åˆ†é›†å¤±è´¥ï¼Œåœ¨æ—¥å¿—åå­—é‡Œå¸¦ä¸Šåˆ†é›†ä¿¡æ¯ï¼Œæ–¹ä¾¿æ’æŸ¥
                        if not stream_check_passed:
                            s_idx = item_details_from_emby.get('ParentIndexNumber')
                            e_idx = item_details_from_emby.get('IndexNumber')
                            stream_fail_reason = f"[S{s_idx}E{e_idx}] {stream_fail_reason}"

                # æœ€ç»ˆåˆ¤å®šä¸æ—¥å¿—å†™å…¥ ---
                # ä¼˜å…ˆçº§ï¼šè§†é¢‘æµç¼ºå¤± > è¯„åˆ†è¿‡ä½
                if not stream_check_passed:
                    # æƒ…å†µ A: è§†é¢‘æµç¼ºå¤± -> å¼ºåˆ¶å¾…å¤æ ¸
                    logger.warning(f"  âœ [è´¨æ£€]ã€Š{item_name_for_log}ã€‹å› ç¼ºå¤±è§†é¢‘æµæ•°æ®ï¼Œéœ€é‡æ–°å¤„ç†ã€‚")
                    # è®°å½•åˆ° Series ID
                    self.log_db_manager.save_to_failed_log(cursor, target_log_id, target_log_name, stream_fail_reason, target_log_type, score=0.0)
                    # æ ‡è®°ä¸ºå·²å¤„ç† (é˜²æ­¢æ­»å¾ªç¯)ï¼Œä½†åœ¨UIä¸­ä¼šæ˜¾ç¤ºåœ¨â€œå¾…å¤æ ¸â€
                    self._mark_item_as_processed(cursor, target_log_id, target_log_name, score=0.0)
                    
                elif processing_score < min_score_for_review:
                    # æƒ…å†µ B: è¯„åˆ†è¿‡ä½ -> å¾…å¤æ ¸
                    reason = f"å¤„ç†è¯„åˆ† ({processing_score:.2f}) ä½äºé˜ˆå€¼ ({min_score_for_review})ã€‚"
                    
                    if cache_row:
                        logger.warning(f"  âœ [è´¨æ£€]ã€Š{item_name_for_log}ã€‹æœ¬åœ°ç¼“å­˜æ•°æ®è´¨é‡ä¸ä½³ (è¯„åˆ†: {processing_score:.2f})ï¼Œå·²é‡æ–°æ ‡è®°ä¸ºã€å¾…å¤æ ¸ã€‘ã€‚")
                    else:
                        logger.warning(f"  âœ [è´¨æ£€]ã€Š{item_name_for_log}ã€‹å¤„ç†è´¨é‡ä¸ä½³ï¼Œå·²æ ‡è®°ä¸ºã€å¾…å¤æ ¸ã€‘ã€‚åŸå› : {reason}")
                        
                    self.log_db_manager.save_to_failed_log(cursor, target_log_id, target_log_name, reason, target_log_type, score=processing_score)
                    self._mark_item_as_processed(cursor, target_log_id, target_log_name, score=processing_score)
                    
                else:
                    # æƒ…å†µ C: ä¸€åˆ‡æ­£å¸¸ -> ç§»é™¤å¾…å¤æ ¸æ ‡è®°ï¼ˆå¦‚æœä¹‹å‰æœ‰ï¼‰
                    logger.info(f"  âœ ã€Š{item_name_for_log}ã€‹è´¨æ£€é€šè¿‡ (è¯„åˆ†: {processing_score:.2f})ï¼Œæ ‡è®°ä¸ºå·²å¤„ç†ã€‚")
                    # åªæœ‰å½“åˆ†é›†ä¹Ÿé€šè¿‡æ—¶ï¼Œæ‰æ›´æ–°å‰§é›†çš„è®°å½•ä¸ºæˆåŠŸ
                    self._mark_item_as_processed(cursor, target_log_id, target_log_name, score=processing_score)
                    self.log_db_manager.remove_from_failed_log(cursor, target_log_id)
                
                conn.commit()

            logger.trace(f"--- å¤„ç†å®Œæˆ '{item_name_for_log}' ---")

        except (ValueError, InterruptedError) as e:
            logger.warning(f"å¤„ç† '{item_name_for_log}' çš„è¿‡ç¨‹ä¸­æ–­: {e}")
            return False
        except Exception as outer_e:
            logger.error(f"æ ¸å¿ƒå¤„ç†æµç¨‹ä¸­å‘ç”ŸæœªçŸ¥ä¸¥é‡é”™è¯¯ for '{item_name_for_log}': {outer_e}", exc_info=True)
            try:
                with get_central_db_connection() as conn_fail:
                    self.log_db_manager.save_to_failed_log(conn_fail.cursor(), item_id, item_name_for_log, f"æ ¸å¿ƒå¤„ç†å¼‚å¸¸: {str(outer_e)}", item_type)
            except Exception as log_e:
                logger.error(f"å†™å…¥å¾…å¤æ ¸æ—¥å¿—æ—¶å†æ¬¡å‘ç”Ÿé”™è¯¯: {log_e}")
            return False

        logger.trace(f"  âœ… å¤„ç†å®Œæˆ '{item_name_for_log}'")
        return True

    # --- æ ¸å¿ƒå¤„ç†å™¨ ---
    def _process_cast_list(self, tmdb_cast_people: List[Dict[str, Any]],
                                    emby_cast_people: List[Dict[str, Any]],
                                    douban_cast_list: List[Dict[str, Any]],
                                    item_details_from_emby: Dict[str, Any],
                                    cursor: psycopg2.extensions.cursor,
                                    tmdb_api_key: Optional[str],
                                    stop_event: Optional[threading.Event]) -> List[Dict[str, Any]]:
        """
        ã€V-Final with Truncation - Full Codeã€‘
        - åœ¨æ­¥éª¤4çš„å¼€å¤´ï¼Œé‡æ–°åŠ å…¥äº†å¯¹æœ€ç»ˆæ¼”å‘˜åˆ—è¡¨è¿›è¡Œæˆªæ–­çš„é€»è¾‘ã€‚
        - ç¡®ä¿åœ¨è¿›è¡ŒAIç¿»è¯‘ç­‰è€—æ—¶æ“ä½œå‰ï¼Œå°†æ¼”å‘˜æ•°é‡é™åˆ¶åœ¨é…ç½®çš„ä¸Šé™å†…ã€‚
        """
        # --- åœ¨æ‰€æœ‰å¤„ç†å¼€å§‹å‰ï¼Œä»æºå¤´æ¸…æ´—åŒåå¼‚äººæ¼”å‘˜ ---
        logger.debug("  âœ é¢„å¤„ç†ï¼šæ¸…æ´—æºæ•°æ®ä¸­çš„åŒåæ¼”å‘˜ï¼Œåªä¿ç•™orderæœ€å°çš„ä¸€ä¸ªã€‚")
        cleaned_tmdb_cast = []
        seen_names = {} # ä½¿ç”¨å­—å…¸æ¥å­˜å‚¨è§è¿‡çš„åå­—åŠå…¶order
        
        # é¦–å…ˆæŒ‰ order æ’åºï¼Œç¡®ä¿ç¬¬ä¸€ä¸ªé‡åˆ°çš„æ˜¯ order æœ€å°çš„
        tmdb_cast_people.sort(key=lambda x: x.get('order', 999))

        for actor in tmdb_cast_people:
            name = actor.get("name")
            if not name or not isinstance(name, str):
                continue
            
            cleaned_name = name.strip()
            
            if cleaned_name not in seen_names:
                cleaned_tmdb_cast.append(actor)
                seen_names[cleaned_name] = actor.get('order', 999)
            else:
                # è®°å½•è¢«ä¸¢å¼ƒçš„æ¼”å‘˜
                role = actor.get("character", "æœªçŸ¥è§’è‰²")
                logger.info(f"  âœ ä¸ºé¿å…å¼ å† ææˆ´ï¼Œåˆ é™¤åŒåå¼‚äººæ¼”å‘˜: '{cleaned_name}' (è§’è‰²: {role}, order: {actor.get('order', 999)})")

        # ä½¿ç”¨æ¸…æ´—åçš„åˆ—è¡¨è¿›è¡Œåç»­æ‰€æœ‰æ“ä½œ
        tmdb_cast_people = cleaned_tmdb_cast

        # â˜…â˜…â˜… åœ¨æµç¨‹å¼€å§‹æ—¶ï¼Œè®°å½•ä¸‹æ¥è‡ªTMDbçš„åŸå§‹æ¼”å‘˜ID â˜…â˜…â˜…
        original_tmdb_ids = {str(actor.get("id")) for actor in tmdb_cast_people if actor.get("id")}
        # ======================================================================
        # æ­¥éª¤ 1: â˜…â˜…â˜… æ•°æ®é€‚é… â˜…â˜…â˜…
        # ======================================================================
        logger.debug("  âœ å¼€å§‹æ¼”å‘˜æ•°æ®é€‚é… (åæŸ¥ç¼“å­˜æ¨¡å¼)...")
        
        tmdb_actor_map_by_id = {str(actor.get("id")): actor for actor in tmdb_cast_people}
        tmdb_actor_map_by_en_name = {str(actor.get("name") or "").lower().strip(): actor for actor in tmdb_cast_people}

        final_cast_list = []
        used_tmdb_ids = set()

        for emby_actor in emby_cast_people:
            emby_person_id = emby_actor.get("Id")
            emby_tmdb_id = emby_actor.get("ProviderIds", {}).get("Tmdb")
            emby_name_lower = str(emby_actor.get("Name") or "").lower().strip()

            tmdb_match = None

            if emby_tmdb_id and str(emby_tmdb_id) in tmdb_actor_map_by_id:
                tmdb_match = tmdb_actor_map_by_id[str(emby_tmdb_id)]
            else:
                if emby_name_lower in tmdb_actor_map_by_en_name:
                    tmdb_match = tmdb_actor_map_by_en_name[emby_name_lower]
                else:
                    cache_entry = self.actor_db_manager.get_translation_from_db(cursor, emby_actor.get("Name"), by_translated_text=True)
                    if cache_entry and cache_entry.get('original_text'):
                        original_en_name = str(cache_entry['original_text']).lower().strip()
                        if original_en_name in tmdb_actor_map_by_en_name:
                            tmdb_match = tmdb_actor_map_by_en_name[original_en_name]

            if tmdb_match:
                tmdb_id_str = str(tmdb_match.get("id"))
                merged_actor = tmdb_match.copy()
                merged_actor["emby_person_id"] = emby_person_id
                if utils.contains_chinese(emby_actor.get("Name")):
                    merged_actor["name"] = emby_actor.get("Name")
                else:
                    merged_actor["name"] = tmdb_match.get("name")
                merged_actor["character"] = emby_actor.get("Role")
                final_cast_list.append(merged_actor)
                used_tmdb_ids.add(tmdb_id_str)

        for tmdb_id, tmdb_actor_data in tmdb_actor_map_by_id.items():
            if tmdb_id not in used_tmdb_ids:
                new_actor = tmdb_actor_data.copy()
                new_actor["emby_person_id"] = None
                final_cast_list.append(new_actor)

        logger.debug(f"  âœ æ•°æ®é€‚é…å®Œæˆï¼Œç”Ÿæˆäº† {len(final_cast_list)} æ¡åŸºå‡†æ¼”å‘˜æ•°æ®ã€‚")
        
        # ======================================================================
        # æ­¥éª¤ 2: â˜…â˜…â˜… â€œä¸€å¯¹ä¸€åŒ¹é…â€é€»è¾‘ â˜…â˜…â˜…
        # ======================================================================
        douban_candidates = actor_utils.format_douban_cast(douban_cast_list)
        unmatched_local_actors = list(final_cast_list)
        merged_actors = []
        unmatched_douban_actors = []
        logger.info(f"  âœ åŒ¹é…é˜¶æ®µ 1: å¯¹å·å…¥åº§")
        for d_actor in douban_candidates:
            douban_name_zh = d_actor.get("Name", "").lower().strip()
            douban_name_en = d_actor.get("OriginalName", "").lower().strip()
            match_found_for_this_douban_actor = False
            for i, l_actor in enumerate(unmatched_local_actors):
                local_name = str(l_actor.get("name") or "").lower().strip()
                local_original_name = str(l_actor.get("original_name") or "").lower().strip()
                is_match = False
                if douban_name_zh and (douban_name_zh == local_name or douban_name_zh == local_original_name):
                    is_match = True
                elif douban_name_en and (douban_name_en == local_name or douban_name_en == local_original_name):
                    is_match = True
                if is_match:
                    l_actor["name"] = d_actor.get("Name")
                    cleaned_douban_character = utils.clean_character_name_static(d_actor.get("Role"))
                    l_actor["character"] = actor_utils.select_best_role(l_actor.get("character"), cleaned_douban_character)
                    
                    douban_id_to_add = d_actor.get("DoubanCelebrityId")
                    if douban_id_to_add:
                        l_actor["douban_id"] = douban_id_to_add
                    
                    merged_actors.append(unmatched_local_actors.pop(i))
                    match_found_for_this_douban_actor = True
                    break
            if not match_found_for_this_douban_actor:
                unmatched_douban_actors.append(d_actor)

        current_cast_list = merged_actors + unmatched_local_actors
        final_cast_map = {str(actor['id']): actor for actor in current_cast_list if actor.get('id') and str(actor.get('id')) != 'None'}

        # ======================================================================
        # æ­¥éª¤ 3: â˜…â˜…â˜… å¤„ç†è±†ç“£è¡¥å……æ¼”å‘˜ï¼ˆå¸¦ä¸¢å¼ƒé€»è¾‘ å’Œ æ•°é‡ä¸Šé™é€»è¾‘ï¼‰ â˜…â˜…â˜…
        # ======================================================================
        if not unmatched_douban_actors:
            logger.info("  âœ è±†ç“£APIæœªè¿”å›æ¼”å‘˜æˆ–æ‰€æœ‰æ¼”å‘˜å·²åŒ¹é…ï¼Œè·³è¿‡è¡¥å……æ¼”å‘˜æµç¨‹ã€‚")
        else:
            logger.info(f"  âœ å‘ç° {len(unmatched_douban_actors)} ä½æ½œåœ¨çš„è±†ç“£è¡¥å……æ¼”å‘˜ï¼Œå¼€å§‹æ‰§è¡ŒåŒ¹é…ä¸ç­›é€‰...")
            
            limit = self.config.get(constants.CONFIG_OPTION_MAX_ACTORS_TO_PROCESS, 30)
            try:
                limit = int(limit)
                if limit <= 0: limit = 30
            except (ValueError, TypeError):
                limit = 30

            current_actor_count = len(final_cast_map)
            if current_actor_count >= limit:
                logger.info(f"  âœ å½“å‰æ¼”å‘˜æ•° ({current_actor_count}) å·²è¾¾ä¸Šé™ ({limit})ï¼Œå°†è·³è¿‡æ‰€æœ‰è±†ç“£è¡¥å……æ¼”å‘˜çš„æµç¨‹ã€‚")
                still_unmatched_final = unmatched_douban_actors
            else:
                logger.info(f"  âœ å½“å‰æ¼”å‘˜æ•° ({current_actor_count}) ä½äºä¸Šé™ ({limit})ï¼Œè¿›å…¥è¡¥å……æ¨¡å¼ã€‚")
                
                logger.info(f"  âœ åŒ¹é…é˜¶æ®µ 2: ç”¨è±†ç“£IDæŸ¥'æ¼”å‘˜æ˜ å°„è¡¨' ({len(unmatched_douban_actors)} ä½æ¼”å‘˜)")
                still_unmatched = []
                for d_actor in unmatched_douban_actors:
                    if self.is_stop_requested(): raise InterruptedError("ä»»åŠ¡ä¸­æ­¢")
                    d_douban_id = d_actor.get("DoubanCelebrityId")
                    match_found = False
                    if d_douban_id:
                        entry = self.actor_db_manager.find_person_by_any_id(cursor, douban_id=d_douban_id)
                        if entry and entry.get("tmdb_person_id") and entry.get("emby_person_id"):
                            tmdb_id_from_map = str(entry.get("tmdb_person_id"))
                            if tmdb_id_from_map not in final_cast_map:
                                logger.info(f"    â”œâ”€ åŒ¹é…æˆåŠŸ (é€šè¿‡ è±†ç“£IDæ˜ å°„): è±†ç“£æ¼”å‘˜ '{d_actor.get('Name')}' -> åŠ å…¥æœ€ç»ˆæ¼”å‘˜è¡¨")
                                cached_metadata_map = self.actor_db_manager.get_full_actor_details_by_tmdb_ids(cursor, [int(tmdb_id_from_map)])
                                cached_metadata = cached_metadata_map.get(int(tmdb_id_from_map), {})
                                new_actor_entry = {
                                    "id": tmdb_id_from_map, "name": d_actor.get("Name"),
                                    "original_name": cached_metadata.get("original_name") or d_actor.get("OriginalName"),
                                    "character": d_actor.get("Role"), "order": 999,
                                    "imdb_id": entry.get("imdb_id"), "douban_id": d_douban_id,
                                    "emby_person_id": entry.get("emby_person_id")
                                }
                                final_cast_map[tmdb_id_from_map] = new_actor_entry
                            match_found = True
                    if not match_found:
                        still_unmatched.append(d_actor)
                unmatched_douban_actors = still_unmatched

                logger.info(f"  âœ åŒ¹é…é˜¶æ®µ 3: ç”¨IMDb IDè¿›è¡Œæœ€ç»ˆåŒ¹é…å’Œæ–°å¢ ({len(unmatched_douban_actors)} ä½æ¼”å‘˜)")
                still_unmatched_final = []
                for i, d_actor in enumerate(unmatched_douban_actors):
                    if self.is_stop_requested(): raise InterruptedError("ä»»åŠ¡ä¸­æ­¢")
                    
                    if len(final_cast_map) >= limit:
                        logger.info(f"  âœ æ¼”å‘˜æ•°å·²è¾¾ä¸Šé™ ({limit})ï¼Œè·³è¿‡å‰©ä½™ {len(unmatched_douban_actors) - i} ä½æ¼”å‘˜çš„APIæŸ¥è¯¢ã€‚")
                        still_unmatched_final.extend(unmatched_douban_actors[i:])
                        break

                    d_douban_id = d_actor.get("DoubanCelebrityId")
                    match_found = False
                    if d_douban_id and self.douban_api and self.tmdb_api_key:
                        if self.is_stop_requested(): raise InterruptedError("ä»»åŠ¡ä¸­æ­¢")
                        details = self.douban_api.celebrity_details(d_douban_id)
                        time_module.sleep(0.3)
                        d_imdb_id = None
                        if details and not details.get("error"):
                            try:
                                info_list = details.get("extra", {}).get("info", [])
                                if isinstance(info_list, list):
                                    for item in info_list:
                                        if isinstance(item, list) and len(item) == 2 and item[0] == 'IMDbç¼–å·':
                                            d_imdb_id = item[1]
                                            break
                            except Exception as e_parse:
                                logger.warning(f"  âœ è§£æ IMDb ID æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯: {e_parse}")
                        
                        if d_imdb_id:
                            logger.debug(f"  âœ ä¸º '{d_actor.get('Name')}' è·å–åˆ° IMDb ID: {d_imdb_id}ï¼Œå¼€å§‹åŒ¹é…...")
                            
                            entry_from_map = self.actor_db_manager.find_person_by_any_id(cursor, imdb_id=d_imdb_id)
                            if entry_from_map and entry_from_map.get("tmdb_person_id") and entry_from_map.get("emby_person_id"):
                                tmdb_id_from_map = str(entry_from_map.get("tmdb_person_id"))
                                if tmdb_id_from_map not in final_cast_map:
                                    logger.debug(f"    â”œâ”€ åŒ¹é…æˆåŠŸ (é€šè¿‡ IMDbæ˜ å°„): è±†ç“£æ¼”å‘˜ '{d_actor.get('Name')}' -> åŠ å…¥æœ€ç»ˆæ¼”å‘˜è¡¨")
                                    new_actor_entry = {
                                        "id": tmdb_id_from_map, "name": d_actor.get("Name"),
                                        "character": d_actor.get("Role"), "order": 999, "imdb_id": d_imdb_id,
                                        "douban_id": d_douban_id, "emby_person_id": entry_from_map.get("emby_person_id")
                                    }
                                    final_cast_map[tmdb_id_from_map] = new_actor_entry
                                match_found = True
                            
                            if not match_found:
                                logger.debug(f"  âœ æ•°æ®åº“æœªæ‰¾åˆ° {d_imdb_id} çš„æ˜ å°„ï¼Œå¼€å§‹é€šè¿‡ TMDb API åæŸ¥...")
                                if self.is_stop_requested(): raise InterruptedError("ä»»åŠ¡ä¸­æ­¢")
                                person_from_tmdb = tmdb.find_person_by_external_id(
                                    external_id=d_imdb_id, api_key=self.tmdb_api_key, source="imdb_id"
                                )
                                if person_from_tmdb and person_from_tmdb.get("id"):
                                    tmdb_id_from_find = str(person_from_tmdb.get("id"))
                                    
                                    d_actor['tmdb_id_from_api'] = tmdb_id_from_find
                                    d_actor['imdb_id_from_api'] = d_imdb_id

                                    final_check_row = self.actor_db_manager.find_person_by_any_id(cursor, tmdb_id=tmdb_id_from_find)
                                    if final_check_row and dict(final_check_row).get("emby_person_id"):
                                        emby_pid_from_final_check = dict(final_check_row).get("emby_person_id")
                                        if tmdb_id_from_find not in final_cast_map:
                                            logger.info(f"    â”œâ”€ åŒ¹é…æˆåŠŸ (é€šè¿‡ TMDbåæŸ¥): è±†ç“£æ¼”å‘˜ '{d_actor.get('Name')}' -> åŠ å…¥æœ€ç»ˆæ¼”å‘˜è¡¨")
                                            new_actor_entry = {
                                                "id": tmdb_id_from_find, "name": d_actor.get("Name"),
                                                "character": d_actor.get("Role"), "order": 999,
                                                "imdb_id": d_imdb_id, "douban_id": d_douban_id,
                                                "emby_person_id": emby_pid_from_final_check
                                            }
                                            final_cast_map[tmdb_id_from_find] = new_actor_entry
                                        match_found = True
                    
                    if not match_found:
                        still_unmatched_final.append(d_actor)

                # --- å¤„ç†æ–°å¢ ---
                if still_unmatched_final:
                    logger.info(f"  âœ æ£€æŸ¥ {len(still_unmatched_final)} ä½æœªåŒ¹é…æ¼”å‘˜ï¼Œå°è¯•åˆå¹¶æˆ–åŠ å…¥æœ€ç»ˆåˆ—è¡¨...")
                    added_count = 0
                    merged_count = 0
                    
                    for d_actor in still_unmatched_final:
                        tmdb_id_to_process = d_actor.get('tmdb_id_from_api')
                        if tmdb_id_to_process:
                            # æƒ…å†µä¸€ï¼šæ¼”å‘˜å·²å­˜åœ¨ï¼Œæ‰§è¡Œåˆå¹¶/æ›´æ–°
                            if tmdb_id_to_process in final_cast_map:
                                existing_actor = final_cast_map[tmdb_id_to_process]
                                original_name = existing_actor.get("name")
                                new_name = d_actor.get("Name")
                                
                                # ä»…å½“è±†ç“£æä¾›äº†æ›´ä¼˜çš„åå­—ï¼ˆå¦‚ä¸­æ–‡åï¼‰æ—¶æ‰æ›´æ–°
                                if new_name and new_name != original_name and utils.contains_chinese(new_name):
                                    existing_actor["name"] = new_name
                                    logger.debug(f"    âœ [åˆå¹¶] å·²å°†æ¼”å‘˜ (TMDb ID: {tmdb_id_to_process}) çš„åå­—ä» '{original_name}' æ›´æ–°ä¸º '{new_name}'")
                                    merged_count += 1
                            
                            # æƒ…å†µäºŒï¼šæ¼”å‘˜ä¸å­˜åœ¨ï¼Œæ‰§è¡Œæ–°å¢
                            else:
                                new_actor_entry = {
                                    "id": tmdb_id_to_process,
                                    "name": d_actor.get("Name"),
                                    "character": d_actor.get("Role"),
                                    "order": 999,
                                    "imdb_id": d_actor.get("imdb_id_from_api"),
                                    "douban_id": d_actor.get("DoubanCelebrityId"),
                                    "emby_person_id": None
                                }
                                final_cast_map[tmdb_id_to_process] = new_actor_entry
                                added_count += 1
                    
                    if merged_count > 0:
                        logger.info(f"  âœ æˆåŠŸåˆå¹¶äº† {merged_count} ä½ç°æœ‰æ¼”å‘˜çš„è±†ç“£ä¿¡æ¯ã€‚")
                    if added_count > 0:
                        logger.info(f"  âœ æˆåŠŸæ–°å¢äº† {added_count} ä½æ¼”å‘˜åˆ°æœ€ç»ˆåˆ—è¡¨ã€‚")
        
        # ======================================================================
        # æ­¥éª¤ 4: â˜…â˜…â˜… ä»TMDbè¡¥å…¨å¤´åƒ â˜…â˜…â˜…
        # ======================================================================
        current_cast_list = list(final_cast_map.values())
        
        # â˜…â˜…â˜… æ ¸å¿ƒä¿®æ”¹ 2/3: ç­›é€‰éœ€è¦è¡¥å…¨çš„æ¼”å‘˜æ—¶ï¼Œæ’é™¤æ‰åŸå§‹TMDbåˆ—è¡¨ä¸­çš„æ¼”å‘˜ â˜…â˜…â˜…
        actors_to_supplement = [
            actor for actor in current_cast_list 
            if str(actor.get("id")) not in original_tmdb_ids and actor.get("id")
        ]
        
        if actors_to_supplement:
            total_to_supplement = len(actors_to_supplement)
            logger.info(f"  âœ å¼€å§‹ä¸º {total_to_supplement} ä½æ–°å¢æ¼”å‘˜æ£€æŸ¥å¹¶è¡¥å…¨å¤´åƒä¿¡æ¯...")

            ids_to_fetch = [actor.get("id") for actor in actors_to_supplement if actor.get("id")]
            all_cached_metadata = self.actor_db_manager.get_full_actor_details_by_tmdb_ids(cursor, ids_to_fetch)
            
            supplemented_count = 0
            for actor in actors_to_supplement:
                if stop_event and stop_event.is_set(): raise InterruptedError("ä»»åŠ¡ä¸­æ­¢")
                
                tmdb_id = actor.get("id")
                profile_path = None
                cached_meta = all_cached_metadata.get(tmdb_id)
                if cached_meta and cached_meta.get("profile_path"):
                    profile_path = cached_meta["profile_path"]
                
                elif tmdb_api_key:
                    person_details = tmdb.get_person_details_tmdb(tmdb_id, tmdb_api_key)
                    if person_details:
                        if person_details.get("profile_path"):
                            profile_path = person_details["profile_path"]
                
                if profile_path:
                    actor["profile_path"] = profile_path
                    supplemented_count += 1

            logger.info(f"  âœ æ–°å¢æ¼”å‘˜å¤´åƒä¿¡æ¯è¡¥å…¨å®Œæˆï¼ŒæˆåŠŸä¸º {supplemented_count}/{total_to_supplement} ä½æ¼”å‘˜è¡¥å……äº†å¤´åƒã€‚")
        else:
            logger.info("  âœ æ²¡æœ‰éœ€è¦è¡¥å……å¤´åƒçš„æ–°å¢æ¼”å‘˜ã€‚")

        # ======================================================================
        # æ­¥éª¤ 5: â˜…â˜…â˜… ä»æ¼”å‘˜è¡¨ç§»é™¤æ— å¤´åƒæ¼”å‘˜ â˜…â˜…â˜…
        # ======================================================================
        if self.config.get(constants.CONFIG_OPTION_REMOVE_ACTORS_WITHOUT_AVATARS, True):
            actors_with_avatars = [actor for actor in current_cast_list if actor.get("profile_path")]
            actors_without_avatars = [actor for actor in current_cast_list if not actor.get("profile_path")]

            if actors_without_avatars:
                removed_names = [a.get('name', f"TMDbID:{a.get('id')}") for a in actors_without_avatars]
                logger.info(f"  âœ å°†ç§»é™¤ {len(actors_without_avatars)} ä½æ— å¤´åƒçš„æ¼”å‘˜: {removed_names}")
                current_cast_list = actors_with_avatars
        else:
            logger.info("  âœ æœªå¯ç”¨ç§»é™¤æ— å¤´åƒæ¼”å‘˜ã€‚")

        # ======================================================================
        # æ­¥éª¤ 6ï¼šæ™ºèƒ½æˆªæ–­é€»è¾‘ (Smart Truncation) â˜…â˜…â˜…
        # ======================================================================
        max_actors = self.config.get(constants.CONFIG_OPTION_MAX_ACTORS_TO_PROCESS, 30)
        try:
            limit = int(max_actors)
            if limit <= 0: limit = 30
        except (ValueError, TypeError):
            limit = 30

        original_count = len(current_cast_list)
        
        if original_count > limit:
            logger.info(f"  âœ æ¼”å‘˜åˆ—è¡¨æ€»æ•° ({original_count}) è¶…è¿‡ä¸Šé™ ({limit})ï¼Œå°†ä¼˜å…ˆä¿ç•™æœ‰å¤´åƒçš„æ¼”å‘˜åè¿›è¡Œæˆªæ–­ã€‚")
            sort_key = lambda x: x.get('order') if x.get('order') is not None and x.get('order') >= 0 else 999
            with_profile = [actor for actor in current_cast_list if actor.get("profile_path")]
            without_profile = [actor for actor in current_cast_list if not actor.get("profile_path")]
            with_profile.sort(key=sort_key)
            without_profile.sort(key=sort_key)
            prioritized_list = with_profile + without_profile
            current_cast_list = prioritized_list[:limit]
            logger.debug(f"  âœ æˆªæ–­åï¼Œä¿ç•™äº† {len(with_profile)} ä½æœ‰å¤´åƒæ¼”å‘˜ä¸­çš„ {len([a for a in current_cast_list if a.get('profile_path')])} ä½ã€‚")
        else:
            # â–¼â–¼â–¼ æ ¸å¿ƒä¿®æ”¹ï¼šç›´æ¥åœ¨ current_cast_list ä¸Šæ’åº â–¼â–¼â–¼
            current_cast_list.sort(key=lambda x: x.get('order') if x.get('order') is not None and x.get('order') >= 0 else 999)

        # ======================================================================
        # æ­¥éª¤ 7: â˜…â˜…â˜… ç¿»è¯‘å’Œæ ¼å¼åŒ– â˜…â˜…â˜…
        # ======================================================================
        logger.info(f"  âœ å°†å¯¹ {len(current_cast_list)} ä½æ¼”å‘˜è¿›è¡Œæœ€ç»ˆçš„ç¿»è¯‘å’Œæ ¼å¼åŒ–å¤„ç†...")

        if not (self.ai_translator and self.config.get(constants.CONFIG_OPTION_AI_TRANSLATE_ACTOR_ROLE, False)):
            logger.info("  âœ ç¿»è¯‘æœªå¯ç”¨ï¼Œå°†ä¿ç•™æ¼”å‘˜å’Œè§’è‰²ååŸæ–‡ã€‚")
        else:
            final_translation_map = {}
            terms_to_translate = set()
            for actor in current_cast_list:
                character = actor.get('character')
                if character:
                    cleaned_character = utils.clean_character_name_static(character)
                    if cleaned_character and not utils.contains_chinese(cleaned_character):
                        terms_to_translate.add(cleaned_character)
                name = actor.get('name')
                if name and not utils.contains_chinese(name):
                    terms_to_translate.add(name)
            
            total_terms_count = len(terms_to_translate)
            logger.info(f"  âœ [ç¿»è¯‘ç»Ÿè®¡] 1. ä»»åŠ¡æ¦‚è§ˆ: å…±æ”¶é›†åˆ° {total_terms_count} ä¸ªç‹¬ç«‹è¯æ¡éœ€è¦ç¿»è¯‘ã€‚")
            if total_terms_count > 0:
                logger.debug(f"    âœ å¾…å¤„ç†è¯æ¡åˆ—è¡¨: {list(terms_to_translate)}")

            remaining_terms = list(terms_to_translate)
            if remaining_terms:
                cached_results = {}
                terms_for_api = []
                for term in remaining_terms:
                    cached = self.actor_db_manager.get_translation_from_db(cursor, term)
                    if cached and cached.get('translated_text'):
                        cached_results[term] = cached['translated_text']
                    else:
                        terms_for_api.append(term)
                
                cached_count = len(cached_results)
                logger.info(f"  âœ [ç¿»è¯‘ç»Ÿè®¡] 2. ç¼“å­˜æ£€æŸ¥: å‘½ä¸­æ•°æ®åº“ç¼“å­˜ {cached_count} æ¡ã€‚")
                if cached_count > 0:
                    logger.debug("    âœ å‘½ä¸­ç¼“å­˜çš„è¯æ¡ä¸è¯‘æ–‡:")
                    for k, v in sorted(cached_results.items()):
                        logger.debug(f"    â”œâ”€ {k} âœ {v}")

                if cached_results:
                    final_translation_map.update(cached_results)
                if terms_for_api:
                    logger.info(f"  âœ [ç¿»è¯‘ç»Ÿè®¡] 3. AIå¤„ç† (å¿«é€Ÿæ¨¡å¼): æäº¤ {len(terms_for_api)} æ¡ã€‚")
                    if terms_for_api:
                        logger.debug(f"    âœ æäº¤ç»™[å¿«é€Ÿæ¨¡å¼]çš„è¯æ¡: {terms_for_api}")
                    fast_api_results = self.ai_translator.batch_translate(terms_for_api, mode='fast')
                    for term, translation in fast_api_results.items():
                        final_translation_map[term] = translation
                        self.actor_db_manager.save_translation_to_db(cursor, term, translation, self.ai_translator.provider)
                failed_terms = []
                for term in remaining_terms:
                    if not utils.contains_chinese(final_translation_map.get(term, term)):
                        failed_terms.append(term)
                remaining_terms = failed_terms
            if remaining_terms:
                logger.info(f"  âœ [ç¿»è¯‘ç»Ÿè®¡] 4. AIå¤„ç† (éŸ³è¯‘æ¨¡å¼): æäº¤ {len(remaining_terms)} æ¡ã€‚")
                if remaining_terms:
                    logger.debug(f"    âœ æäº¤ç»™[éŸ³è¯‘æ¨¡å¼]çš„è¯æ¡: {remaining_terms}")
                transliterate_results = self.ai_translator.batch_translate(remaining_terms, mode='transliterate')
                final_translation_map.update(transliterate_results)
                still_failed_terms = []
                for term in remaining_terms:
                    if not utils.contains_chinese(final_translation_map.get(term, term)):
                        still_failed_terms.append(term)
                remaining_terms = still_failed_terms
            if remaining_terms:
                item_title = item_details_from_emby.get("Name")
                item_year = item_details_from_emby.get("ProductionYear")
                logger.info(f"  âœ [ç¿»è¯‘ç»Ÿè®¡] 5. AIå¤„ç† (é¡¾é—®æ¨¡å¼): æäº¤ {len(remaining_terms)} æ¡ã€‚")
                if remaining_terms:
                    logger.debug(f"  âœ æäº¤ç»™[é¡¾é—®æ¨¡å¼]çš„è¯æ¡: {remaining_terms}")
                quality_results = self.ai_translator.batch_translate(remaining_terms, mode='quality', title=item_title, year=item_year)
                final_translation_map.update(quality_results)
            
            successfully_translated_terms = {term for term in terms_to_translate if utils.contains_chinese(final_translation_map.get(term, ''))}
            failed_to_translate_terms = terms_to_translate - successfully_translated_terms
            
            logger.info(f"  âœ [ç¿»è¯‘ç»Ÿè®¡] 6. ç»“æœæ€»ç»“: æˆåŠŸç¿»è¯‘ {len(successfully_translated_terms)}/{total_terms_count} ä¸ªè¯æ¡ã€‚")
            if successfully_translated_terms:
                logger.debug("  âœ ç¿»è¯‘æˆåŠŸåˆ—è¡¨ (åŸæ–‡ âœ è¯‘æ–‡):")
                for term in sorted(list(successfully_translated_terms)):
                    translation = final_translation_map.get(term)
                    logger.debug(f"    â”œâ”€ {term} âœ {translation}")
            if failed_to_translate_terms:
                logger.warning(f"    âœ ç¿»è¯‘å¤±è´¥åˆ—è¡¨ ({len(failed_to_translate_terms)}æ¡): {list(failed_to_translate_terms)}")

            for actor in current_cast_list:
                original_name = actor.get('name')
                if original_name and original_name in final_translation_map:
                    actor['name'] = final_translation_map[original_name]
                original_character = actor.get('character')
                if original_character:
                    cleaned_character = utils.clean_character_name_static(original_character)
                    actor['character'] = final_translation_map.get(cleaned_character, cleaned_character)
                else:
                    actor['character'] = ''

        tmdb_to_emby_id_map = {
            str(actor.get('id')): actor.get('emby_person_id')
            for actor in current_cast_list if actor.get('id') and actor.get('emby_person_id')
        }
        # è·å–åŸå§‹æ•°æ®
        raw_genres = item_details_from_emby.get("Genres", [])

        # å¦‚æœæ•°æ®æœ¬èº«å°±æ˜¯å­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆå…¼å®¹æ—§æ•°æ®ï¼‰ï¼Œåˆ™ä¿æŒä¸å˜
        if raw_genres and isinstance(raw_genres[0], dict):
            genres = [g.get('name') for g in raw_genres if g.get('name')]
        else:
            genres = raw_genres

        is_animation = "Animation" in genres or "åŠ¨ç”»" in genres or "Documentary" in genres or "çºªå½•" in genres or "è®°å½•" in genres
        final_cast_perfect = actor_utils.format_and_complete_cast_list(
            current_cast_list, is_animation, self.config, mode='auto'
        )
        for actor in final_cast_perfect:
            tmdb_id_str = str(actor.get("id"))
            if tmdb_id_str in tmdb_to_emby_id_map:
                actor["emby_person_id"] = tmdb_to_emby_id_map[tmdb_id_str]
        for actor in final_cast_perfect:
            actor["provider_ids"] = {
                "Tmdb": str(actor.get("id")),
                "Imdb": actor.get("imdb_id"),
                "Douban": actor.get("douban_id")
            }

        # ======================================================================
        # æ­¥éª¤ 8: â˜…â˜…â˜… æœ€ç»ˆæ•°æ®å›å†™/åå“º â˜…â˜…â˜… 
        # ======================================================================
        logger.info(f"  âœ å¼€å§‹å°† {len(final_cast_perfect)} ä½æœ€ç»ˆæ¼”å‘˜çš„å®Œæ•´ä¿¡æ¯åŒæ­¥å›æ•°æ®åº“...")
        processed_count = 0
        
        # åœ¨å¾ªç¯å¤–å‡†å¤‡ emby_configï¼Œé¿å…é‡å¤åˆ›å»º
        emby_config_for_upsert = {"url": self.emby_url, "api_key": self.emby_api_key, "user_id": self.emby_user_id}

        for actor in final_cast_perfect:
            # ç›´æ¥å°† actor å­—å…¸å’Œ emby_config ä¼ é€’ç»™ upsert_person å‡½æ•°
            map_id, action = self.actor_db_manager.upsert_person(cursor, actor, emby_config_for_upsert)
            
            if action not in ["ERROR", "SKIPPED", "CONFLICT_ERROR", "UNKNOWN_ERROR"]:
                processed_count += 1
            else:
                # å¦‚æœå‘ç”Ÿé”™è¯¯ï¼Œå›æ»šå½“å‰æ¼”å‘˜çš„æ“ä½œï¼Œå¹¶ä¸ºä¸‹ä¸€ä¸ªæ¼”å‘˜å¼€å¯æ–°äº‹åŠ¡
                # è¿™æ˜¯ä¸ºäº†é˜²æ­¢ä¸€ä¸ªæ¼”å‘˜çš„é”™è¯¯å¯¼è‡´æ•´ä¸ªæ‰¹æ¬¡å¤±è´¥
                cursor.connection.rollback()
                cursor.execute("BEGIN")

        logger.info(f"  âœ æˆåŠŸå¤„ç†äº† {processed_count} ä½æ¼”å‘˜çš„æ•°æ®åº“å›å†™/æ›´æ–°ã€‚")

        return final_cast_perfect
    
    # --- ä¸€é”®ç¿»è¯‘ ---
    def translate_cast_list_for_editing(self, 
                                    cast_list: List[Dict[str, Any]], 
                                    title: Optional[str] = None, 
                                    year: Optional[int] = None,
                                    tmdb_id: Optional[str] = None) -> List[Dict[str, Any]]:
        """
        ã€V14 - çº¯AIç¿»è¯‘ç‰ˆã€‘ä¸ºæ‰‹åŠ¨ç¼–è¾‘é¡µé¢æä¾›çš„ä¸€é”®ç¿»è¯‘åŠŸèƒ½ã€‚
        - å½»åº•ç§»é™¤ä¼ ç»Ÿç¿»è¯‘å¼•æ“çš„é™çº§é€»è¾‘ã€‚
        - å¦‚æœAIç¿»è¯‘æœªå¯ç”¨æˆ–å¤±è´¥ï¼Œåˆ™ç›´æ¥æ”¾å¼ƒç¿»è¯‘ã€‚
        """
        if not cast_list:
            return []

        # â˜…â˜…â˜… æ ¸å¿ƒä¿®æ”¹ 1: æ£€æŸ¥AIç¿»è¯‘æ˜¯å¦å¯ç”¨ï¼Œå¦‚æœæœªå¯ç”¨åˆ™ç›´æ¥è¿”å› â˜…â˜…â˜…
        if not self.ai_translator or not self.config.get(constants.CONFIG_OPTION_AI_TRANSLATE_ACTOR_ROLE, False):
            logger.info("æ‰‹åŠ¨ç¼–è¾‘-ä¸€é”®ç¿»è¯‘ï¼šAIç¿»è¯‘æœªå¯ç”¨ï¼Œä»»åŠ¡è·³è¿‡ã€‚")
            # å¯ä»¥åœ¨è¿™é‡Œè¿”å›ä¸€ä¸ªæç¤ºç»™å‰ç«¯ï¼Œæˆ–è€…ç›´æ¥è¿”å›åŸå§‹åˆ—è¡¨
            # ä¸ºäº†å‰ç«¯ä½“éªŒï¼Œæˆ‘ä»¬å¯ä»¥åœ¨ç¬¬ä¸€ä¸ªéœ€è¦ç¿»è¯‘çš„æ¼”å‘˜ä¸ŠåŠ ä¸€ä¸ªçŠ¶æ€
            translated_cast_for_status = [dict(actor) for actor in cast_list]
            for actor in translated_cast_for_status:
                name_needs_translation = actor.get('name') and not utils.contains_chinese(actor.get('name'))
                role_needs_translation = actor.get('role') and not utils.contains_chinese(actor.get('role'))
                if name_needs_translation or role_needs_translation:
                    actor['matchStatus'] = 'AIæœªå¯ç”¨'
                    break # åªæ ‡è®°ç¬¬ä¸€ä¸ªå³å¯
            return translated_cast_for_status

        # ä»é…ç½®ä¸­è¯»å–æ¨¡å¼
        translation_mode = self.config.get(constants.CONFIG_OPTION_AI_TRANSLATION_MODE, "fast")
        
        context_log = f" (ä¸Šä¸‹æ–‡: {title} {year})" if title and translation_mode == 'quality' else ""
        logger.info(f"æ‰‹åŠ¨ç¼–è¾‘-ä¸€é”®ç¿»è¯‘ï¼šå¼€å§‹æ‰¹é‡å¤„ç† {len(cast_list)} ä½æ¼”å‘˜ (æ¨¡å¼: {translation_mode}){context_log}ã€‚")
        
        translated_cast = [dict(actor) for actor in cast_list]
        
        # --- çº¯AIæ‰¹é‡ç¿»è¯‘é€»è¾‘ ---
        try:
            with get_central_db_connection() as conn:
                cursor = conn.cursor()
                
                translation_cache = {} # æœ¬æ¬¡è¿è¡Œçš„å†…å­˜ç¼“å­˜
                texts_to_translate = set()

                # 1. æ”¶é›†æ‰€æœ‰éœ€è¦ç¿»è¯‘çš„è¯æ¡
                texts_to_collect = set()
                for actor in translated_cast:
                    for field_key in ['name', 'role']:
                        text = actor.get(field_key, '').strip()
                        if field_key == 'role':
                            text = utils.clean_character_name_static(text)
                        if text and not utils.contains_chinese(text):
                            texts_to_collect.add(text)

                # 2. æ ¹æ®æ¨¡å¼å†³å®šæ˜¯å¦ä½¿ç”¨ç¼“å­˜
                if translation_mode == 'fast':
                    logger.debug("[å¿«é€Ÿæ¨¡å¼] æ­£åœ¨æ£€æŸ¥å…¨å±€ç¿»è¯‘ç¼“å­˜...")
                    for text in texts_to_collect:
                        cached_entry = self.actor_db_manager.get_translation_from_db(cursor=cursor, text=text)
                        if cached_entry:
                            translation_cache[text] = cached_entry.get("translated_text")
                        else:
                            texts_to_translate.add(text)
                else: # 'quality' mode
                    logger.debug("[é¡¾é—®æ¨¡å¼] è·³è¿‡ç¼“å­˜æ£€æŸ¥ï¼Œç›´æ¥ç¿»è¯‘æ‰€æœ‰è¯æ¡ã€‚")
                    texts_to_translate = texts_to_collect

                # 3. å¦‚æœæœ‰éœ€è¦ç¿»è¯‘çš„è¯æ¡ï¼Œè°ƒç”¨AI
                if texts_to_translate:
                    logger.info(f"æ‰‹åŠ¨ç¼–è¾‘-ç¿»è¯‘ï¼šå°† {len(texts_to_translate)} ä¸ªè¯æ¡æäº¤ç»™AI (æ¨¡å¼: {translation_mode})ã€‚")
                    translation_map_from_api = self.ai_translator.batch_translate(
                        texts=list(texts_to_translate),
                        mode=translation_mode,
                        title=title,
                        year=year
                    )
                    if translation_map_from_api:
                        translation_cache.update(translation_map_from_api)
                        
                        if translation_mode == 'fast':
                            for original, translated in translation_map_from_api.items():
                                self.actor_db_manager.save_translation_to_db(
                                    cursor=cursor,
                                    original_text=original, 
                                    translated_text=translated, 
                                    engine_used=self.ai_translator.provider
                                )
                    else:
                        logger.warning("æ‰‹åŠ¨ç¼–è¾‘-ç¿»è¯‘ï¼šAIæ‰¹é‡ç¿»è¯‘æœªè¿”å›ä»»ä½•ç»“æœã€‚")
                else:
                    logger.info("æ‰‹åŠ¨ç¼–è¾‘-ç¿»è¯‘ï¼šæ‰€æœ‰è¯æ¡å‡åœ¨ç¼“å­˜ä¸­æ‰¾åˆ°ï¼Œæ— éœ€è°ƒç”¨APIã€‚")

                # 4. å›å¡«æ‰€æœ‰ç¿»è¯‘ç»“æœ
                if translation_cache:
                    for i, actor in enumerate(translated_cast):
                        original_name = actor.get('name', '').strip()
                        if original_name in translation_cache:
                            translated_cast[i]['name'] = translation_cache[original_name]
                        
                        original_role_raw = actor.get('role', '').strip()
                        cleaned_original_role = utils.clean_character_name_static(original_role_raw)
                        
                        if cleaned_original_role in translation_cache:
                            translated_cast[i]['role'] = translation_cache[cleaned_original_role]
                        
                        if translated_cast[i].get('name') != actor.get('name') or translated_cast[i].get('role') != actor.get('role'):
                            translated_cast[i]['matchStatus'] = 'å·²ç¿»è¯‘'
        
        except Exception as e:
            logger.error(f"ä¸€é”®ç¿»è¯‘æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
            # å¯ä»¥åœ¨è¿™é‡Œç»™å‡ºä¸€ä¸ªé”™è¯¯æç¤º
            for actor in translated_cast:
                actor['matchStatus'] = 'ç¿»è¯‘å‡ºé”™'
                break
            return translated_cast

        logger.info("æ‰‹åŠ¨ç¼–è¾‘-ç¿»è¯‘å®Œæˆã€‚")
        return translated_cast
    
    # âœ¨âœ¨âœ¨æ‰‹åŠ¨å¤„ç†âœ¨âœ¨âœ¨
    def process_item_with_manual_cast(self, item_id: str, manual_cast_list: List[Dict[str, Any]], item_name: str) -> bool:
        """
        ã€V2.5 - ç»ˆæä¿®å¤ç‰ˆã€‘
        1. å¢åŠ äº†å®Œæ•´çš„æ—¥å¿—è®°å½•ï¼Œè®©æ¯ä¸€æ­¥æ“ä½œéƒ½æ¸…æ™°å¯è§ã€‚
        2. ä¿®å¤å¹¶å¼ºåŒ–äº†â€œç¿»è¯‘ç¼“å­˜åå“ºâ€åŠŸèƒ½ã€‚
        3. å¢åŠ äº†åœ¨å†™å…¥æ–‡ä»¶å‰çš„å¼ºåˆ¶â€œæœ€ç»ˆæ ¼å¼åŒ–â€æ­¥éª¤ï¼Œç¡®ä¿å‰ç¼€æ°¸è¿œæ­£ç¡®ã€‚
        """
        logger.info(f"  âœ æ‰‹åŠ¨å¤„ç†æµç¨‹å¯åŠ¨ï¼šItemID: {item_id} ('{item_name}')")
        
        try:
            item_details = emby.get_emby_item_details(item_id, self.emby_url, self.emby_api_key, self.emby_user_id)
            if not item_details: raise ValueError(f"æ— æ³•è·å–é¡¹ç›® {item_id} çš„è¯¦æƒ…ã€‚")
            
            raw_emby_actors = [p for p in item_details.get("People", []) if p.get("Type") == "Actor"]
            emby_config = {"url": self.emby_url, "api_key": self.emby_api_key, "user_id": self.emby_user_id}

            # â˜…â˜…â˜… æ ¸å¿ƒä¿®æ”¹: åœ¨æ‰€æœ‰æ“ä½œå¼€å§‹å‰ï¼Œä¸€æ¬¡æ€§è·å–æ‰€æœ‰ enriched_actors â˜…â˜…â˜…
            with get_central_db_connection() as conn:
                cursor = conn.cursor()
                enriched_actors = self.actor_db_manager.enrich_actors_with_provider_ids(cursor, raw_emby_actors, emby_config)

            # ======================================================================
            # æ­¥éª¤ 1: æ•°æ®å‡†å¤‡ä¸å®šä½ (ç°åœ¨åªè´Ÿè´£æ„å»ºæ˜ å°„)
            # ======================================================================
            logger.info(f"  âœ æ‰‹åŠ¨å¤„ç†ï¼šæ­¥éª¤ 1/6: æ„å»ºTMDbä¸Embyæ¼”å‘˜çš„IDæ˜ å°„...")
            tmdb_to_emby_map = {}
            for person in enriched_actors:
                person_tmdb_id = (person.get("ProviderIds") or {}).get("Tmdb")
                if person_tmdb_id:
                    tmdb_to_emby_map[str(person_tmdb_id)] = person.get("Id")
            logger.info(f"  âœ æˆåŠŸæ„å»ºäº† {len(tmdb_to_emby_map)} æ¡IDæ˜ å°„ã€‚")
            
            item_type = item_details.get("Type")
            tmdb_id = item_details.get("ProviderIds", {}).get("Tmdb")
            if not tmdb_id: raise ValueError(f"é¡¹ç›® {item_id} ç¼ºå°‘ TMDb IDã€‚")

            # --- è·å– TMDb è¯¦æƒ…ç”¨äºåˆ†çº§æ•°æ®æå– ---
            tmdb_details_for_manual_extra = None
            aggregated_tmdb_data_manual = None
            if self.tmdb_api_key:
                if item_type == "Movie":
                    tmdb_details_for_manual_extra = tmdb.get_movie_details(tmdb_id, self.tmdb_api_key)
                    if not tmdb_details_for_manual_extra:
                        logger.warning(f"  âœ æ‰‹åŠ¨å¤„ç†ï¼šæ— æ³•ä» TMDb è·å–ç”µå½± '{item_name}' ({tmdb_id}) çš„è¯¦æƒ…ã€‚")
                elif item_type == "Series":
                    aggregated_tmdb_data_manual = tmdb.aggregate_full_series_data_from_tmdb(int(tmdb_id), self.tmdb_api_key)
                    if aggregated_tmdb_data_manual:
                        tmdb_details_for_manual_extra = aggregated_tmdb_data_manual.get("series_details")
                    else:
                        logger.warning(f"  âœ æ‰‹åŠ¨å¤„ç†ï¼šæ— æ³•ä» TMDb è·å–å‰§é›† '{item_name}' ({tmdb_id}) çš„è¯¦æƒ…ã€‚")
            else:
                logger.warning("  âœ æ‰‹åŠ¨å¤„ç†ï¼šæœªé…ç½® TMDb API Keyï¼Œæ— æ³•è·å– TMDb è¯¦æƒ…ç”¨äºåˆ†çº§æ•°æ®ã€‚")

            cache_folder_name = "tmdb-movies2" if item_type == "Movie" else "tmdb-tv"
            target_override_dir = os.path.join(self.local_data_path, "override", cache_folder_name, tmdb_id)
            main_json_filename = "all.json" if item_type == "Movie" else "series.json"
            main_json_path = os.path.join(target_override_dir, main_json_filename)

            if not os.path.exists(main_json_path):
                raise FileNotFoundError(f"æ‰‹åŠ¨å¤„ç†å¤±è´¥ï¼šæ‰¾ä¸åˆ°ä¸»å…ƒæ•°æ®æ–‡ä»¶ '{main_json_path}'ã€‚")

            # ======================================================================
            # æ­¥éª¤ 2: æ›´æ–°AIç¿»è¯‘ç¼“å­˜
            # ======================================================================
            logger.info(f"  âœ æ‰‹åŠ¨å¤„ç†ï¼šæ­¥éª¤ 2/5: æ£€æŸ¥å¹¶æ›´æ–°AIç¿»è¯‘ç¼“å­˜...")
            try:
                # â˜…â˜…â˜… æ ¸å¿ƒä¿®å¤ â‘ : ä»ç¼“å­˜è·å–çš„æ˜¯ tmdbId -> åŸå§‹è§’è‰²å çš„å­—å…¸ â˜…â˜…â˜…
                original_roles_map = self.manual_edit_cache.get(item_id)
                if original_roles_map:
                    with get_central_db_connection() as conn:
                        cursor = conn.cursor()
                        updated_count = 0
                        
                        # â˜…â˜…â˜… æ ¸å¿ƒä¿®å¤ â‘¡: éå†å‰ç«¯æäº¤çš„åˆ—è¡¨ â˜…â˜…â˜…
                        for actor_from_frontend in manual_cast_list:
                            tmdb_id_str = str(actor_from_frontend.get("tmdbId"))
                            if not tmdb_id_str: continue
                            
                            # â˜…â˜…â˜… æ ¸å¿ƒä¿®å¤ â‘¢: ç”¨ tmdbId ç²¾å‡†æ‰¾åˆ°ä¿®æ”¹å‰çš„è§’è‰²å â˜…â˜…â˜…
                            original_role = original_roles_map.get(tmdb_id_str)
                            if original_role is None: # å¦‚æœåŸå§‹è®°å½•é‡Œå°±æ²¡æœ‰è¿™ä¸ªæ¼”å‘˜ï¼Œå°±è·³è¿‡
                                continue

                            new_role = actor_from_frontend.get('role', '')
                            
                            cleaned_new_role = utils.clean_character_name_static(new_role)
                            cleaned_original_role = utils.clean_character_name_static(original_role)

                            if cleaned_new_role and cleaned_new_role != cleaned_original_role:
                                cache_entry = self.actor_db_manager.get_translation_from_db(text=cleaned_original_role, by_translated_text=True, cursor=cursor)
                                if cache_entry and 'original_text' in cache_entry:
                                    original_text_key = cache_entry['original_text']
                                    self.actor_db_manager.save_translation_to_db(
                                        cursor=cursor, original_text=original_text_key,
                                        translated_text=cleaned_new_role, engine_used="manual"
                                    )
                                    logger.debug(f"  âœ AIç¿»è¯‘ç¼“å­˜å·²æ›´æ–°: '{original_text_key}' ('{cleaned_original_role}' -> '{cleaned_new_role}')")
                                    updated_count += 1
                        if updated_count > 0:
                            logger.info(f"  âœ æˆåŠŸæ›´æ–°äº† {updated_count} æ¡ç¿»è¯‘ç¼“å­˜ã€‚")
                        else:
                            logger.info(f"  âœ æ— éœ€æ›´æ–°ç¿»è¯‘ç¼“å­˜ (è§’è‰²åæœªå‘ç”Ÿæœ‰æ•ˆå˜æ›´)ã€‚")
                        conn.commit()
                else:
                    logger.warning(f"  âœ æ— æ³•æ›´æ–°ç¿»è¯‘ç¼“å­˜ï¼šå†…å­˜ä¸­æ‰¾ä¸åˆ° ItemID {item_id} çš„åŸå§‹æ¼”å‘˜æ•°æ®ä¼šè¯ã€‚")
            except Exception as e:
                logger.error(f"  âœ æ‰‹åŠ¨å¤„ç†æœŸé—´æ›´æ–°ç¿»è¯‘ç¼“å­˜æ—¶å‘ç”Ÿé¡¶å±‚é”™è¯¯: {e}", exc_info=True)
            
            # ======================================================================
            # æ­¥éª¤ 3: APIå‰ç½®æ“ä½œ (æ›´æ–°æ¼”å‘˜å)
            # ======================================================================
            logger.info(f"  âœ æ‰‹åŠ¨å¤„ç†ï¼šæ­¥éª¤ 3/6: é€šè¿‡APIæ›´æ–°ç°æœ‰æ¼”å‘˜çš„åå­—...")
            # æ„å»º TMDb ID -> Emby Person ID å’Œ Emby Person ID -> å½“å‰åå­—çš„æ˜ å°„
            emby_id_to_name_map = {}
            for person in enriched_actors: # â˜…â˜…â˜… ç›´æ¥ä½¿ç”¨ enriched_actors
                person_emby_id = person.get("Id")
                if person_emby_id:
                    emby_id_to_name_map[person_emby_id] = person.get("Name")
            
            tmdb_to_emby_map = {}
            emby_id_to_name_map = {}
            for person in enriched_actors:
                person_tmdb_id = (person.get("ProviderIds") or {}).get("Tmdb")
                person_emby_id = person.get("Id")
                if person_tmdb_id and person_emby_id:
                    tmdb_to_emby_map[str(person_tmdb_id)] = person_emby_id
                    emby_id_to_name_map[person_emby_id] = person.get("Name")

            updated_names_count = 0
            for actor_from_frontend in manual_cast_list:
                tmdb_id_str = str(actor_from_frontend.get("tmdbId"))
                
                # åªå¤„ç†åœ¨æ˜ å°„ä¸­èƒ½æ‰¾åˆ°çš„ã€å·²å­˜åœ¨çš„æ¼”å‘˜
                actor_emby_id = tmdb_to_emby_map.get(tmdb_id_str)
                if not actor_emby_id: continue

                new_name = actor_from_frontend.get("name")
                original_name = emby_id_to_name_map.get(actor_emby_id)
                
                if new_name and original_name and new_name != original_name:
                    emby.update_person_details(
                        person_id=actor_emby_id, new_data={"Name": new_name},
                        emby_server_url=self.emby_url, emby_api_key=self.emby_api_key, user_id=self.emby_user_id
                    )
                    updated_names_count += 1
            
            if updated_names_count > 0:
                logger.info(f"  âœ æˆåŠŸé€šè¿‡ API æ›´æ–°äº† {updated_names_count} ä½æ¼”å‘˜çš„åå­—ã€‚")

            # ======================================================================
            # æ­¥éª¤ 4: æ–‡ä»¶è¯»ã€æ”¹ã€å†™ (åŒ…å«æœ€ç»ˆæ ¼å¼åŒ–)
            # ======================================================================
            logger.info(f"  âœ æ‰‹åŠ¨å¤„ç†ï¼šæ­¥éª¤ 4/6: è¯»å–åŸå§‹æ•°æ®ï¼Œè¯†åˆ«å¹¶è¡¥å…¨æ–°å¢æ¼”å‘˜çš„å…ƒæ•°æ®...")
            with open(main_json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            original_cast_data = (data.get('casts', {}) or data.get('credits', {})).get('cast', [])
            original_cast_map = {str(actor.get('id')): actor for actor in original_cast_data if actor.get('id')}

            new_actor_tmdb_ids = [
                int(actor.get("tmdbId")) for actor in manual_cast_list 
                if str(actor.get("tmdbId")) not in original_cast_map
            ]

            all_new_actors_metadata = {}
            if new_actor_tmdb_ids:
                with get_central_db_connection() as conn_new:
                    cursor_new = conn_new.cursor()
                    all_new_actors_metadata = self.actor_db_manager.get_full_actor_details_by_tmdb_ids(cursor_new, new_actor_tmdb_ids)

            new_cast_built = []
            
            with get_central_db_connection() as conn:
                cursor = conn.cursor()

                for actor_from_frontend in manual_cast_list:
                    tmdb_id_str = str(actor_from_frontend.get("tmdbId"))
                    if not tmdb_id_str: continue
                    
                    # --- A. å¤„ç†ç°æœ‰æ¼”å‘˜ ---
                    if tmdb_id_str in original_cast_map:
                        updated_actor_entry = original_cast_map[tmdb_id_str].copy()
                        updated_actor_entry['name'] = actor_from_frontend.get('name')
                        updated_actor_entry['character'] = actor_from_frontend.get('role')
                        new_cast_built.append(updated_actor_entry)
                    
                    # --- B. å¤„ç†æ–°å¢æ¼”å‘˜ ---
                    else:
                        logger.info(f"    â”œâ”€ å‘ç°æ–°æ¼”å‘˜: '{actor_from_frontend.get('name')}' (TMDb ID: {tmdb_id_str})ï¼Œå¼€å§‹è¡¥å…¨å…ƒæ•°æ®...")
                        
                        # B1: ä¼˜å…ˆä» å†…å­˜ ç¼“å­˜è·å–
                        person_details = all_new_actors_metadata.get(int(tmdb_id_str))
                        
                        # B2: å¦‚æœç¼“å­˜æ²¡æœ‰ï¼Œåˆ™ä» TMDb API è·å–å¹¶åå“º
                        if not person_details:
                            logger.debug(f"  âœ ç¼“å­˜æœªå‘½ä¸­ï¼Œä» TMDb API è·å–è¯¦æƒ…...")
                            person_details_from_api = tmdb.get_person_details_tmdb(tmdb_id_str, self.tmdb_api_key)
                            if person_details_from_api:
                                self.actor_db_manager.update_actor_metadata_from_tmdb(cursor, tmdb_id_str, person_details_from_api)
                                person_details = person_details_from_api # ä½¿ç”¨APIè¿”å›çš„æ•°æ®
                            else:
                                logger.warning(f"  âœ æ— æ³•è·å–TMDb ID {tmdb_id_str} çš„è¯¦æƒ…ï¼Œå°†ä½¿ç”¨åŸºç¡€ä¿¡æ¯è·³è¿‡ã€‚")
                                # å³ä½¿å¤±è´¥ï¼Œä¹Ÿåˆ›å»ºä¸€ä¸ªåŸºç¡€å¯¹è±¡ï¼Œé¿å…ä¸¢å¤±
                                person_details = {} 
                        else:
                            logger.debug(f"  âœ æˆåŠŸä»æ•°æ®åº“ç¼“å­˜å‘½ä¸­å…ƒæ•°æ®ã€‚")

                        # B3: æ„å»ºä¸€ä¸ªä¸ override æ–‡ä»¶æ ¼å¼ä¸€è‡´çš„æ–°æ¼”å‘˜å¯¹è±¡
                        new_actor_entry = {
                            "id": int(tmdb_id_str),
                            "name": actor_from_frontend.get('name'),
                            "character": actor_from_frontend.get('role'),
                            "original_name": person_details.get("original_name"),
                            "profile_path": person_details.get("profile_path"),
                            "adult": person_details.get("adult", False),
                            "gender": person_details.get("gender", 0),
                            "known_for_department": person_details.get("known_for_department", "Acting"),
                            "popularity": person_details.get("popularity", 0.0),
                            # æ–°å¢æ¼”å‘˜æ²¡æœ‰è¿™äº›ç”µå½±ç‰¹å®šçš„IDï¼Œè®¾ä¸ºNone
                            "cast_id": None, 
                            "credit_id": None,
                            "order": 999 # æ”¾åˆ°æœ€åï¼Œåç»­æ ¼å¼åŒ–æ­¥éª¤ä¼šé‡æ–°æ’åº
                        }
                        new_cast_built.append(new_actor_entry)

            # ======================================================================
            # æ­¥éª¤ 5: æœ€ç»ˆæ ¼å¼åŒ–å¹¶å†™å…¥æ–‡ä»¶ (é€»è¾‘ä¸å˜)
            # ======================================================================
            logger.info(f"  âœ æ‰‹åŠ¨å¤„ç†ï¼šæ­¥éª¤ 5/6: é‡å»ºæ¼”å‘˜åˆ—è¡¨å¹¶æ‰§è¡Œæœ€ç»ˆæ ¼å¼åŒ–...")
            genres = item_details.get("Genres", [])
            is_animation = "Animation" in genres or "åŠ¨ç”»" in genres or "Documentary" in genres or "çºªå½•" in genres
            final_formatted_cast = actor_utils.format_and_complete_cast_list(
                new_cast_built, is_animation, self.config, mode='manual'
            )
            # _build_cast_from_final_data ç¡®ä¿äº†æ‰€æœ‰å­—æ®µéƒ½å­˜åœ¨ï¼Œå³ä½¿æ˜¯None
            final_cast_for_json = self._build_cast_from_final_data(final_formatted_cast)

            if 'casts' in data:
                data['casts']['cast'] = final_cast_for_json
            elif 'credits' in data:
                data['credits']['cast'] = final_cast_for_json
            else:
                data.setdefault('credits', {})['cast'] = final_cast_for_json
            
            with open(main_json_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)

            if item_type == "Series":
                self._inject_cast_to_series_files(
                    target_dir=target_override_dir, cast_list=final_cast_for_json,
                    series_details=item_details
                )

            # ======================================================================
            # æ­¥éª¤ 6: è§¦å‘åˆ·æ–°å¹¶æ›´æ–°æ—¥å¿—
            # ======================================================================
            logger.info("  âœ æ‰‹åŠ¨å¤„ç†ï¼šæ­¥éª¤ 6/6: è§¦å‘ Emby åˆ·æ–°å¹¶æ›´æ–°å†…éƒ¨æ—¥å¿—...")
            
            emby.refresh_emby_item_metadata(
                item_emby_id=item_id,
                emby_server_url=self.emby_url,
                emby_api_key=self.emby_api_key,
                user_id_for_ops=self.emby_user_id,
                replace_all_metadata_param=True,
                item_name_for_log=item_name
            )

            # æ›´æ–°æˆ‘ä»¬è‡ªå·±çš„æ•°æ®åº“æ—¥å¿—å’Œç¼“å­˜
            with get_central_db_connection() as conn:
                cursor = conn.cursor()
                formatted_manual_metadata = None
                if tmdb_details_for_manual_extra:
                    formatted_manual_metadata = construct_metadata_payload(
                        item_type=item_type,
                        tmdb_data=tmdb_details_for_manual_extra,
                        aggregated_tmdb_data=aggregated_tmdb_data_manual,
                        emby_data_fallback=item_details
                    )
                # å†™å…¥æ•°æ®åº“
                self._upsert_media_metadata(
                    cursor=cursor,
                    item_type=item_type,
                    item_details_from_emby=item_details,
                    final_processed_cast=final_formatted_cast, 
                    source_data_package=formatted_manual_metadata, 
                )
                
                logger.info(f"  âœ æ­£åœ¨å°†æ‰‹åŠ¨å¤„ç†å®Œæˆçš„ã€Š{item_name}ã€‹å†™å…¥å·²å¤„ç†æ—¥å¿—...")
                self.log_db_manager.save_to_processed_log(cursor, item_id, item_name, score=10.0)
                self.log_db_manager.remove_from_failed_log(cursor, item_id)
                conn.commit()

            logger.info(f"  âœ æ‰‹åŠ¨å¤„ç† '{item_name}' æµç¨‹å®Œæˆã€‚")
            return True

        except Exception as e:
            logger.error(f"  âœ æ‰‹åŠ¨å¤„ç† '{item_name}' æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}", exc_info=True)
            return False
        finally:
            if item_id in self.manual_edit_cache:
                del self.manual_edit_cache[item_id]
                logger.trace(f"å·²æ¸…ç† ItemID {item_id} çš„æ‰‹åŠ¨ç¼–è¾‘ä¼šè¯ç¼“å­˜ã€‚")
    
    # --- ä¸ºå‰ç«¯å‡†å¤‡æ¼”å‘˜åˆ—è¡¨ç”¨äºç¼–è¾‘ ---
    def get_cast_for_editing(self, item_id: str) -> Optional[Dict[str, Any]]:
        """
        ã€V2 - Overrideæ–‡ä»¶ä¸­å¿ƒåŒ–ç‰ˆã€‘
        é‡æ„æ•°æ®æºï¼Œç¡®ä¿å‰ç«¯è·å–å’Œç¼–è¾‘çš„æ¼”å‘˜åˆ—è¡¨ï¼Œä¸ override æ–‡ä»¶ä¸­çš„â€œçœŸç†ä¹‹æºâ€å®Œå…¨ä¸€è‡´ã€‚
        - æ¼”å‘˜è¡¨ä¸»ä½“(åå­—, è§’è‰², é¡ºåº) æ¥è‡ª override ä¸»JSONæ–‡ä»¶ã€‚
        - é€šè¿‡ä¸€æ¬¡ Emby API è°ƒç”¨æ¥è·å– emby_person_id å¹¶è¿›è¡Œæ˜ å°„ã€‚
        """
        logger.info(f"  âœ ä¸ºç¼–è¾‘é¡µé¢å‡†å¤‡æ•°æ®ï¼šItemID {item_id}")
        
        try:
            # æ­¥éª¤ 1: è·å– Emby åŸºç¡€è¯¦æƒ… å’Œ ç”¨äºIDæ˜ å°„çš„Peopleåˆ—è¡¨
            emby_details = emby.get_emby_item_details(item_id, self.emby_url, self.emby_api_key, self.emby_user_id)
            if not emby_details:
                raise ValueError(f"åœ¨Embyä¸­æœªæ‰¾åˆ°é¡¹ç›® {item_id}")

            item_name_for_log = emby_details.get("Name", f"æœªçŸ¥(ID:{item_id})")
            tmdb_id = emby_details.get("ProviderIds", {}).get("Tmdb")
            item_type = emby_details.get("Type")
            if not tmdb_id:
                raise ValueError(f"é¡¹ç›® '{item_name_for_log}' ç¼ºå°‘ TMDb IDï¼Œæ— æ³•å®šä½å…ƒæ•°æ®æ–‡ä»¶ã€‚")

            # æ­¥éª¤ 2: è¯»å– override æ–‡ä»¶ï¼Œè·å–æƒå¨æ¼”å‘˜è¡¨
            cache_folder_name = "tmdb-movies2" if item_type == "Movie" else "tmdb-tv"
            target_override_dir = os.path.join(self.local_data_path, "override", cache_folder_name, tmdb_id)
            main_json_filename = "all.json" if item_type == "Movie" else "series.json"
            main_json_path = os.path.join(target_override_dir, main_json_filename)

            if not os.path.exists(main_json_path):
                raise FileNotFoundError(f"æ— æ³•ä¸º '{item_name_for_log}' å‡†å¤‡ç¼–è¾‘æ•°æ®ï¼šæ‰¾ä¸åˆ°ä¸»å…ƒæ•°æ®æ–‡ä»¶ '{main_json_path}'ã€‚è¯·ç¡®ä¿è¯¥é¡¹ç›®å·²è¢«è‡³å°‘å¤„ç†è¿‡ä¸€æ¬¡ã€‚")

            with open(main_json_path, 'r', encoding='utf-8') as f:
                override_data = json.load(f)
            
            cast_from_override = (override_data.get('casts', {}) or override_data.get('credits', {})).get('cast', [])
            logger.debug(f"  âœ æˆåŠŸä» override æ–‡ä»¶ä¸º '{item_name_for_log}' åŠ è½½äº† {len(cast_from_override)} ä½æ¼”å‘˜ã€‚")

            # æ­¥éª¤ 3: æ„å»º TMDb ID -> emby_person_id çš„æ˜ å°„
            tmdb_to_emby_map = {}
            for person in emby_details.get("People", []):
                person_tmdb_id = (person.get("ProviderIds") or {}).get("Tmdb")
                if person_tmdb_id:
                    tmdb_to_emby_map[str(person_tmdb_id)] = person.get("Id")
            
            # æ­¥éª¤ 4: ç»„è£…æœ€ç»ˆæ•°æ® (åˆå¹¶ override å†…å®¹ å’Œ emby_person_id)
            cast_for_frontend = []
            session_cache_map = {}
            
            with get_central_db_connection() as conn:
                cursor = conn.cursor()
                for actor_data in cast_from_override:
                    actor_tmdb_id = actor_data.get('id')
                    if not actor_tmdb_id: continue
                    
                    emby_person_id = tmdb_to_emby_map.get(str(actor_tmdb_id))
                    
                    # ä»æœ¬åœ°æ•°æ®åº“è·å–å¤´åƒ
                    image_url = None
                    # actor_data å°±æ˜¯ä» override æ–‡ä»¶é‡Œè¯»å‡ºçš„é‚£æ¡è®°å½•ï¼Œå®ƒåŒ…å«äº†æœ€å‡†ç¡®çš„ profile_path
                    profile_path = actor_data.get("profile_path")
                    if profile_path:
                        # å¦‚æœæ˜¯å®Œæ•´çš„ URL (æ¥è‡ªè±†ç“£)ï¼Œåˆ™ç›´æ¥ä½¿ç”¨
                        if profile_path.startswith('http'):
                            image_url = profile_path
                        # å¦åˆ™ï¼Œè®¤ä¸ºæ˜¯ TMDb çš„ç›¸å¯¹è·¯å¾„ï¼Œè¿›è¡Œæ‹¼æ¥
                        else:
                            image_url = f"https://wsrv.nl/?url=https://image.tmdb.org/t/p/w185{profile_path}"
                    
                    # æ¸…ç†è§’è‰²å
                    original_role = actor_data.get('character', '')
                    session_cache_map[str(actor_tmdb_id)] = original_role
                    cleaned_role_for_display = utils.clean_character_name_static(original_role)

                    # ä¸ºå‰ç«¯å‡†å¤‡çš„æ•°æ®
                    cast_for_frontend.append({
                        "tmdbId": actor_tmdb_id,
                        "name": actor_data.get('name'),
                        "role": cleaned_role_for_display,
                        "imageUrl": image_url,
                        "emby_person_id": emby_person_id
                    })
                    
            # æ­¥éª¤ 5: ç¼“å­˜ä¼šè¯æ•°æ®å¹¶å‡†å¤‡æœ€ç»ˆå“åº”
            self.manual_edit_cache[item_id] = session_cache_map
            logger.debug(f"å·²ä¸º ItemID {item_id} ç¼“å­˜äº† {len(session_cache_map)} æ¡ç”¨äºæ‰‹åŠ¨ç¼–è¾‘ä¼šè¯çš„æ¼”å‘˜æ•°æ®ã€‚")

            failed_log_info = {}
            with get_central_db_connection() as conn:
                cursor = conn.cursor()
                cursor.execute("SELECT error_message, score FROM failed_log WHERE item_id = %s", (item_id,))
                row = cursor.fetchone()
                if row: failed_log_info = dict(row)

            response_data = {
                "item_id": item_id,
                "item_name": emby_details.get("Name"),
                "item_type": emby_details.get("Type"),
                "image_tag": emby_details.get('ImageTags', {}).get('Primary'),
                "original_score": failed_log_info.get("score"),
                "review_reason": failed_log_info.get("error_message"),
                "current_emby_cast": cast_for_frontend,
                "search_links": {
                    "baidu": utils.generate_search_url('baike', emby_details.get("Name"), emby_details.get("ProductionYear")),
                    "wikipedia": utils.generate_search_url('wikipedia', emby_details.get("Name"), emby_details.get("ProductionYear")),
                    "google": utils.generate_search_url('google', emby_details.get("Name"), emby_details.get("ProductionYear"))
                }
            }
            return response_data

        except Exception as e:
            logger.error(f"  âœ è·å–ç¼–è¾‘æ•°æ®å¤±è´¥ for ItemID {item_id}: {e}", exc_info=True)
            return None
    
    # --- å®æ—¶è¦†ç›–ç¼“å­˜åŒæ­¥ ---
    def sync_single_item_assets(self, item_id: str, 
                                update_description: Optional[str] = None, 
                                sync_timestamp_iso: Optional[str] = None,
                                final_cast_override: Optional[List[Dict[str, Any]]] = None,
                                episode_ids_to_sync: Optional[List[str]] = None,
                                metadata_override: Optional[Dict[str, Any]] = None): 
        """
        çº¯ç²¹çš„é¡¹ç›®ç»ç†ï¼Œè´Ÿè´£æ¥æ”¶è®¾è®¡å¸ˆçš„æ‰€æœ‰ææ–™ï¼Œå¹¶åˆ†å‘ç»™æ–½å·¥é˜Ÿã€‚
        """
        log_prefix = f"å®æ—¶è¦†ç›–ç¼“å­˜åŒæ­¥"
        logger.trace(f"--- {log_prefix} å¼€å§‹æ‰§è¡Œ (ItemID: {item_id}) ---")

        if not self.local_data_path:
            logger.warning(f"  âœ {log_prefix} ä»»åŠ¡è·³è¿‡ï¼Œå› ä¸ºæœªé…ç½®æœ¬åœ°æ•°æ®æºè·¯å¾„ã€‚")
            return

        try:
            item_details = emby.get_emby_item_details(
                item_id, self.emby_url, self.emby_api_key, self.emby_user_id,
                fields="ProviderIds,Type,Name,IndexNumber,ParentIndexNumber"
            )
            if not item_details:
                raise ValueError("åœ¨Embyä¸­æ‰¾ä¸åˆ°è¯¥é¡¹ç›®ã€‚")

            tmdb_id = item_details.get("ProviderIds", {}).get("Tmdb")
            if not tmdb_id:
                logger.warning(f"{log_prefix} é¡¹ç›® '{item_details.get('Name')}' ç¼ºå°‘TMDb IDï¼Œæ— æ³•åŒæ­¥ã€‚")
                return

            # 1. è°ƒåº¦å¤–å¢™æ–½å·¥é˜Ÿ
            self.sync_item_images(item_details, update_description, episode_ids_to_sync=episode_ids_to_sync)
            
            # 2. è°ƒåº¦ç²¾è£…ä¿®æ–½å·¥é˜Ÿï¼Œå¹¶æŠŠæ‰€æœ‰å›¾çº¸å’Œææ–™éƒ½ç»™ä»–
            self.sync_item_metadata(
                item_details, 
                tmdb_id, 
                final_cast_override=final_cast_override, 
                episode_ids_to_sync=episode_ids_to_sync,
                metadata_override=metadata_override 
            )

            # 3. è®°å½•å·¥æ—¶
            timestamp_to_log = sync_timestamp_iso or datetime.now(timezone.utc).isoformat()
            with get_central_db_connection() as conn:
                cursor = conn.cursor()
                self.log_db_manager.mark_assets_as_synced(
                    cursor, 
                    item_id, 
                    timestamp_to_log
                )
                conn.commit()
            
            logger.trace(f"--- {log_prefix} æˆåŠŸå®Œæˆ (ItemID: {item_id}) ---")

        except Exception as e:
            logger.error(f"{log_prefix} æ‰§è¡Œæ—¶å‘ç”Ÿé”™è¯¯ (ItemID: {item_id}): {e}", exc_info=True)

    # --- å¤‡ä»½å›¾ç‰‡ ---
    def sync_item_images(self, item_details: Dict[str, Any], update_description: Optional[str] = None, episode_ids_to_sync: Optional[List[str]] = None) -> bool:
        """
        ã€æ–°å¢-é‡æ„ã€‘è¿™ä¸ªæ–¹æ³•è´Ÿè´£åŒæ­¥ä¸€ä¸ªåª’ä½“é¡¹ç›®çš„æ‰€æœ‰ç›¸å…³å›¾ç‰‡ã€‚
        å®ƒä» _process_item_core_logic ä¸­æå–å‡ºæ¥ï¼Œä»¥ä¾¿å¤ç”¨ã€‚
        """
        item_id = item_details.get("Id")
        item_type = item_details.get("Type")
        item_name_for_log = item_details.get("Name", f"æœªçŸ¥é¡¹ç›®(ID:{item_id})")
        
        if not all([item_id, item_type, self.local_data_path]):
            logger.error(f"  âœ è·³è¿‡ '{item_name_for_log}'ï¼Œå› ä¸ºç¼ºå°‘IDã€ç±»å‹æˆ–æœªé…ç½®æœ¬åœ°æ•°æ®è·¯å¾„ã€‚")
            return False

        try:
            # --- å‡†å¤‡å·¥ä½œ (ç›®å½•ã€TMDb IDç­‰) ---
            log_prefix = "è¦†ç›–ç¼“å­˜-å›¾ç‰‡å¤‡ä»½ï¼š"
            tmdb_id = item_details.get("ProviderIds", {}).get("Tmdb")
            if not tmdb_id:
                logger.warning(f"  âœ {log_prefix} é¡¹ç›® '{item_name_for_log}' ç¼ºå°‘TMDb IDï¼Œæ— æ³•ç¡®å®šè¦†ç›–ç›®å½•ï¼Œè·³è¿‡ã€‚")
                return False
            
            cache_folder_name = "tmdb-movies2" if item_type == "Movie" else "tmdb-tv"
            base_override_dir = os.path.join(self.local_data_path, "override", cache_folder_name, tmdb_id)
            image_override_dir = os.path.join(base_override_dir, "images")
            os.makedirs(image_override_dir, exist_ok=True)

            # --- å®šä¹‰æ‰€æœ‰å¯èƒ½çš„å›¾ç‰‡æ˜ å°„ ---
            full_image_map = {"Primary": "poster.jpg", "Backdrop": "fanart.jpg", "Logo": "clearlogo.png"}
            if item_type == "Movie":
                full_image_map["Thumb"] = "landscape.jpg"

            # â˜…â˜…â˜… å…¨æ–°é€»è¾‘åˆ†å‘ â˜…â˜…â˜…
            images_to_sync = {}
            
            # æ¨¡å¼ä¸€ï¼šç²¾å‡†åŒæ­¥ (å½“æè¿°å­˜åœ¨æ—¶)
            if update_description:
                log_prefix = "[è¦†ç›–ç¼“å­˜-å›¾ç‰‡å¤‡ä»½]"
                logger.trace(f"{log_prefix} æ­£åœ¨è§£ææè¿°: '{update_description}'")
                
                # å®šä¹‰å…³é”®è¯åˆ°Embyå›¾ç‰‡ç±»å‹çš„æ˜ å°„ (ä½¿ç”¨å°å†™ä»¥æ–¹ä¾¿åŒ¹é…)
                keyword_map = {
                    "primary": "Primary",
                    "backdrop": "Backdrop",
                    "logo": "Logo",
                    "thumb": "Thumb", # ç”µå½±ç¼©ç•¥å›¾
                    "banner": "Banner" # å‰§é›†æ¨ªå¹… (å¦‚æœéœ€è¦å¯ä»¥æ·»åŠ )
                }
                
                desc_lower = update_description.lower()
                found_specific_image = False
                for keyword, image_type_api in keyword_map.items():
                    if keyword in desc_lower and image_type_api in full_image_map:
                        images_to_sync[image_type_api] = full_image_map[image_type_api]
                        logger.trace(f"{log_prefix} åŒ¹é…åˆ°å…³é”®è¯ '{keyword}'ï¼Œå°†åªåŒæ­¥ {image_type_api} å›¾ç‰‡ã€‚")
                        found_specific_image = True
                        break # æ‰¾åˆ°ç¬¬ä¸€ä¸ªåŒ¹é…å°±åœæ­¢ï¼Œé¿å…é‡å¤
                
                if not found_specific_image:
                    logger.trace(f"{log_prefix} æœªèƒ½åœ¨æè¿°ä¸­æ‰¾åˆ°å¯è¯†åˆ«çš„å›¾ç‰‡å…³é”®è¯ï¼Œå°†å›é€€åˆ°å®Œå…¨åŒæ­¥ã€‚")
                    images_to_sync = full_image_map # å›é€€
            
            # æ¨¡å¼äºŒï¼šå®Œå…¨åŒæ­¥ (é»˜è®¤æˆ–å›é€€)
            else:
                log_prefix = "[è¦†ç›–ç¼“å­˜-å›¾ç‰‡å¤‡ä»½]"
                logger.trace(f"  âœ {log_prefix} æœªæä¾›æ›´æ–°æè¿°ï¼Œå°†åŒæ­¥æ‰€æœ‰ç±»å‹çš„å›¾ç‰‡ã€‚")
                images_to_sync = full_image_map

            # --- æ‰§è¡Œä¸‹è½½ ---
            if not episode_ids_to_sync:
                logger.info(f"  âœ {log_prefix} å¼€å§‹ä¸º '{item_name_for_log}' ä¸‹è½½ {len(images_to_sync)} å¼ ä¸»å›¾ç‰‡è‡³è¦†ç›–ç¼“å­˜")
                for image_type, filename in images_to_sync.items():
                    if self.is_stop_requested():
                        logger.warning(f"  ğŸš« {log_prefix} æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œä¸­æ­¢å›¾ç‰‡ä¸‹è½½ã€‚")
                        return False
                    emby.download_emby_image(item_id, image_type, os.path.join(image_override_dir, filename), self.emby_url, self.emby_api_key)
            
            # --- åˆ†é›†å›¾ç‰‡é€»è¾‘ ---
            if item_type == "Series" and self.config.get(constants.CONFIG_OPTION_BACKUP_EPISODE_IMAGE, False):
                children_to_process = []
                # è·å–æ‰€æœ‰å­é¡¹ä¿¡æ¯ï¼Œç”¨äºæŸ¥æ‰¾
                all_children = emby.get_series_children(item_id, self.emby_url, self.emby_api_key, self.emby_user_id, series_name_for_log=item_name_for_log) or []
                
                if episode_ids_to_sync:
                    # æ¨¡å¼ä¸€ï¼šåªå¤„ç†æŒ‡å®šçš„åˆ†é›†
                    logger.info(f"  âœ {log_prefix} å°†åªåŒæ­¥ {len(episode_ids_to_sync)} ä¸ªæŒ‡å®šåˆ†é›†çš„å›¾ç‰‡ã€‚")
                    id_set = set(episode_ids_to_sync)
                    children_to_process = [child for child in all_children if child.get("Id") in id_set]
                elif images_to_sync == full_image_map:
                    # æ¨¡å¼äºŒï¼šå¤„ç†æ‰€æœ‰å­é¡¹ï¼ˆåŸé€»è¾‘ï¼‰
                    children_to_process = all_children

                for child in children_to_process:
                    if self.is_stop_requested():
                        logger.warning(f"  ğŸš« {log_prefix} æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œä¸­æ­¢å­é¡¹ç›®å›¾ç‰‡ä¸‹è½½ã€‚")
                        return False
                    child_type, child_id = child.get("Type"), child.get("Id")
                    if child_type == "Season":
                        season_number = child.get("IndexNumber")
                        if season_number is not None:
                            emby.download_emby_image(child_id, "Primary", os.path.join(image_override_dir, f"season-{season_number}.jpg"), self.emby_url, self.emby_api_key)
                    elif child_type == "Episode":
                        season_number, episode_number = child.get("ParentIndexNumber"), child.get("IndexNumber")
                        if season_number is not None and episode_number is not None:
                            emby.download_emby_image(child_id, "Primary", os.path.join(image_override_dir, f"season-{season_number}-episode-{episode_number}.jpg"), self.emby_url, self.emby_api_key)
            
            logger.trace(f"  âœ {log_prefix} æˆåŠŸå®Œæˆ '{item_name_for_log}' çš„è¦†ç›–ç¼“å­˜-å›¾ç‰‡å¤‡ä»½ã€‚")
            return True
        except Exception as e:
            logger.error(f"{log_prefix} ä¸º '{item_name_for_log}' å¤‡ä»½å›¾ç‰‡æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}", exc_info=True)
            return False
    
    # --- ä» TMDb ç›´æ¥ä¸‹è½½å›¾ç‰‡ (ç”¨äºå®æ—¶ç›‘æ§/é¢„å¤„ç†) ---
    def download_images_from_tmdb(self, tmdb_id: str, item_type: str) -> bool:
        """
        ã€ä¸»åŠ¨ç›‘æ§ä¸“ç”¨ã€‘
        ç›´æ¥ä» TMDb API è·å–å¹¶ä¸‹è½½å›¾ç‰‡åˆ°æœ¬åœ° override ç›®å½•ã€‚
        ç”¨äºåœ¨ Emby å°šæœªå…¥åº“æ—¶ï¼Œé¢„å…ˆå‡†å¤‡å¥½å›¾ç‰‡ç´ æã€‚
        """
        if not tmdb_id or not self.local_data_path:
            logger.error(f"  âœ [TMDbå›¾ç‰‡é¢„å–] ç¼ºå°‘ TMDb ID æˆ–æœ¬åœ°è·¯å¾„é…ç½®ï¼Œæ— æ³•ä¸‹è½½ã€‚")
            return False

        try:
            log_prefix = "[TMDbå›¾ç‰‡é¢„å–]"
            
            # 1. å‡†å¤‡ç›®å½• (ä¿æŒä¸ sync_item_images ä¸€è‡´çš„ç›®å½•ç»“æ„)
            cache_folder_name = "tmdb-movies2" if item_type == "Movie" else "tmdb-tv"
            base_override_dir = os.path.join(self.local_data_path, "override", cache_folder_name, str(tmdb_id))
            image_override_dir = os.path.join(base_override_dir, "images")
            os.makedirs(image_override_dir, exist_ok=True)

            # 2. ä» TMDb è·å–å›¾ç‰‡æ•°æ®
            logger.info(f"  âœ {log_prefix} æ­£åœ¨ä» TMDb API è·å–å›¾ç‰‡é“¾æ¥ (ID: {tmdb_id})...")
            
            tmdb_data = None
            if item_type == "Movie":
                tmdb_data = tmdb.get_movie_details(int(tmdb_id), self.tmdb_api_key, append_to_response="images")
            elif item_type == "Series":
                tmdb_data = tmdb.get_tv_details(int(tmdb_id), self.tmdb_api_key, append_to_response="images,seasons")
            
            if not tmdb_data:
                logger.error(f"  âœ {log_prefix} æ— æ³•è·å– TMDb æ•°æ®ã€‚")
                return False
            
            # â˜…â˜…â˜… è¯»å–è¯­è¨€åå¥½é…ç½® â˜…â˜…â˜…
            lang_pref = self.config.get(constants.CONFIG_OPTION_TMDB_IMAGE_LANGUAGE_PREFERENCE, 'zh')
            original_lang_code = tmdb_data.get("original_language", "en")
            
            logger.debug(f"  âœ {log_prefix} å›¾ç‰‡åå¥½: {'ä¸­æ–‡ä¼˜å…ˆ' if lang_pref == 'zh' else 'åŸè¯­è¨€ä¼˜å…ˆ'} (åŸè¯­è¨€: {original_lang_code})")

            # =========================================================
            # â˜…â˜…â˜… å®šä¹‰é€šç”¨å›¾ç‰‡é€‰æ‹©é€»è¾‘ (ä¸å†å·æ‡’ï¼Œç»Ÿä¸€é€»è¾‘) â˜…â˜…â˜…
            # =========================================================
            def _select_best_image(image_list: list, preference: str, orig_lang: str) -> Optional[str]:
                if not image_list:
                    return None
                
                selected = None
                if preference == 'zh':
                    # ç­–ç•¥ A: ä¸­æ–‡ > åŸè¯­è¨€ > è‹±æ–‡ > ç¬¬ä¸€ä¸ª
                    for img in image_list:
                        if img.get("iso_639_1") == "zh": return img["file_path"]
                    for img in image_list:
                        if img.get("iso_639_1") == orig_lang: return img["file_path"]
                    for img in image_list:
                        if img.get("iso_639_1") == "en": return img["file_path"]
                else:
                    # ç­–ç•¥ B: åŸè¯­è¨€ > è‹±æ–‡ > ä¸­æ–‡ > ç¬¬ä¸€ä¸ª
                    for img in image_list:
                        if img.get("iso_639_1") == orig_lang: return img["file_path"]
                    if orig_lang != 'en':
                        for img in image_list:
                            if img.get("iso_639_1") == "en": return img["file_path"]
                    for img in image_list:
                        if img.get("iso_639_1") == "zh": return img["file_path"]
                
                # å…œåº•ï¼šè¿”å›è¯„åˆ†æœ€é«˜çš„ç¬¬ä¸€ä¸ªï¼ˆTMDbé»˜è®¤å·²æŒ‰è¯„åˆ†æ’åºï¼‰
                return image_list[0]["file_path"]

            # 3. å®šä¹‰ä¸‹è½½ä»»åŠ¡åˆ—è¡¨
            downloads = []
            images_node = tmdb_data.get("images", {})

            # --- A. æµ·æŠ¥ (Poster) ---
            # â˜…â˜…â˜… ä¿®å¤ï¼šä¸å†ç›´æ¥å– poster_pathï¼Œè€Œæ˜¯å» posters åˆ—è¡¨é‡ŒæŒ‘ â˜…â˜…â˜…
            posters_list = images_node.get("posters", [])
            selected_poster = _select_best_image(posters_list, lang_pref, original_lang_code)
            
            # å¦‚æœåˆ—è¡¨é‡Œæ²¡æŒ‘å‡ºæ¥ï¼ˆæå°‘è§ï¼‰ï¼Œå†ç”¨é¡¶å±‚å­—æ®µå…œåº•
            if not selected_poster:
                selected_poster = tmdb_data.get("poster_path")
            
            if selected_poster:
                downloads.append((selected_poster, "poster.jpg"))
            
            # --- B. èƒŒæ™¯ (Backdrop / Fanart) ---
            # èƒŒæ™¯å›¾é€šå¸¸é¦–é€‰æ— æ–‡å­—(null)ï¼Œå…¶æ¬¡æ‰çœ‹è¯­è¨€ã€‚
            # è¿™é‡Œæˆ‘ä»¬ç¨å¾®å˜é€šä¸€ä¸‹ï¼šå¦‚æœç”¨æˆ·é€‰äº†åŸè¯­è¨€ä¼˜å…ˆï¼Œæˆ‘ä»¬å°è¯•æ‰¾åŸè¯­è¨€çš„ï¼›
            # å¦åˆ™ï¼ˆä¸­æ–‡ä¼˜å…ˆï¼‰ï¼Œæˆ‘ä»¬å€¾å‘äºæ‰¾æ— æ–‡å­—çš„æˆ–è€…ä¸­æ–‡çš„ã€‚
            # ä½†ä¸ºäº†ç®€å•ä¸”ç¬¦åˆâ€œåŸå›¾â€çš„é«˜è´¨é‡è¦æ±‚ï¼ŒèƒŒæ™¯å›¾æˆ‘ä»¬é€šå¸¸è¿˜æ˜¯ä¿¡ä»» TMDb çš„é»˜è®¤æ’åºï¼ˆé€šå¸¸æ˜¯æ— æ–‡å­—çš„é«˜åˆ†å›¾ï¼‰ã€‚
            # ä¸è¿‡æ—¢ç„¶ä½ è¦æ±‚äº†ï¼Œæˆ‘ä¹Ÿè®©å®ƒèµ°ä¸€éé€‰æ‹©é€»è¾‘ï¼Œä½†æŠŠ 'null' (æ— æ–‡å­—) è§†ä¸ºæœ€é«˜ä¼˜å…ˆçº§ã€‚
            
            backdrops_list = images_node.get("backdrops", [])
            selected_backdrop = None
            
            # ç‰¹æ®Šé€»è¾‘ï¼šèƒŒæ™¯å›¾ä¼˜å…ˆæ‰¾æ— æ–‡å­— (iso_639_1 is None or 'null')
            for img in backdrops_list:
                if img.get("iso_639_1") in [None, "null"]:
                    selected_backdrop = img["file_path"]
                    break
            
            # å¦‚æœæ²¡æ‰¾åˆ°æ— æ–‡å­—çš„ï¼Œå†æŒ‰è¯­è¨€åå¥½æ‰¾
            if not selected_backdrop:
                selected_backdrop = _select_best_image(backdrops_list, lang_pref, original_lang_code)
            
            # å…œåº•
            if not selected_backdrop:
                selected_backdrop = tmdb_data.get("backdrop_path")

            if selected_backdrop:
                downloads.append((selected_backdrop, "fanart.jpg"))
                # é¡ºä¾¿æ‹¿ç¬¬ä¸€å¼ èƒŒæ™¯åš landscape (ç¼©ç•¥å›¾)
                downloads.append((selected_backdrop, "landscape.jpg"))

            # --- C. Logo (Clearlogo) ---
            logos_list = images_node.get("logos", [])
            selected_logo = _select_best_image(logos_list, lang_pref, original_lang_code)
            
            if selected_logo:
                downloads.append((selected_logo, "clearlogo.png"))

            # --- D. å‰§é›†ç‰¹æœ‰ï¼šå­£æµ·æŠ¥ ---
            if item_type == "Series":
                seasons = tmdb_data.get("seasons", [])
                for season in seasons:
                    s_num = season.get("season_number")
                    # å­£æµ·æŠ¥é€šå¸¸åœ¨é¡¶å±‚æ•°æ®é‡Œæ²¡æœ‰è¯¦ç»†çš„ images åˆ—è¡¨ï¼Œåªèƒ½æ‹¿ poster_path
                    # è¦æƒ³ç²¾ç¡®æ§åˆ¶å­£æµ·æŠ¥è¯­è¨€ï¼Œéœ€è¦å•ç‹¬è¯·æ±‚æ¯ä¸€å­£çš„è¯¦æƒ…ï¼Œè¿™ä¼šå¢åŠ å¾ˆå¤š API è¯·æ±‚ã€‚
                    # è€ƒè™‘åˆ°æ€§èƒ½ï¼Œå­£æµ·æŠ¥è¿™é‡Œæš‚æ—¶ä¿æŒåŸæ ·ï¼ˆé€šå¸¸å­£æµ·æŠ¥æ–‡å­—è¾ƒå°‘ï¼‰ã€‚
                    s_poster = season.get("poster_path")
                    if s_num is not None and s_poster:
                        downloads.append((s_poster, f"season-{s_num}.jpg"))

            # 4. æ‰§è¡Œä¸‹è½½
            base_image_url = "https://wsrv.nl/?url=https://image.tmdb.org/t/p/original"
            success_count = 0
            
            import requests
            
            for tmdb_path, local_name in downloads:
                if not tmdb_path: continue
                
                full_url = f"{base_image_url}{tmdb_path}"
                save_path = os.path.join(image_override_dir, local_name)
                
                # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ä¸”å¤§å°ä¸ä¸º0ï¼Œè·³è¿‡
                if os.path.exists(save_path) and os.path.getsize(save_path) > 0:
                    continue

                try:
                    # ä½¿ç”¨ç®€å•çš„ requests ä¸‹è½½ï¼Œå¸¦è¶…æ—¶
                    resp = requests.get(full_url, timeout=15)
                    if resp.status_code == 200:
                        with open(save_path, 'wb') as f:
                            f.write(resp.content)
                        success_count += 1
                        # ç¨å¾®å»¶æ—¶é¿å…è§¦å‘ TMDb é€Ÿç‡é™åˆ¶
                        time_module.sleep(0.1)
                except Exception as e:
                    logger.warning(f"  âœ ä¸‹è½½å›¾ç‰‡å¤±è´¥ {local_name}: {e}")

            logger.info(f"  âœ {log_prefix} å›¾ç‰‡é¢„å–å®Œæˆï¼Œå…±ä¸‹è½½ {success_count} å¼ å›¾ç‰‡ã€‚")
            return True

        except Exception as e:
            logger.error(f"{log_prefix} å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}", exc_info=True)
            return False

    # --- å¤‡ä»½å…ƒæ•°æ® ---
    def sync_item_metadata(self, item_details: Dict[str, Any], tmdb_id: str,
                       final_cast_override: Optional[List[Dict[str, Any]]] = None,
                       episode_ids_to_sync: Optional[List[str]] = None,
                       metadata_override: Optional[Dict[str, Any]] = None):
        """
        ã€V6 - æœ€ç»ˆç‰ˆã€‘
        ä¸å†ä» cache å¤åˆ¶æ–‡ä»¶ï¼Œè€Œæ˜¯åŸºäºæ¨¡æ¿å’Œç°æœ‰æ•°æ®æ„å»º override æ–‡ä»¶ã€‚
        åŒæ—¶æ”¯æŒä¼ é€’ TMDb åˆ†é›†åŸå§‹æ•°æ®ã€‚
        """
        item_id = item_details.get("Id")
        item_name_for_log = item_details.get("Name", f"æœªçŸ¥é¡¹ç›®(ID:{item_id})")
        item_type = item_details.get("Type")
        log_prefix = "[è¦†ç›–ç¼“å­˜-å…ƒæ•°æ®å†™å…¥]"

        # å®šä¹‰æ ¸å¿ƒè·¯å¾„
        cache_folder_name = "tmdb-movies2" if item_type == "Movie" else "tmdb-tv"
        target_override_dir = os.path.join(self.local_data_path, "override", cache_folder_name, tmdb_id)
        main_json_filename = "all.json" if item_type == "Movie" else "series.json"
        main_json_path = os.path.join(target_override_dir, main_json_filename)

        # ç¡®ä¿ç›®æ ‡ç›®å½•å­˜åœ¨
        os.makedirs(target_override_dir, exist_ok=True)

        perfect_cast_for_injection = []
        
        # å®šä¹‰ä¸€ä¸ªå˜é‡ç”¨æ¥å­˜åˆ†é›†æ•°æ® 
        tmdb_episodes_data = None 
        # â˜…â˜…â˜… æ–°å¢ï¼šå®šä¹‰ä¸€ä¸ªå˜é‡ç”¨æ¥å­˜åˆ†å­£æ•°æ® â˜…â˜…â˜…
        tmdb_seasons_data = None

        # å¦‚æœæœ‰å…ƒæ•°æ®è¦†ç›–ï¼Œå…ˆå†™å…¥å…ƒæ•°æ® 
        if metadata_override:
            logger.trace(f"  âœ {log_prefix} æ£€æµ‹åˆ°å…ƒæ•°æ®ä¿®æ­£ï¼Œæ­£åœ¨å†™å…¥ä¸»æ–‡ä»¶...")
            
            #  åœ¨åˆ é™¤å‰ï¼Œå…ˆæŠŠåˆ†é›†æ•°æ®æå–å‡ºæ¥ï¼ 
            if 'episodes_details' in metadata_override:
                tmdb_episodes_data = metadata_override['episodes_details']
            
            # â˜…â˜…â˜… æ–°å¢ï¼šæå–åˆ†å­£æ•°æ® â˜…â˜…â˜…
            if 'seasons_details' in metadata_override:
                tmdb_seasons_data = metadata_override['seasons_details']

            # 1. åˆ›å»ºä¸€ä¸ªå‰¯æœ¬ï¼Œé¿å…ä¿®æ”¹åŸå§‹å¯¹è±¡å½±å“åç»­é€»è¾‘
            data_to_write = metadata_override.copy()

            # --- å…³é”®è¯æ˜ å°„å¤„ç†å¹¶å†™å…¥ tags.json ---
            if self.config.get(constants.CONFIG_OPTION_KEYWORD_TO_TAGS, False):
                try:
                    # A. è·å–æ˜ å°„è¡¨ (æ•°æ®åº“ä¼˜å…ˆ -> Utilså…œåº•)
                    mapping_data = settings_db.get_setting('keyword_mapping')
                    if not mapping_data:
                        mapping_data = utils.DEFAULT_KEYWORD_MAPPING
                    
                    # B. æ„å»º ID -> ä¸­æ–‡Label çš„æŸ¥æ‰¾è¡¨
                    keyword_map = {}
                    for entry in mapping_data:
                        label = entry.get('label')
                        if label:
                            for kid in entry.get('ids', []):
                                keyword_map[str(kid)] = label
                    
                    # C. ä»å…ƒæ•°æ®ä¸­æå–åŸå§‹å…³é”®è¯
                    source_keywords = []
                    kw_data = data_to_write.get('keywords', {})
                    if isinstance(kw_data, dict):
                        # å…¼å®¹ Movie ('keywords') å’Œ Series ('results') çš„ç»“æ„
                        source_keywords = kw_data.get('keywords') or kw_data.get('results') or []
                    
                    # D. è¿‡æ»¤å¹¶æ˜ å°„
                    final_tags = set()
                    for k in source_keywords:
                        if isinstance(k, dict):
                            kid = str(k.get('id', ''))
                            if kid in keyword_map:
                                final_tags.add(keyword_map[kid])
                    
                    # E. å†™å…¥ tags.json (å¦‚æœå­˜åœ¨æ˜ å°„ç»“æœ)
                    tags_json_path = os.path.join(target_override_dir, "tags.json")
                    if final_tags:
                        with open(tags_json_path, 'w', encoding='utf-8') as f:
                            json.dump({"tags": list(final_tags)}, f, ensure_ascii=False, indent=2)
                        logger.info(f"  âœ {log_prefix} å·²æ ¹æ®æ˜ å°„è¡¨ç”Ÿæˆ tags.jsonï¼ŒåŒ…å« {len(final_tags)} ä¸ªä¸­æ–‡æ ‡ç­¾ã€‚")
                    else:
                        # å¦‚æœæ²¡æœ‰åŒ¹é…çš„æ ‡ç­¾ï¼Œä¸”å­˜åœ¨æ—§æ–‡ä»¶ï¼Œåˆ™åˆ é™¤æ—§æ–‡ä»¶ä»¥ä¿æŒå¹²å‡€
                        if os.path.exists(tags_json_path):
                            os.remove(tags_json_path)

                except Exception as e_tags:
                    logger.warning(f"  âœ {log_prefix} å¤„ç†å…³é”®è¯æ˜ å°„å†™å…¥ tags.json æ—¶å‘ç”Ÿé”™è¯¯: {e_tags}")
            
            # 2. å‰”é™¤ä¸éœ€è¦å†™å…¥ä¸»æ–‡ä»¶çš„ä¸´æ—¶å­—æ®µ
            # (æ³¨æ„ï¼šè¿™é‡Œåˆ é™¤äº† episodes_detailsï¼Œæ‰€ä»¥ä¸Šé¢å¿…é¡»å…ˆæå–)
            keys_to_remove = ['seasons_details', 'episodes_details', 'release_dates'] 
            for k in keys_to_remove:
                if k in data_to_write:
                    del data_to_write[k]

            # 3. å†™å…¥å‡€åŒ–åçš„æ•°æ®
            with open(main_json_path, 'w', encoding='utf-8') as f:
                json.dump(data_to_write, f, ensure_ascii=False, indent=2)

        if final_cast_override is not None:
            # --- è§’è‰²ä¸€ï¼šä¸»ä½“ç²¾è£…ä¿® ---
            new_cast_for_json = self._build_cast_from_final_data(final_cast_override)
            perfect_cast_for_injection = new_cast_for_json

            # æ­¥éª¤ 2: ä¿®æ”¹æˆ–åˆ›å»ºä¸»æ–‡ä»¶
            if not os.path.exists(main_json_path):
                skeleton = utils.MOVIE_SKELETON_TEMPLATE if item_type == "Movie" else utils.SERIES_SKELETON_TEMPLATE
                data = json.loads(json.dumps(skeleton))
            else:
                with open(main_json_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)

            if 'casts' in data: data['casts']['cast'] = perfect_cast_for_injection
            else: data.setdefault('credits', {})['cast'] = perfect_cast_for_injection
            
            with open(main_json_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        else:
            # --- è§’è‰²äºŒï¼šé›¶æ´»å¤„ç† (è¿½æ›´) ---
            if os.path.exists(main_json_path):
                 with open(main_json_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    perfect_cast_for_injection = (data.get('casts', {}) or data.get('credits', {})).get('cast', [])

        # æ­¥éª¤ 3: å…¬å…±æ–½å·¥ - æ³¨å…¥åˆ†é›†æ–‡ä»¶
        if item_type == "Series" and perfect_cast_for_injection:
            self._inject_cast_to_series_files(
                target_dir=target_override_dir, 
                cast_list=perfect_cast_for_injection, 
                series_details=item_details, 
                episode_ids_to_sync=episode_ids_to_sync,
                tmdb_episodes_data=tmdb_episodes_data,
                tmdb_seasons_data=tmdb_seasons_data 
            )

    # --- è¾…åŠ©å‡½æ•°ï¼šä»ä¸åŒæ•°æ®æºæ„å»ºæ¼”å‘˜åˆ—è¡¨ ---
    def _build_cast_from_final_data(self, final_cast_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """è¾…åŠ©å‡½æ•°ï¼šä»ä¸»æµç¨‹çš„æœ€ç»ˆç»“æœæ„å»ºæ¼”å‘˜åˆ—è¡¨"""
        cast_list = []
        for i, actor_info in enumerate(final_cast_data):
            if not actor_info.get("id"): continue
            cast_list.append({
                "id": actor_info.get("id"), "name": actor_info.get("name"), "character": actor_info.get("character"),
                "original_name": actor_info.get("original_name"), "profile_path": actor_info.get("profile_path"),
                "adult": actor_info.get("adult", False), "gender": actor_info.get("gender", 0),
                "known_for_department": actor_info.get("known_for_department", "Acting"),
                "popularity": actor_info.get("popularity", 0.0), "cast_id": actor_info.get("cast_id"),
                "credit_id": actor_info.get("credit_id"), "order": actor_info.get("order", i)
            })
        return cast_list

    # --- è¾…åŠ©å‡½æ•°ï¼šå°†æ¼”å‘˜è¡¨æ³¨å…¥å‰§é›†çš„å­£/é›†JSONæ–‡ä»¶ ---
    def _inject_cast_to_series_files(self, target_dir: str, cast_list: List[Dict[str, Any]], series_details: Dict[str, Any], episode_ids_to_sync: Optional[List[str]] = None, tmdb_episodes_data: Optional[Dict[str, Any]] = None, tmdb_seasons_data: Optional[List[Dict[str, Any]]] = None):
        """
        è¾…åŠ©å‡½æ•°ï¼šå°†æ¼”å‘˜è¡¨æ³¨å…¥å‰§é›†çš„å­£/é›†JSONæ–‡ä»¶ã€‚
        ã€ä¿®å¤ç‰ˆã€‘æ”¯æŒä¸»åŠ¨ç›‘æ§æ¨¡å¼ (ID='pending')ï¼Œæ­¤æ—¶ä»…åŸºäº TMDb æ•°æ®ç”Ÿæˆæ–‡ä»¶ï¼Œä¸è¯·æ±‚ Embyã€‚
        ã€æ–°å¢ã€‘ä¸¥æ ¼æ ¡éªŒ tmdb_id å’Œ season_numberï¼Œé˜²æ­¢ç”Ÿæˆæ— æ•ˆæ–‡ä»¶ã€‚
        """
        log_prefix = "[è¦†ç›–ç¼“å­˜-å…ƒæ•°æ®å†™å…¥]"
        if cast_list is not None:
            logger.info(f"  âœ {log_prefix} å¼€å§‹å°†æ¼”å‘˜è¡¨æ™ºèƒ½åŒæ­¥åˆ°æ‰€æœ‰å­£/é›†å¤‡ä»½æ–‡ä»¶...")
        else:
            logger.info(f"  âœ {log_prefix} å¼€å§‹å°†å®æ—¶å…ƒæ•°æ®ï¼ˆæ ‡é¢˜/ç®€ä»‹ï¼‰åŒæ­¥åˆ°æ‰€æœ‰å­£/é›†å¤‡ä»½æ–‡ä»¶...")
        
        series_id = series_details.get("Id")
        is_pending = (series_id == 'pending') # â˜… æ ‡è®°æ˜¯å¦ä¸ºé¢„å¤„ç†

        # 1. æ„å»ºâ€œå…¨å‰§æ¼”å‘˜ä¿¡æ¯å­—å…¸â€
        master_actor_map = {}
        if cast_list:
            for actor in cast_list:
                aid = actor.get('id')
                if aid:
                    try: master_actor_map[int(aid)] = actor
                    except ValueError: continue

        def patch_actor_list(target_list):
            if not target_list: return
            for person in target_list:
                pid = person.get('id')
                if not pid: continue
                try:
                    pid_int = int(pid)
                    if pid_int in master_actor_map:
                        master_info = master_actor_map[pid_int]
                        if master_info.get('name'): person['name'] = master_info.get('name')
                        if master_info.get('original_name'): person['original_name'] = master_info.get('original_name')
                        if master_info.get('profile_path'): person['profile_path'] = master_info.get('profile_path')
                        if master_info.get('character'): person['character'] = master_info.get('character')
                except ValueError: continue

        # â˜…â˜…â˜… 2. è·å–å­é¡¹ç›®åˆ—è¡¨ (æ ¸å¿ƒä¿®æ”¹) â˜…â˜…â˜…
        children_from_emby = []
        
        if not is_pending:
            # æ­£å¸¸æ¨¡å¼ï¼šä» Emby è·å–
            children_from_emby = emby.get_series_children(
                series_id=series_id, base_url=self.emby_url,
                api_key=self.emby_api_key, user_id=self.emby_user_id,
                series_name_for_log=series_details.get("Name")
            ) or []
        else:
            # ä¸»åŠ¨ç›‘æ§æ¨¡å¼ï¼šä» TMDb æ•°æ®æ„é€ è™šæ‹Ÿå­é¡¹ç›®
            logger.info(f"  âœ {log_prefix} å¤„äºé¢„å¤„ç†æ¨¡å¼ï¼Œå°†åŸºäº TMDb æ•°æ®ç”Ÿæˆåˆ†é›†æ–‡ä»¶åˆ—è¡¨...")
            
            seen_seasons = set() # ç”¨äºå»é‡

            # A. å¤„ç†åˆ†é›†æ•°æ® (Episode)
            if tmdb_episodes_data:
                import re
                # tmdb_episodes_data çš„ key æ˜¯ "S1E1" æ ¼å¼
                for key, ep_data in tmdb_episodes_data.items():
                    match = re.match(r'S(\d+)E(\d+)', key)
                    if match:
                        s_num = int(match.group(1))
                        e_num = int(match.group(2))
                        
                        if s_num == 0 or e_num == 0: continue

                        children_from_emby.append({
                            "Type": "Episode",
                            "ParentIndexNumber": s_num,
                            "IndexNumber": e_num,
                            "Name": ep_data.get('name'),
                            "Overview": ep_data.get('overview')
                        })
                        
                        # â˜…â˜…â˜… æ¢å¤ï¼šé¡ºä¾¿æ„é€ è™šæ‹Ÿ Season å¯¹è±¡ (å»é‡) â˜…â˜…â˜…
                        # è¿™ä¸€æ­¥éå¸¸å…³é”®ï¼å®ƒä¿è¯äº†åªè¦æœ‰åˆ†é›†ï¼Œå¯¹åº”çš„å­£æ–‡ä»¶ season-X.json å°±ä¸€å®šä¼šè¢«åˆ›å»ºã€‚
                        # å³ä½¿åé¢çš„ B æ­¥éª¤å¤±è´¥äº†ï¼Œæˆ‘ä»¬è‡³å°‘è¿˜æœ‰ä¸€ä¸ª ID=0 çš„æ–‡ä»¶ï¼Œè€Œä¸æ˜¯æ–‡ä»¶ä¸¢å¤±ã€‚
                        if s_num not in seen_seasons:
                            children_from_emby.append({
                                "Type": "Season",
                                "IndexNumber": s_num,
                                "Name": f"Season {s_num}"
                            })
                            seen_seasons.add(s_num)

            # B. â˜…â˜…â˜… æ–°å¢ï¼šæ˜¾å¼å¤„ç†åˆ†å­£æ•°æ® (Season) â˜…â˜…â˜…
            # å³ä½¿æ²¡æœ‰åˆ†é›†æ•°æ®ï¼Œæˆ–è€…åˆ†é›†å¾ªç¯æ¼æ‰äº†æŸäº›å­£ï¼Œè¿™é‡Œä¹Ÿèƒ½è¡¥å…¨
            if tmdb_seasons_data:
                for season in tmdb_seasons_data:
                    if not isinstance(season, dict): continue
                    
                    s_num = season.get('season_number')
                    if s_num is not None:
                        try:
                            s_num_int = int(s_num)
                            # åªæœ‰å½“è¿™ä¸ªå­£è¿˜æ²¡è¢«ä¸Šé¢çš„åˆ†é›†å¾ªç¯æ·»åŠ è¿‡æ—¶ï¼Œæ‰æ·»åŠ 
                            # æˆ–è€…ï¼Œæˆ‘ä»¬å¯ä»¥æ— è§† seen_seasonsï¼Œå› ä¸ºåé¢çš„ child_data_map ä¼šè‡ªåŠ¨å»é‡
                            # ä½†ä¸ºäº†é€»è¾‘æ¸…æ™°ï¼Œæˆ‘ä»¬è¿˜æ˜¯æ„é€ ä¸€ä¸ª Season å¯¹è±¡
                            children_from_emby.append({
                                "Type": "Season",
                                "IndexNumber": s_num_int,
                                "Name": season.get('name', f"Season {s_num_int}")
                            })
                        except ValueError:
                            pass

        child_data_map = {}
        for child in children_from_emby:
            key = None
            
            if child.get("Type") == "Season": 
                idx = child.get('IndexNumber')
                if idx is not None:
                    try:
                        # â˜…â˜…â˜… æ ¸å¿ƒæ ¡éªŒï¼šç¡®ä¿å­£å·æœ‰æ•ˆ â˜…â˜…â˜…
                        s_num_int = int(idx)
                        if s_num_int > 0:
                            key = f"season-{s_num_int}"
                    except (ValueError, TypeError):
                        logger.warning(f"  âœ {log_prefix} è·³è¿‡æ— æ•ˆçš„å­£å· '{idx}'ã€‚")
            
            elif child.get("Type") == "Episode": 
                s_num = child.get('ParentIndexNumber')
                e_num = child.get('IndexNumber')
                
                if s_num is not None and e_num is not None:
                    try:
                        # â˜…â˜…â˜… æ ¸å¿ƒæ ¡éªŒï¼šç¡®ä¿å­£å·å’Œé›†å·éƒ½å¤§äº0 â˜…â˜…â˜…
                        s_num_int = int(s_num)
                        e_num_int = int(e_num)
                        if s_num_int > 0 and e_num_int > 0:
                            key = f"season-{s_num_int}-episode-{e_num_int}"
                        else:
                            logger.warning(f"  âœ {log_prefix} è·³è¿‡æ— æ•ˆçš„å­£/é›†å· (S{s_num}E{e_num})ã€‚")
                    except (ValueError, TypeError):
                        logger.warning(f"  âœ {log_prefix} è·³è¿‡æ— æ•ˆçš„å­£/é›†å·æ ¼å¼ (S:{s_num}, E:{e_num})ã€‚")
            
            if key: 
                child_data_map[key] = child

        updated_children_count = 0
        try:
            files_to_process = set() 
            if episode_ids_to_sync and not is_pending: # åªæœ‰é pending çŠ¶æ€æ‰æ”¯æŒæŒ‰ ID è¿‡æ»¤
                id_set = set(episode_ids_to_sync)
                for child in children_from_emby:
                    if child.get("Id") in id_set and child.get("Type") == "Episode":
                        s_num = child.get('ParentIndexNumber')
                        e_num = child.get('IndexNumber')
                        if s_num is not None and e_num is not None:
                            try:
                                s_num_int = int(s_num)
                                e_num_int = int(e_num)
                                if s_num_int > 0 and e_num_int > 0:
                                    files_to_process.add(f"season-{s_num_int}-episode-{e_num_int}.json")
                                    files_to_process.add(f"season-{s_num_int}.json") # å­£æ–‡ä»¶ä¹Ÿéœ€è¦æ›´æ–°
                            except (ValueError, TypeError):
                                pass
            else:
                # å¦‚æœæ²¡æœ‰æŒ‡å®šåˆ†é›†ï¼Œæˆ–è€…å¤„äº Pending æ¨¡å¼ï¼Œåˆ™å¤„ç†æ‰€æœ‰å­£/é›†æ–‡ä»¶
                for key in child_data_map.keys():
                    files_to_process.add(f"{key}.json")

            sorted_files_to_process = sorted(list(files_to_process))

            # ç¡®ä¿ç›®æ ‡ç›®å½•å­˜åœ¨
            os.makedirs(target_dir, exist_ok=True)

            for filename in sorted_files_to_process:
                child_json_path = os.path.join(target_dir, filename)
                
                is_season_file = filename.startswith("season-") and "-episode-" not in filename
                is_episode_file = "-episode-" in filename
                
                # â˜…â˜…â˜… æ­¥éª¤ A: åˆå§‹åŒ–å®Œç¾éª¨æ¶ â˜…â˜…â˜…
                if is_season_file:
                    child_data = json.loads(json.dumps(utils.SEASON_SKELETON_TEMPLATE))
                elif is_episode_file:
                    child_data = json.loads(json.dumps(utils.EPISODE_SKELETON_TEMPLATE))
                else:
                    continue

                # â˜…â˜…â˜… æ­¥éª¤ B: åŠ è½½æ•°æ®æº (ä¼˜å…ˆ Overrideï¼Œå…¶æ¬¡ Source) â˜…â˜…â˜…
                data_source = None
                if os.path.exists(child_json_path):
                    data_source = _read_local_json(child_json_path)
                    if data_source:
                        for key in child_data.keys():
                            if key == 'credits' and 'casts' in data_source and 'credits' not in data_source:
                                 child_data['credits'] = data_source['casts']
                            elif key in data_source:
                                child_data[key] = data_source[key]
                
                # â˜…â˜…â˜… æ­¥éª¤ C: å¡«å……éª¨æ¶ â˜…â˜…â˜…
                if data_source:
                    for key in child_data.keys():
                        if key == 'credits' and 'casts' in data_source and 'credits' not in data_source:
                             child_data['credits'] = data_source['casts']
                        elif key in data_source:
                            child_data[key] = data_source[key]

                # --- è§£ææ–‡ä»¶åä¸­çš„å­£/é›†å· ---
                current_s_num = None
                current_e_num = None
                try:
                    parts = filename.replace(".json", "").split("-")
                    if is_season_file and len(parts) >= 2:
                        current_s_num = int(parts[1])
                    elif is_episode_file and len(parts) >= 4:
                        current_s_num = int(parts[1])
                        current_e_num = int(parts[3])
                except:
                    pass

                # â˜…â˜…â˜… æ­¥éª¤ D: æ™ºèƒ½ä¿®è¡¥ (æ•°æ®æ³¨å…¥) â˜…â˜…â˜…
                specific_tmdb_data = None
                
                # 1. å¤„ç†åˆ†é›† (Episode)
                if is_episode_file and tmdb_episodes_data and current_s_num is not None and current_e_num is not None:
                    key_s1e1 = f"S{current_s_num}E{current_e_num}"
                    if isinstance(tmdb_episodes_data, dict):
                        specific_tmdb_data = tmdb_episodes_data.get(key_s1e1)
                    elif isinstance(tmdb_episodes_data, list):
                        for ep in tmdb_episodes_data:
                            if ep.get('season_number') == current_s_num and ep.get('episode_number') == current_e_num:
                                specific_tmdb_data = ep
                                break
                    
                    if specific_tmdb_data:
                        child_data['id'] = specific_tmdb_data.get('id')
                        child_data['name'] = specific_tmdb_data.get('name')
                        child_data['overview'] = specific_tmdb_data.get('overview')
                        child_data['season_number'] = current_s_num
                        child_data['episode_number'] = current_e_num
                        if specific_tmdb_data.get('air_date'):
                            child_data['air_date'] = specific_tmdb_data.get('air_date')
                        if specific_tmdb_data.get('vote_average'):
                            child_data['vote_average'] = specific_tmdb_data.get('vote_average')

                # 2. å¤„ç†åˆ†å­£ (Season)
                elif is_season_file and tmdb_seasons_data and current_s_num is not None:
                    for season in tmdb_seasons_data:
                        if not isinstance(season, dict): continue
                        
                        # â˜…â˜…â˜… å¼ºåˆ¶ç±»å‹è½¬æ¢æ¯”è¾ƒï¼Œç¡®ä¿åŒ¹é…æˆåŠŸ â˜…â˜…â˜…
                        s_num_tmdb = season.get('season_number')
                        if s_num_tmdb is not None and int(s_num_tmdb) == current_s_num:
                            specific_tmdb_data = season
                            break
                    
                    if specific_tmdb_data:
                        # â˜…â˜…â˜… ä¿®å¤ ID=0 çš„å…³é”®ï¼šæ³¨å…¥çœŸå® ID â˜…â˜…â˜…
                        child_data['id'] = specific_tmdb_data.get('id')
                        child_data['name'] = specific_tmdb_data.get('name')
                        child_data['overview'] = specific_tmdb_data.get('overview')
                        child_data['season_number'] = current_s_num
                        if specific_tmdb_data.get('air_date'):
                            child_data['air_date'] = specific_tmdb_data.get('air_date')
                        if specific_tmdb_data.get('poster_path'):
                            child_data['poster_path'] = specific_tmdb_data.get('poster_path')
                
                # â˜…â˜…â˜… æ­¥éª¤ D: æ™ºèƒ½ä¿®è¡¥æ¼”å‘˜è¡¨ (é’ˆå¯¹ credits èŠ‚ç‚¹) â˜…â˜…â˜…
                specific_tmdb_data = None
                if is_episode_file and tmdb_episodes_data:
                    try:
                        parts = filename.replace(".json", "").split("-")
                        if len(parts) >= 4:
                            s_num = int(parts[1])
                            e_num = int(parts[3])
                            key = f"S{s_num}E{e_num}" 
                            specific_tmdb_data = tmdb_episodes_data.get(key)
                    except:
                        pass

                credits_node = child_data.get('credits')
                if not isinstance(credits_node, dict):
                    credits_node = {}
                    child_data['credits'] = credits_node

                if specific_tmdb_data:
                    should_remove_no_avatar = self.config.get(constants.CONFIG_OPTION_REMOVE_ACTORS_WITHOUT_AVATARS, True)

                    def process_actor_list(actors):
                        if not actors: return []
                        if should_remove_no_avatar:
                            return [a for a in actors if a.get('profile_path')]
                        return actors

                    # 1. å°è¯•ä» TMDb æ•°æ®ä¸­æå– (å› ä¸ºæˆ‘ä»¬deläº†ï¼Œè¿™é‡Œä¼šæ˜¯ç©ºçš„)
                    raw_cast = specific_tmdb_data.get('credits', {}).get('cast', [])
                    filtered_cast = process_actor_list(raw_cast)
                    if filtered_cast:
                        credits_node['cast'] = filtered_cast
                    
                    # â˜…â˜…â˜… æ ¸å¿ƒä¿®å¤ï¼šå¦‚æœ TMDb æ²¡æä¾›æ¼”å‘˜ (æˆ–è¢«æˆ‘ä»¬åˆ äº†)ï¼Œå°±ç”¨å…¨å‰§é€šç”¨çš„ â˜…â˜…â˜…
                    if not credits_node.get('cast') and cast_list:
                        credits_node['cast'] = cast_list

                    # 2. å¤„ç†å®¢ä¸²æ¼”å‘˜ (Guest Stars) - è¿™ä¸ªä¿ç•™ TMDb çš„
                    raw_guests = specific_tmdb_data.get('credits', {}).get('guest_stars', [])
                    filtered_guests = process_actor_list(raw_guests)
                    if filtered_guests:
                        credits_node['guest_stars'] = filtered_guests
                    
                    # 3. å¤„ç†å¹•å (Crew)
                    if specific_tmdb_data.get('credits', {}).get('crew'):
                        credits_node['crew'] = specific_tmdb_data['credits']['crew']
                
                elif is_episode_file:
                    if not credits_node.get('cast'):
                        credits_node['cast'] = cast_list

                elif is_season_file:
                    if not credits_node.get('cast'):
                        credits_node['cast'] = cast_list

                # åº”ç”¨ä¸»æ¼”å‘˜è¡¨çš„ç¿»è¯‘æˆæœ 
                if credits_node.get('cast'):
                    patch_actor_list(credits_node['cast'])
                
                if credits_node.get('guest_stars'):
                    patch_actor_list(credits_node['guest_stars'])

                # æ­¥éª¤ E: æ›´æ–° Emby å®æ—¶å…ƒæ•°æ® 
                file_key = os.path.splitext(filename)[0]
                fresh_emby_data = child_data_map.get(file_key)
                if fresh_emby_data:
                    if not specific_tmdb_data:
                        child_data['name'] = fresh_emby_data.get('Name', child_data.get('name'))
                        child_data['overview'] = fresh_emby_data.get('Overview', child_data.get('overview'))
                    if fresh_emby_data.get('CommunityRating'):
                        child_data['vote_average'] = fresh_emby_data.get('CommunityRating')

                # æ­¥éª¤ F: å†™å…¥æ–‡ä»¶ 
                try:
                    with open(child_json_path, 'w', encoding='utf-8') as f_child:
                        json.dump(child_data, f_child, ensure_ascii=False, indent=2)
                        updated_children_count += 1
                except Exception as e_child:
                    logger.warning(f"  âœ å†™å…¥å­æ–‡ä»¶ '{filename}' æ—¶å¤±è´¥: {e_child}")
            
            logger.info(f"  âœ {log_prefix} æˆåŠŸæ™ºèƒ½åŒæ­¥äº† {updated_children_count} ä¸ªå­£/é›†æ–‡ä»¶ã€‚")
        except Exception as e_list:
            logger.error(f"  âœ {log_prefix} éå†å¹¶æ›´æ–°å­£/é›†æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e_list}", exc_info=True)

    # æå–æ ‡ç­¾
    def extract_tag_names(item_data):
        """
        å…¼å®¹æ–°æ—§ç‰ˆ Emby API æå–æ ‡ç­¾åã€‚
        """
        tags_set = set()

        # 1. å°è¯•æå– TagItems (æ–°ç‰ˆ/è¯¦ç»†ç‰ˆ)
        tag_items = item_data.get('TagItems')
        if isinstance(tag_items, list):
            for t in tag_items:
                if isinstance(t, dict):
                    name = t.get('Name')
                    if name:
                        tags_set.add(name)
                elif isinstance(t, str) and t:
                    tags_set.add(t)
        
        # 2. å°è¯•æå– Tags (æ—§ç‰ˆ/ç®€ç•¥ç‰ˆ)
        tags = item_data.get('Tags')
        if isinstance(tags, list):
            for t in tags:
                if t:
                    tags_set.add(str(t))
        
        return list(tags_set)

    # --- ä¸ºä¸€ä¸ªåª’ä½“é¡¹åŒæ­¥å…ƒæ•°æ®ç¼“å­˜ ---
    def sync_single_item_to_metadata_cache(self, item_id: str, item_name: Optional[str] = None):
        """
        ã€V12 - æç®€ç‰ˆã€‘
        ä»…ç”¨äºå“åº” 'metadata.update' äº‹ä»¶ã€‚
        å°† Emby ä¸­çš„æœ€æ–°å…ƒæ•°æ®ï¼ˆæ ‡é¢˜ã€ç®€ä»‹ã€æ ‡ç­¾ç­‰ï¼‰å¿«é€Ÿé•œåƒåˆ°æœ¬åœ°æ•°æ®åº“ã€‚
        
        æ³¨æ„ï¼š'è¿½æ›´/æ–°åˆ†é›†å…¥åº“' ä¸å†ä½¿ç”¨æ­¤å‡½æ•°ï¼Œè€Œæ˜¯èµ° process_single_item -> _upsert_media_metadata æµç¨‹ã€‚
        """
        log_prefix = f"å®æ—¶åŒæ­¥åª’ä½“å…ƒæ•°æ® '{item_name}'"
        # logger.trace(f"  âœ {log_prefix} å¼€å§‹æ‰§è¡Œ...")
        
        try:
            # 1. è·å– Emby æœ€æ–°è¯¦æƒ…
            # ä¸éœ€è¦è¯·æ±‚ MediaSources ç­‰é‡å‹å­—æ®µï¼Œåªéœ€è¦å…ƒæ•°æ®
            fields_to_get = "ProviderIds,Type,Name,OriginalTitle,Overview,Tags,TagItems,OfficialRating,CustomRating,Path,_SourceLibraryId,PremiereDate,ProductionYear"
            item_details = emby.get_emby_item_details(item_id, self.emby_url, self.emby_api_key, self.emby_user_id, fields=fields_to_get)
            
            if not item_details:
                logger.warning(f"  âœ {log_prefix} æ— æ³•è·å–è¯¦æƒ…ï¼Œè·³è¿‡ã€‚")
                return
            
            tmdb_id = item_details.get("ProviderIds", {}).get("Tmdb")
            item_type = item_details.get("Type")
            
            if not tmdb_id or item_type not in ['Movie', 'Series', 'Season', 'Episode']:
                # ä»…åŒæ­¥æˆ‘ä»¬å…³å¿ƒçš„ç±»å‹
                return
            
            # è¡¥å…¨ Library ID
            if not item_details.get('_SourceLibraryId'):
                lib_info = emby.get_library_root_for_item(item_id, self.emby_url, self.emby_api_key, self.emby_user_id)
                if lib_info: item_details['_SourceLibraryId'] = lib_info.get('Id')

            # 2. ç›´æ¥æ›´æ–°æ•°æ®åº“
            with get_central_db_connection() as conn:
                with conn.cursor() as cursor:
                    final_tags = extract_tag_names(item_details)
                    
                    # åŸºç¡€å­—æ®µæ›´æ–°
                    updates = {
                        "title": item_details.get('Name'),
                        "original_title": item_details.get('OriginalTitle'),
                        "overview": item_details.get('Overview'),
                        "tags_json": json.dumps(final_tags, ensure_ascii=False),
                        "last_synced_at": datetime.now(timezone.utc)
                    }
                    
                    # æ—¥æœŸå­—æ®µå¤„ç†
                    if item_details.get('PremiereDate'):
                        updates["release_date"] = item_details['PremiereDate']
                    if item_details.get('ProductionYear'):
                        updates["release_year"] = item_details['ProductionYear']

                    # åˆ†çº§åŒæ­¥é€»è¾‘ 
                    new_official_rating = item_details.get('OfficialRating')
                    if new_official_rating is not None: # å…è®¸ç©ºå­—ç¬¦ä¸²æ›´æ–°ï¼Œä»£è¡¨æ¸…ç©º
                        # å…ˆæŸ¥æ—§æ•°æ®
                        cursor.execute("SELECT official_rating_json FROM media_metadata WHERE tmdb_id = %s AND item_type = %s", (tmdb_id, item_type))
                        row = cursor.fetchone()
                        current_rating_json = row['official_rating_json'] if row and row['official_rating_json'] else {}
                        
                        # æ›´æ–° US å­—æ®µ
                        current_rating_json['US'] = new_official_rating
                        updates["official_rating_json"] = json.dumps(current_rating_json, ensure_ascii=False)
                    
                    # B. åŒæ­¥è‡ªå®šä¹‰åˆ†çº§ (Emby çš„ CustomRating -> æ•°æ®åº“ custom_rating)
                    # ç›´æ¥èµ‹å€¼ï¼ŒEmby ä¼ ä»€ä¹ˆå°±æ˜¯ä»€ä¹ˆ
                    updates["custom_rating"] = item_details.get('CustomRating')
                    
                    # æ„å»º SQL
                    set_clauses = [f"{key} = %s" for key in updates.keys()]
                    sql = f"UPDATE media_metadata SET {', '.join(set_clauses)} WHERE tmdb_id = %s AND item_type = %s"
                    
                    cursor.execute(sql, tuple(updates.values()) + (tmdb_id, item_type))
                    
                    # å¦‚æœæ˜¯å‰§é›†ï¼Œä¸” Emby æ”¹äº†åå­—ï¼Œå¯èƒ½éœ€è¦çº§è”æ›´æ–°åˆ†é›†å—ï¼Ÿ
                    # é€šå¸¸ä¸éœ€è¦ï¼Œåˆ†é›†æœ‰è‡ªå·±çš„è®°å½•ã€‚å¦‚æœéœ€è¦ï¼Œé‚£æ˜¯å…¨é‡åˆ·æ–°çš„äº‹äº†ã€‚
                    
                    conn.commit()
            
            logger.info(f"  âœ {log_prefix} æ•°æ®åº“åŒæ­¥å®Œæˆã€‚")

        except Exception as e:
            logger.error(f"{log_prefix} æ‰§è¡Œæ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)

    # --- å°†æ¥è‡ª Emby çš„å®æ—¶å…ƒæ•°æ®æ›´æ–°åŒæ­¥åˆ° override ç¼“å­˜æ–‡ä»¶ ---
    def sync_emby_updates_to_override_files(self, item_details: Dict[str, Any]):
        """
        å°†æ¥è‡ª Emby çš„å®æ—¶å…ƒæ•°æ®æ›´æ–°åŒæ­¥åˆ° override ç¼“å­˜æ–‡ä»¶ã€‚
        è¿™æ˜¯ä¸€ä¸ª "è¯»-æ”¹-å†™" æ“ä½œï¼Œç”¨äºæŒä¹…åŒ–ç”¨æˆ·åœ¨ Emby UI ä¸Šçš„ä¿®æ”¹ã€‚
        """
        item_id = item_details.get("Id")
        item_name_for_log = item_details.get("Name", f"æœªçŸ¥é¡¹ç›®(ID:{item_id})")
        item_type = item_details.get("Type")
        tmdb_id = item_details.get("ProviderIds", {}).get("Tmdb")
        log_prefix = "[è¦†ç›–ç¼“å­˜-å…ƒæ•°æ®æŒä¹…åŒ–]"

        if not all([item_id, item_type, tmdb_id, self.local_data_path]):
            logger.warning(f"  âœ {log_prefix} è·³è¿‡ '{item_name_for_log}'ï¼Œç¼ºå°‘å…³é”®IDæˆ–è·¯å¾„é…ç½®ã€‚")
            return

        logger.info(f"  âœ {log_prefix} å¼€å§‹ä¸º '{item_name_for_log}' æ›´æ–°è¦†ç›–ç¼“å­˜æ–‡ä»¶...")

        # --- å®šä½ä¸»æ–‡ä»¶ ---
        cache_folder_name = "tmdb-movies2" if item_type == "Movie" else "tmdb-tv"
        target_override_dir = os.path.join(self.local_data_path, "override", cache_folder_name, tmdb_id)
        main_json_filename = "all.json" if item_type == "Movie" else "series.json"
        main_json_path = os.path.join(target_override_dir, main_json_filename)

        # --- å®‰å…¨æ£€æŸ¥ ---
        if not os.path.exists(main_json_path):
            logger.warning(f"  âœ {log_prefix} æ— æ³•æŒä¹…åŒ–ä¿®æ”¹ï¼šä¸»è¦†ç›–æ–‡ä»¶ '{main_json_path}' ä¸å­˜åœ¨ã€‚è¯·å…ˆå¯¹è¯¥é¡¹ç›®è¿›è¡Œä¸€æ¬¡å®Œæ•´å¤„ç†ã€‚")
            return

        try:
            # --- æ ¸å¿ƒçš„ "è¯»-æ”¹-å†™" é€»è¾‘ ---
            with open(main_json_path, 'r+', encoding='utf-8') as f:
                data = json.load(f)
                updated_count = 0

                # 1. åŸºç¡€å­—æ®µæ˜ å°„æ›´æ–°
                fields_to_update = {
                    "Name": "title",
                    "OriginalTitle": "original_title",
                    "Overview": "overview",
                    "Tagline": "tagline",
                    "CommunityRating": "vote_average",
                    "Genres": "genres",
                    "Studios": "production_companies",
                    "Tags": "keywords"
                }
                
                for emby_key, json_key in fields_to_update.items():
                    if emby_key in item_details:
                        new_value = item_details[emby_key]
                        
                        # ç‰¹æ®Šå¤„ç† Studios å’Œ Genres (Embyè¿”å›çš„æ˜¯å¯¹è±¡åˆ—è¡¨æˆ–å­—ç¬¦ä¸²åˆ—è¡¨)
                        if emby_key in ["Studios", "Genres"]:
                            if isinstance(new_value, list):
                                if emby_key == "Studios":
                                     data[json_key] = [{"name": s.get("Name")} for s in new_value if s.get("Name")]
                                else: # Genres
                                     data[json_key] = [{"id": 0, "name": g} for g in new_value] # ä¿æŒ utils éª¨æ¶æ ¼å¼
                                updated_count += 1
                        else:
                            data[json_key] = new_value
                            updated_count += 1
                
                # 2. åˆ†çº§ (OfficialRating) æ·±åº¦æ³¨å…¥ 
                if 'OfficialRating' in item_details:
                    new_rating = item_details['OfficialRating']
                    
                    # A. æ›´æ–°é¡¶å±‚å…¼å®¹å­—æ®µ (Emby/Kodi å¸¸ç”¨)
                    data['mpaa'] = new_rating
                    data['certification'] = new_rating
                    
                    # B. æ›´æ–°åµŒå¥—ç»“æ„ (TMDb æ ‡å‡†ç»“æ„)
                    # é»˜è®¤æˆ‘ä»¬å°† Emby çš„åˆ†çº§è§†ä¸º 'US' åˆ†çº§
                    target_country = 'US'
                    
                    if item_type == 'Movie':
                        # ç»“æ„: releases -> countries -> list
                        releases = data.setdefault('releases', {})
                        countries = releases.setdefault('countries', [])
                        
                        # æŸ¥æ‰¾å¹¶æ›´æ–° US æ¡ç›®
                        found = False
                        for c in countries:
                            if c.get('iso_3166_1') == target_country:
                                c['certification'] = new_rating
                                found = True
                                break
                        # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œè¿½åŠ ä¸€ä¸ª
                        if not found:
                            countries.append({
                                "iso_3166_1": target_country,
                                "certification": new_rating,
                                "primary": False,
                                "release_date": ""
                            })
                            
                    elif item_type == 'Series':
                        # ç»“æ„: content_ratings -> results -> list
                        c_ratings = data.setdefault('content_ratings', {})
                        results = c_ratings.setdefault('results', [])
                        
                        # æŸ¥æ‰¾å¹¶æ›´æ–° US æ¡ç›®
                        found = False
                        for r in results:
                            if r.get('iso_3166_1') == target_country:
                                r['rating'] = new_rating
                                found = True
                                break
                        # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œè¿½åŠ ä¸€ä¸ª
                        if not found:
                            results.append({
                                "iso_3166_1": target_country,
                                "rating": new_rating
                            })
                    
                    updated_count += 1

                # 3. å¤„ç†æ—¥æœŸ
                if 'PremiereDate' in item_details:
                    # Emby: 2023-01-01T00:00:00.0000000Z -> JSON: 2023-01-01
                    date_val = item_details['PremiereDate']
                    if date_val and len(date_val) >= 10:
                        if item_type == 'Movie':
                            data['release_date'] = date_val[:10]
                        elif item_type == 'Series':
                            data['first_air_date'] = date_val[:10]
                        updated_count += 1

                logger.info(f"  âœ {log_prefix} å‡†å¤‡å°† {updated_count} é¡¹æ›´æ–°å†™å…¥ '{main_json_filename}'ã€‚")

                f.seek(0)
                json.dump(data, f, ensure_ascii=False, indent=2)
                f.truncate()

            # å¦‚æœæ˜¯å‰§é›†ï¼Œè¿˜éœ€è¦æ›´æ–°æ‰€æœ‰å­æ–‡ä»¶çš„ name å’Œ overview
            if item_type == "Series":
                logger.info(f"  âœ {log_prefix} æ£€æµ‹åˆ°ä¸ºå‰§é›†ï¼Œå¼€å§‹åŒæ­¥æ›´æ–°å­é¡¹ï¼ˆå­£/é›†ï¼‰çš„å…ƒæ•°æ®...")
                self._inject_cast_to_series_files(
                    target_dir=target_override_dir,
                    cast_list=None, 
                    series_details=item_details
                )

            logger.info(f"  âœ {log_prefix} æˆåŠŸä¸º '{item_name_for_log}' æŒä¹…åŒ–äº†å…ƒæ•°æ®ä¿®æ”¹ã€‚")

        except Exception as e:
            logger.error(f"  âœ {log_prefix} ä¸º '{item_name_for_log}' æ›´æ–°è¦†ç›–ç¼“å­˜æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)


    def close(self):
        if self.douban_api: self.douban_api.close()
