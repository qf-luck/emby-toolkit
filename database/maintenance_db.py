# database/maintenance_db.py
import psycopg2
import re
import json
from psycopg2 import sql
from psycopg2.extras import Json, execute_values
import logging
from typing import List, Dict, Any, Optional
from datetime import datetime, timezone

from .connection import get_db_connection
from .log_db import LogDBManager
from .media_db import get_tmdb_id_from_emby_id
import constants

logger = logging.getLogger(__name__)

# ======================================================================
# æ¨¡å—: ç»´æŠ¤æ•°æ®è®¿é—®
# ======================================================================
# --- é€šç”¨ç»´æŠ¤å‡½æ•° ---
def clear_table(table_name: str) -> int:
    """æ¸…ç©ºæŒ‡å®šçš„æ•°æ®åº“è¡¨ï¼Œè¿”å›åˆ é™¤çš„è¡Œæ•°ã€‚"""
    
    try:
        with get_db_connection() as conn:
            cursor = conn.cursor()
            query = sql.SQL("DELETE FROM {}").format(sql.Identifier(table_name))
            cursor.execute(query)
            deleted_count = cursor.rowcount
            conn.commit()
            logger.info(f"æ¸…ç©ºè¡¨ {table_name}ï¼Œåˆ é™¤äº† {deleted_count} è¡Œã€‚")
            return deleted_count
    except Exception as e:
        logger.error(f"æ¸…ç©ºè¡¨ {table_name} æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        raise

def correct_all_sequences() -> list:
    """ã€V2 - æœ€ç»ˆä¿®æ­£ç‰ˆã€‘è‡ªåŠ¨æŸ¥æ‰¾å¹¶æ ¡å‡†æ‰€æœ‰è¡¨çš„è‡ªå¢åºåˆ—ã€‚"""
    
    corrected_tables = []
    with get_db_connection() as conn:
        cursor = conn.cursor()
        try:
            cursor.execute("""
                SELECT c.table_name, c.column_name
                FROM information_schema.columns c
                WHERE c.table_schema = 'public' AND c.column_default LIKE 'nextval%';
            """)
            tables_with_sequences = cursor.fetchall()

            if not tables_with_sequences:
                logger.info("  âœ æœªæ‰¾åˆ°ä»»ä½•ä½¿ç”¨è‡ªå¢åºåˆ—çš„è¡¨ï¼Œæ— éœ€æ ¡å‡†ã€‚")
                return []

            logger.info(f"  âœ å¼€å§‹æ ¡å‡† {len(tables_with_sequences)} ä¸ªè¡¨çš„è‡ªå¢åºåˆ—...")

            for row in tables_with_sequences:
                table_name = row['table_name']
                column_name = row['column_name']
                
                query = sql.SQL("""
                    SELECT setval(
                        pg_get_serial_sequence({table}, {column}),
                        COALESCE((SELECT MAX({id_col}) FROM {table_ident}), 0) + 1,
                        false
                    )
                """).format(
                    table=sql.Literal(table_name),
                    column=sql.Literal(column_name),
                    id_col=sql.Identifier(column_name),
                    table_ident=sql.Identifier(table_name)
                )
                
                cursor.execute(query)
                logger.info(f"  âœ å·²æˆåŠŸæ ¡å‡†è¡¨ '{table_name}' çš„åºåˆ—ã€‚")
                corrected_tables.append(table_name)
            
            conn.commit()
            return corrected_tables

        except Exception as e:
            conn.rollback()
            logger.error(f"  âœ æ ¡å‡†è‡ªå¢åºåˆ—æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}", exc_info=True)
            raise

# ======================================================================
# æ¨¡å—: æ•°æ®çœ‹æ¿ç»Ÿè®¡ (æ‹†åˆ†ç‰ˆ)
# ======================================================================

def _execute_single_row_query(sql_query: str) -> dict:
    """è¾…åŠ©å‡½æ•°ï¼šæ‰§è¡Œè¿”å›å•è¡Œç»“æœçš„æŸ¥è¯¢"""
    try:
        with get_db_connection() as conn:
            with conn.cursor() as cursor:
                cursor.execute(sql_query)
                result = cursor.fetchone()
                return dict(result) if result else {}
    except Exception as e:
        logger.error(f"ç»Ÿè®¡æŸ¥è¯¢å‡ºé”™: {e}")
        return {}

def get_stats_core() -> dict:
    """1. æ ¸å¿ƒå¤´éƒ¨æ•°æ® (æå¿«)"""
    sql = """
    SELECT
        (SELECT COUNT(*) FROM media_metadata WHERE item_type IN ('Movie', 'Series')) AS media_cached_total,
        (SELECT COUNT(*) FROM person_identity_map) AS actor_mappings_total
    """
    return _execute_single_row_query(sql)

def get_stats_library() -> dict:
    """2. åª’ä½“åº“æ¦‚è§ˆ (è¾ƒå¿«)"""
    sql = """
    SELECT
        (SELECT COUNT(*) FROM media_metadata WHERE item_type = 'Movie' AND in_library = TRUE) AS media_movies_in_library,
        (SELECT COUNT(*) FROM media_metadata WHERE item_type = 'Series' AND in_library = TRUE) AS media_series_in_library,
        (SELECT COUNT(*) FROM media_metadata WHERE item_type = 'Episode' AND in_library = TRUE) AS media_episodes_in_library
    """
    data = _execute_single_row_query(sql)
    data['resolution_stats'] = get_resolution_distribution() # å¤ç”¨ç°æœ‰çš„åˆ†è¾¨ç‡å‡½æ•°
    return data

def get_stats_system() -> dict:
    """3. ç³»ç»Ÿæ—¥å¿—ä¸ç¼“å­˜ (å¿«)"""
    sql = """
    SELECT
        (SELECT COUNT(*) FROM person_identity_map WHERE emby_person_id IS NOT NULL) AS actor_mappings_linked,
        (SELECT COUNT(*) FROM person_identity_map WHERE emby_person_id IS NULL) AS actor_mappings_unlinked,
        (SELECT COUNT(*) FROM translation_cache) AS translation_cache_count,
        (SELECT COUNT(*) FROM processed_log) AS processed_log_count,
        (SELECT COUNT(*) FROM failed_log) AS failed_log_count
    """
    return _execute_single_row_query(sql)

def get_stats_subscription():
    """
    è·å–è®¢é˜…ç›¸å…³çš„ç»Ÿè®¡æ•°æ® (æœ€ç»ˆä¿®æ­£ï¼šé™åˆ¶ä¸º Series ç±»å‹ï¼Œé˜²æ­¢ç»Ÿè®¡å­£å±‚çº§)
    """
    try:
        with get_db_connection() as conn:
            with conn.cursor() as cursor:
                # 1. è¿½å‰§ç»Ÿè®¡
                # å¢åŠ  AND item_type = 'Series'ï¼Œåªç»Ÿè®¡å‰§é›†å±‚çº§ï¼Œæ’é™¤å­£å’Œé›†
                cursor.execute("""
                    SELECT 
                        COUNT(*) FILTER (WHERE TRIM(watching_status) ILIKE 'Watching' OR TRIM(watching_status) ILIKE 'Pending') as watching,
                        COUNT(*) FILTER (WHERE TRIM(watching_status) ILIKE 'Paused') as paused,
                        COUNT(*) FILTER (WHERE TRIM(watching_status) ILIKE 'Completed') as completed
                    FROM media_metadata
                    WHERE watching_status IS NOT NULL 
                      AND watching_status NOT ILIKE 'NONE'
                      AND item_type = 'Series'
                """)
                watchlist_row = cursor.fetchone()
                
                # 2. æ¼”å‘˜è®¢é˜…ç»Ÿè®¡
                cursor.execute("SELECT COUNT(*) FROM actor_subscriptions WHERE status = 'active'")
                actor_sub_count = cursor.fetchone()['count']

                # åªç»Ÿè®¡ in_library = TRUE çš„é¡¹ç›®ï¼Œä¸å†è®¡ç®—æ€»æ•°
                cursor.execute("""
                    SELECT 
                        COUNT(*) as in_lib
                    FROM media_metadata 
                    WHERE subscription_sources_json @> '[{"type": "actor_subscription"}]'::jsonb
                      AND in_library = TRUE
                """)
                actor_works_row = cursor.fetchone()

                # 3. æ´—ç‰ˆç»Ÿè®¡
                cursor.execute("SELECT COUNT(*) FROM resubscribe_index WHERE status IN ('needed', 'auto_subscribed')")
                resub_pending = cursor.fetchone()['count']

                # 4. åŸç”Ÿåˆé›†ç»Ÿè®¡ (å®æ—¶è®¡ç®—)
                # é€»è¾‘ï¼šå±•å¼€ collections_info ä¸­çš„ TMDB ID -> å…³è” media_metadata -> ç­›é€‰ä¸åœ¨åº“ä¸”æ— è®¢é˜…çŠ¶æ€çš„ç”µå½±
                cursor.execute("""
                    WITH expanded_ids AS (
                        -- 1. å±•å¼€æ‰€æœ‰åˆé›†çš„ TMDB IDï¼Œå¹¶ç¡®ä¿æ˜¯æ•°ç»„ç±»å‹
                        SELECT 
                            emby_collection_id,
                            jsonb_array_elements_text(all_tmdb_ids_json) AS tmdb_id
                        FROM collections_info
                        WHERE all_tmdb_ids_json IS NOT NULL AND jsonb_typeof(all_tmdb_ids_json) = 'array'
                    ),
                    missing_pairs AS (
                        -- 2. å…³è”åª’ä½“è¡¨ï¼Œæ‰¾å‡ºçœŸæ­£ç¼ºå¤±çš„é¡¹ç›® (Collection ID, TMDB ID) å¯¹
                        -- ä½¿ç”¨ LEFT JOIN åŒ…å«é‚£äº›åœ¨ media_metadata è¡¨ä¸­å®Œå…¨ä¸å­˜åœ¨çš„è®°å½•
                        SELECT 
                            e.emby_collection_id,
                            e.tmdb_id
                        FROM expanded_ids e
                        LEFT JOIN media_metadata m ON e.tmdb_id = m.tmdb_id AND m.item_type = 'Movie'
                        WHERE 
                            -- æ ¸å¿ƒä¿®æ”¹ï¼šåªè¦ä¸åœ¨åº“ï¼ˆè®°å½•ä¸ºNULL æˆ– in_library=FALSEï¼‰ï¼Œå°±ç®—ç¼ºå¤±
                            -- ä¸å†åˆ¤æ–­ subscription_statusï¼Œæ— è®ºæ˜¯å¦è®¢é˜…/å¿½ç•¥ï¼Œåªè¦æ²¡å…¥åº“éƒ½ç®—
                            (m.in_library IS NULL OR m.in_library = FALSE)
                    )
                    SELECT 
                        (SELECT COUNT(*) FROM collections_info) as total,
                        -- ç»Ÿè®¡æœ‰å¤šå°‘ä¸ªåˆé›†å­˜åœ¨ç¼ºå¤± (æŒ‰åˆé›†IDå»é‡)
                        (SELECT COUNT(DISTINCT emby_collection_id) FROM missing_pairs) as with_missing,
                        -- ç»Ÿè®¡æ€»å…±ç¼ºå¤±å¤šå°‘éƒ¨ç”µå½± (æŒ‰TMDB IDå»é‡ï¼Œé¿å…ä¸€éƒ¨ç”µå½±åœ¨å¤šä¸ªåˆé›†ä¸­è¢«é‡å¤è®¡ç®—)
                        (SELECT COUNT(DISTINCT tmdb_id) FROM missing_pairs) as missing_items;
                """)
                native_col_row = cursor.fetchone()

                # 5. è‡ªå»ºåˆé›†ç»Ÿè®¡
                cursor.execute("""
                    SELECT id, type, generated_media_info_json 
                    FROM custom_collections 
                    WHERE status = 'active'
                """)
                active_collections = cursor.fetchall()
                
                custom_total = len(active_collections)
                custom_with_missing = 0
                custom_missing_items_set = set() # å­˜å‚¨ "{id}_{type}" å­—ç¬¦ä¸²å»é‡

                # 5.2 æ”¶é›†æ‰€æœ‰éœ€è¦æ£€æŸ¥çš„ ID (SQLæŸ¥è¯¢åªéœ€è¦ID)
                all_tmdb_ids_to_check = set()
                for col in active_collections:
                    if col['type'] not in ['list', 'ai_recommendation_global']:
                        continue
                        
                    media_list = col['generated_media_info_json']
                    if not media_list: continue
                    
                    if isinstance(media_list, str):
                        try: media_list = json.loads(media_list)
                        except: continue
                    
                    if isinstance(media_list, list):
                        for item in media_list:
                            tid = None
                            if isinstance(item, dict): tid = item.get('tmdb_id')
                            elif isinstance(item, str): tid = item
                            
                            if tid: all_tmdb_ids_to_check.add(str(tid))

                # 5.3 æ‰¹é‡æŸ¥è¯¢åœ¨åº“çŠ¶æ€ (â˜… å¿…é¡»æŸ¥ item_type â˜…)
                in_library_status_map = {}
                if all_tmdb_ids_to_check:
                    cursor.execute("""
                        SELECT tmdb_id, item_type, in_library 
                        FROM media_metadata 
                        WHERE tmdb_id = ANY(%s)
                    """, (list(all_tmdb_ids_to_check),))
                    
                    for row in cursor.fetchall():
                        # â˜… æ„é€ ç»„åˆé”®ï¼š12345_Movie
                        key = f"{row['tmdb_id']}_{row['item_type']}"
                        in_library_status_map[key] = row['in_library']

                # 5.4 è®¡ç®—ç¼ºå¤± (â˜… ç²¾ç¡®æ¯”å¯¹ â˜…)
                for col in active_collections:
                    if col['type'] not in ['list', 'ai_recommendation_global']:
                        continue
                        
                    media_list = col['generated_media_info_json']
                    if not media_list: continue
                    if isinstance(media_list, str):
                        try: media_list = json.loads(media_list)
                        except: continue
                    
                    has_missing_in_this_col = False
                    
                    for item in media_list:
                        tid = None
                        media_type = 'Movie' # é»˜è®¤ç±»å‹

                        if isinstance(item, dict): 
                            tid = item.get('tmdb_id')
                            media_type = item.get('media_type') or 'Movie'
                        elif isinstance(item, str): 
                            tid = item
                        
                        if not tid or str(tid).lower() == 'none': 
                            # æ²¡æœ‰IDç®—ç¼ºå¤±
                            has_missing_in_this_col = True
                            continue
                        
                        # â˜… æ„é€ ç›®æ ‡é”®ï¼š12345_Series
                        target_key = f"{tid}_{media_type}"
                        
                        # æŸ¥å­—å…¸ï¼šå¿…é¡» ID å’Œ ç±»å‹ éƒ½åŒ¹é…ï¼Œä¸” in_library ä¸º True æ‰ç®—åœ¨åº“
                        is_in_lib = in_library_status_map.get(target_key, False)
                        
                        if not is_in_lib:
                            has_missing_in_this_col = True
                            # åŠ å…¥ç¼ºå¤±é›†åˆå»é‡ (å¸¦ç±»å‹)
                            custom_missing_items_set.add(target_key)
                    
                    if has_missing_in_this_col:
                        custom_with_missing += 1

                return {
                    'watchlist_active': watchlist_row['watching'],
                    'watchlist_paused': watchlist_row['paused'],
                    'watchlist_completed': watchlist_row['completed'],
                    
                    'actor_subscriptions_active': actor_sub_count,
                    'actor_works_in_library': actor_works_row['in_lib'],
                    
                    'resubscribe_pending': resub_pending,
                    
                    'native_collections_total': native_col_row['total'],
                    'native_collections_with_missing': native_col_row['with_missing'],
                    'native_collections_missing_items': native_col_row['missing_items'],
                    
                    'custom_collections_total': custom_total,
                    'custom_collections_with_missing': custom_with_missing,
                    'custom_collections_missing_items': len(custom_missing_items_set)
                }
    except Exception as e:
        logger.error(f"è·å–è®¢é˜…ç»Ÿè®¡å¤±è´¥: {e}", exc_info=True)
        return {}
    
def get_resolution_distribution() -> List[Dict[str, Any]]:
    """è·å–åœ¨åº“åª’ä½“çš„åˆ†è¾¨ç‡åˆ†å¸ƒï¼Œç”¨äºç”Ÿæˆå›¾è¡¨ã€‚"""
    sql = """
        SELECT 
            -- æå– asset_details_json æ•°ç»„ä¸­ç¬¬ä¸€ä¸ªå…ƒç´ çš„ resolution_display å­—æ®µ
            (jsonb_array_elements(asset_details_json) ->> 'resolution_display') as resolution,
            COUNT(*) as count
        FROM 
            media_metadata
        WHERE 
            in_library = TRUE 
            AND item_type IN ('Movie', 'Episode')
            AND asset_details_json IS NOT NULL
            AND jsonb_array_length(asset_details_json) > 0
        GROUP BY 
            resolution
        ORDER BY 
            count DESC;
    """
    try:
        with get_db_connection() as conn:
            with conn.cursor() as cursor:
                cursor.execute(sql)
                return [dict(row) for row in cursor.fetchall()]
    except Exception as e:
        logger.error(f"DB: è·å–åˆ†è¾¨ç‡åˆ†å¸ƒæ•°æ®å¤±è´¥: {e}", exc_info=True)
        return []

def get_release_group_ranking(limit: int = 5) -> list:
    """
    ç»Ÿè®¡ã€å½“å¤©å…¥åº“ã€‘çš„å‘å¸ƒç»„ä½œå“ï¼ˆæ–‡ä»¶ï¼‰æ•°é‡ï¼Œå¹¶è¿”å›æ’åå‰Nçš„åˆ—è¡¨ã€‚
    """
    query = """
        SELECT
            release_group,
            COUNT(*) AS count
        FROM (
            SELECT
                jsonb_array_elements_text(asset -> 'release_group_raw') AS release_group,
                ((asset ->> 'date_added_to_library')::timestamp AT TIME ZONE 'UTC') AS asset_added_at_utc
            FROM (
                SELECT jsonb_array_elements(asset_details_json) AS asset
                FROM media_metadata
                WHERE
                    in_library = TRUE
                    AND asset_details_json IS NOT NULL
                    AND jsonb_array_length(asset_details_json) > 0
                    AND asset_details_json::text LIKE %s
            ) AS assets
        ) AS release_groups
        WHERE
            release_group IS NOT NULL AND release_group != ''
            AND (asset_added_at_utc AT TIME ZONE %s)::date = (NOW() AT TIME ZONE %s)::date
        GROUP BY release_group
        ORDER BY count DESC
        LIMIT %s;
    """
    try:
        with get_db_connection() as conn:
            with conn.cursor() as cursor:
                like_pattern = '%date_added_to_library%'
                params = (like_pattern, constants.TIMEZONE, constants.TIMEZONE, limit)
                cursor.execute(query, params)
                results = cursor.fetchall()
                return [dict(row) for row in results]
    except Exception as e:
        logger.error(f"è·å–ã€æ¯æ—¥ã€‘å‘å¸ƒç»„æ’è¡Œæ—¶å‘ç”Ÿæ•°æ®åº“é”™è¯¯: {e}", exc_info=True)
        return []
    
def get_historical_release_group_ranking(limit: int = 5) -> list:
    """
    ç»Ÿè®¡ã€å†å²å…¥åº“ã€‘çš„æ‰€æœ‰å‘å¸ƒç»„ä½œå“ï¼ˆæ–‡ä»¶ï¼‰æ•°é‡ï¼Œå¹¶è¿”å›æ€»æ’åå‰Nçš„åˆ—è¡¨ã€‚
    """
    # è¿™ä¸ªæŸ¥è¯¢ä¸ get_release_group_ranking å‡ ä¹ä¸€æ ·ï¼Œä½†æ²¡æœ‰æŒ‰â€œå½“å¤©â€è¿‡æ»¤
    query = """
        SELECT
            release_group,
            COUNT(*) AS count
        FROM (
            SELECT 
                jsonb_array_elements_text(asset -> 'release_group_raw') AS release_group
            FROM (
                SELECT jsonb_array_elements(asset_details_json) AS asset
                FROM media_metadata
                WHERE 
                    in_library = TRUE 
                    AND asset_details_json IS NOT NULL 
                    AND jsonb_array_length(asset_details_json) > 0
                    -- ä»ç„¶æ£€æŸ¥ date_added_to_library å­—æ®µæ˜¯å¦å­˜åœ¨ï¼Œä»¥ç¡®ä¿æ˜¯æœ‰æ•ˆå…¥åº“è®°å½•
                    AND asset_details_json::text LIKE %s
            ) AS assets
        ) AS release_groups
        WHERE 
            release_group IS NOT NULL AND release_group != ''
        GROUP BY release_group
        ORDER BY count DESC
        LIMIT %s;
    """
    try:
        with get_db_connection() as conn:
            with conn.cursor() as cursor:
                # å‚æ•°å‡å°‘äº†ï¼Œå› ä¸ºä¸å†éœ€è¦æ—¶åŒº
                like_pattern = '%date_added_to_library%'
                params = (like_pattern, limit)
                cursor.execute(query, params)
                results = cursor.fetchall()
                return [dict(row) for row in results]
    except Exception as e:
        logger.error(f"è·å–ã€å†å²ã€‘å‘å¸ƒç»„æ’è¡Œæ—¶å‘ç”Ÿæ•°æ®åº“é”™è¯¯: {e}", exc_info=True)
        return []

def get_all_table_names() -> List[str]:
    """
    ä½¿ç”¨ information_schema è·å–æ•°æ®åº“ä¸­æ‰€æœ‰è¡¨çš„åç§°ã€‚
    """
    try:
        with get_db_connection() as conn:
            with conn.cursor() as cursor:
                query = """
                    SELECT table_name FROM information_schema.tables 
                    WHERE table_schema = 'public' AND table_type = 'BASE TABLE'
                    ORDER BY table_name;
                """
                cursor.execute(query)
                return [row['table_name'] for row in cursor.fetchall()]
    except Exception as e:
        logger.error(f"è·å– PostgreSQL è¡¨åˆ—è¡¨æ—¶å‡ºé”™: {e}", exc_info=True)
        raise

def export_tables_data(tables_to_export: List[str]) -> Dict[str, List[Dict]]:
    """
    ä»æŒ‡å®šçš„å¤šä¸ªè¡¨ä¸­å¯¼å‡ºæ‰€æœ‰æ•°æ®ã€‚
    """
    exported_data = {}
    try:
        with get_db_connection() as conn:
            with conn.cursor() as cursor:
                for table_name in tables_to_export:
                    if not re.match(r'^[a-zA-Z0-9_]+$', table_name):
                        logger.warning(f"æ£€æµ‹åˆ°æ— æ•ˆçš„è¡¨å '{table_name}'ï¼Œå·²è·³è¿‡å¯¼å‡ºã€‚")
                        continue
                    
                    query = sql.SQL("SELECT * FROM {table}").format(table=sql.Identifier(table_name))
                    cursor.execute(query)
                    rows = cursor.fetchall()
                    exported_data[table_name] = [dict(row) for row in rows]
        return exported_data
    except Exception as e:
        logger.error(f"å¯¼å‡ºæ•°æ®åº“è¡¨æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        raise

def prepare_for_library_rebuild() -> Dict[str, Dict]:
    """
    ã€é«˜å± - ä¿®å¤ç‰ˆã€‘æ‰§è¡Œä¸º Emby åª’ä½“åº“é‡å»ºåšå‡†å¤‡çš„æ‰€æœ‰æ•°æ®åº“æ“ä½œã€‚
    1. æ¸…ç©º Emby ä¸“å±æ•°æ®è¡¨ (ç”¨æˆ·ã€æ’­æ”¾çŠ¶æ€ã€ç¼“å­˜)ã€‚
    2. é‡ç½®æ ¸å¿ƒå…ƒæ•°æ®è¡¨ä¸­çš„ Emby å…³è”å­—æ®µ (IDã€èµ„äº§è¯¦æƒ…ã€åœ¨åº“çŠ¶æ€)ã€‚
    3. é‡ç½®è¿½å‰§çŠ¶æ€ã€‚
    """
    # 1. éœ€è¦è¢« TRUNCATE (æ¸…ç©º) çš„è¡¨
    tables_to_truncate = [
        'emby_users', 
        'emby_users_extended', 
        'user_media_data', 
        'collections_info', 
        'resubscribe_index', 
        'cleanup_index' 
    ]

    results = {"truncated_tables": [], "updated_rows": {}}
    
    try:
        with get_db_connection() as conn:
            with conn.cursor() as cursor:
                logger.info("ç¬¬ä¸€æ­¥ï¼šå¼€å§‹æ¸…ç©º Emby ä¸“å±æ•°æ®è¡¨...")
                for table_name in tables_to_truncate:
                    # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨ï¼Œé˜²æ­¢æŠ¥é”™
                    cursor.execute("SELECT to_regclass(%s)", (table_name,))
                    result = cursor.fetchone()
                    if result and result.get('to_regclass'):
                        logger.warning(f"  âœ æ­£åœ¨æ¸…ç©ºè¡¨: {table_name}")
                        query = sql.SQL("TRUNCATE TABLE {table} RESTART IDENTITY CASCADE;").format(table=sql.Identifier(table_name))
                        cursor.execute(query)
                        results["truncated_tables"].append(table_name)
                    else:
                        logger.warning(f"  âœ è¡¨ {table_name} ä¸å­˜åœ¨ï¼Œè·³è¿‡æ¸…ç©ºã€‚")

                logger.info("ç¬¬äºŒæ­¥ï¼šé‡ç½® media_metadata è¡¨ä¸­çš„ Emby å…³è”å­—æ®µ...")
                # â˜…â˜…â˜… æ ¸å¿ƒä¿®å¤ï¼šé’ˆå¯¹ JSONB å­—æ®µè®¾ç½® '[]'ï¼Œé’ˆå¯¹çŠ¶æ€å­—æ®µé‡ç½® â˜…â˜…â˜…
                cursor.execute("""
                    UPDATE media_metadata
                    SET 
                        -- 1. æ ¸å¿ƒå…³è”å­—æ®µ
                        in_library = FALSE,
                        emby_item_ids_json = '[]'::jsonb,  -- è®¾ç½®ä¸ºç©ºæ•°ç»„ï¼Œè€Œä¸æ˜¯ NULL
                        asset_details_json = NULL,         -- èµ„äº§è¯¦æƒ…å¯ä»¥ä¸º NULL
                        date_added = NULL,
                        
                        -- 2. è¿½å‰§çŠ¶æ€é‡ç½® (åº“éƒ½æ²¡äº†ï¼Œè¿½å‰§çŠ¶æ€è‡ªç„¶è¦é‡ç½®)
                        watching_status = 'NONE',
                        paused_until = NULL,
                        force_ended = FALSE,
                        watchlist_is_airing = FALSE,
                        watchlist_next_episode_json = NULL,
                        watchlist_missing_info_json = NULL,
                        
                        -- 3. æ›´æ–°æ—¶é—´æˆ³
                        last_updated_at = NOW()
                    WHERE 
                        in_library = TRUE 
                        OR emby_item_ids_json::text != '[]'
                        OR watching_status != 'NONE';
                """)
                results["updated_rows"]["media_metadata"] = cursor.rowcount
                logger.info(f"  âœ media_metadata è¡¨é‡ç½®å®Œæˆï¼Œå½±å“äº† {cursor.rowcount} è¡Œã€‚")

                logger.info("ç¬¬ä¸‰æ­¥ï¼šé‡ç½® æ¼”å‘˜æ˜ å°„è¡¨ (person_identity_map)...")
                cursor.execute("""
                    UPDATE person_identity_map 
                    SET emby_person_id = NULL 
                    WHERE emby_person_id IS NOT NULL;
                """)
                results["updated_rows"]["person_identity_map"] = cursor.rowcount

                logger.info("ç¬¬å››æ­¥ï¼šé‡ç½® è‡ªå»ºåˆé›†è¡¨ (custom_collections)...")
                cursor.execute("""
                    UPDATE custom_collections 
                    SET 
                        emby_collection_id = NULL,
                        in_library_count = 0,
                        missing_count = 0
                    WHERE emby_collection_id IS NOT NULL;
                """)
                results["updated_rows"]["custom_collections"] = cursor.rowcount

            conn.commit()
            logger.info("  âœ æ•°æ®åº“é‡ç½®æ“ä½œå…¨éƒ¨å®Œæˆã€‚")
            
        return results
    except Exception as e:
        logger.error(f"æ‰§è¡Œ prepare_for_library_rebuild æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}", exc_info=True)
        raise

def cleanup_deleted_media_item(item_id: str, item_name: str, item_type: str, series_id_from_webhook: Optional[str] = None) -> Optional[Dict[str, Any]]:
    """
    å¤„ç†ä¸€ä¸ªä» Emby ä¸­è¢«åˆ é™¤çš„åª’ä½“é¡¹ï¼ŒåŒæ­¥æ¸…é™¤æ‰€æœ‰ç›¸å…³çš„æ•°æ®ã€‚
    """
    logger.info(f"  âœ æ£€æµ‹åˆ° Emby åª’ä½“é¡¹è¢«åˆ é™¤: '{item_name}' (Type: {item_type}, EmbyID: {item_id})ï¼Œå¼€å§‹æ¸…ç†æµç¨‹...")

    try:
        # ======================================================================
        # è¾…åŠ©å‡½æ•°ï¼šæ‰§è¡Œå¤–ç§‘æ‰‹æœ¯å¼ç§»é™¤ï¼Œå¹¶è¿”å›å‰©ä½™çš„ ID æ•°é‡
        # ======================================================================
        def remove_id_from_metadata(cursor, target_emby_id):
            """
            ä» media_metadata çš„ JSON æ•°ç»„ä¸­ç§»é™¤æŒ‡å®šçš„ Emby IDã€‚
            è¿”å›: (remaining_count, tmdb_id, item_type, parent_tmdb_id, season_number)
            """
            sql_remove = """
                UPDATE media_metadata
                SET 
                    emby_item_ids_json = COALESCE((
                        SELECT jsonb_agg(elem)
                        FROM jsonb_array_elements_text(emby_item_ids_json) elem
                        WHERE elem != %s
                    ), '[]'::jsonb),
                    asset_details_json = COALESCE((
                        SELECT jsonb_agg(elem)
                        FROM jsonb_array_elements(COALESCE(asset_details_json, '[]'::jsonb)) elem
                        WHERE (elem->>'emby_item_id') IS NULL OR (elem->>'emby_item_id') != %s
                    ), '[]'::jsonb),
                    last_updated_at = NOW()
                WHERE emby_item_ids_json @> %s::jsonb
                RETURNING tmdb_id, item_type, parent_series_tmdb_id, season_number, jsonb_array_length(emby_item_ids_json) as remaining_len;
            """
            cursor.execute(sql_remove, (target_emby_id, target_emby_id, json.dumps([target_emby_id])))
            row = cursor.fetchone()
            
            if row:
                return row['remaining_len'], row['tmdb_id'], row['item_type'], row['parent_series_tmdb_id'], row['season_number']
            return None, None, None, None, None

        # ======================================================================
        # å¼€å§‹å¤„ç†
        # ======================================================================
        
        target_tmdb_id_for_full_cleanup: Optional[str] = None
        target_item_type_for_full_cleanup: Optional[str] = None
        cascaded_cleanup_info = None

        with get_db_connection() as conn:
            with conn.cursor() as cursor:
                
                # --- æ‰§è¡Œç§»é™¤æ“ä½œ ---
                remaining_count, tmdb_id, db_item_type, parent_tmdb_id, season_num = remove_id_from_metadata(cursor, item_id)

                if remaining_count is None:
                    logger.warning(f"  âœ åœ¨æ•°æ®åº“ä¸­æœªæ‰¾åˆ°åŒ…å« Emby ID {item_id} çš„è®°å½•ï¼Œæ— éœ€æ¸…ç†ã€‚")
                    return None

                # --- æƒ…å†µ A: è¿˜æœ‰å…¶ä»–ç‰ˆæœ¬å­˜åœ¨ ---
                if remaining_count > 0:
                    logger.info(f"  âœ åª’ä½“é¡¹ '{item_name}' (TMDB: {tmdb_id}) ç§»é™¤äº†ä¸€ä¸ªç‰ˆæœ¬ï¼Œä½†ä»æœ‰ {remaining_count} ä¸ªç‰ˆæœ¬åœ¨åº“ä¸­ã€‚")
                    conn.commit()
                    return None

                # --- æƒ…å†µ B: æ‰€æœ‰ç‰ˆæœ¬éƒ½å·²åˆ é™¤ (remaining_count == 0) ---
                logger.info(f"  âœ åª’ä½“é¡¹ '{item_name}' (TMDB: {tmdb_id}) çš„æ‰€æœ‰ç‰ˆæœ¬å‡å·²åˆ é™¤ï¼Œæ ‡è®°ä¸ºâ€œä¸åœ¨åº“ä¸­â€ã€‚")
                
                # 1. æ ‡è®°å½“å‰é¡¹ä¸ºä¸åœ¨åº“
                cursor.execute(
                    "UPDATE media_metadata SET in_library = FALSE WHERE tmdb_id = %s AND item_type = %s",
                    (tmdb_id, db_item_type)
                )

                # 2. æ ¹æ®ç±»å‹å†³å®šåç»­é€»è¾‘
                if db_item_type in ['Movie', 'Series']:
                    target_tmdb_id_for_full_cleanup = tmdb_id
                    target_item_type_for_full_cleanup = db_item_type

                elif db_item_type == 'Season':
                    logger.info(f"  âœ ç¬¬ {season_num} å­£å·²å®Œå…¨åˆ é™¤ï¼Œæ­£åœ¨æ£€æŸ¥çˆ¶å‰§é›† (TMDB: {parent_tmdb_id})...")
                    
                    cursor.execute(
                        "UPDATE media_metadata SET in_library = FALSE, emby_item_ids_json = '[]'::jsonb, asset_details_json = NULL WHERE parent_series_tmdb_id = %s AND season_number = %s AND item_type = 'Episode'",
                        (parent_tmdb_id, season_num)
                    )
                    
                    cursor.execute(
                        "DELETE FROM resubscribe_index WHERE tmdb_id = %s AND item_type = 'Season' AND season_number = %s",
                        (parent_tmdb_id, season_num)
                    )

                    cursor.execute(
                        "SELECT COUNT(*) as count FROM media_metadata WHERE parent_series_tmdb_id = %s AND item_type = 'Episode' AND in_library = TRUE",
                        (parent_tmdb_id,)
                    )
                    if cursor.fetchone()['count'] == 0:
                        logger.warning(f"  âœ çˆ¶å‰§é›†å·²æ— ä»»ä½•åœ¨åº“åˆ†é›†ï¼Œå°†è§¦å‘æ•´å‰§æ¸…ç†ã€‚")
                        target_tmdb_id_for_full_cleanup = parent_tmdb_id
                        target_item_type_for_full_cleanup = 'Series'

                elif db_item_type == 'Episode':
                    cursor.execute(
                        """
                        SELECT 1 
                        FROM media_metadata 
                        WHERE parent_series_tmdb_id = %s 
                          AND season_number = %s 
                          AND item_type = 'Episode' 
                          AND in_library = TRUE
                        LIMIT 1
                        """,
                        (parent_tmdb_id, season_num)
                    )
                    has_episodes_in_season = cursor.fetchone()

                    if not has_episodes_in_season:
                        logger.info(f"  âœ ç¬¬ {season_num} å­£å·²æ— ä»»ä½•åœ¨åº“åˆ†é›†ï¼Œæ ‡è®°è¯¥å­£ä¸ºç¦»çº¿ã€‚")
                        cursor.execute(
                            """
                            UPDATE media_metadata 
                            SET in_library = FALSE, asset_details_json = NULL 
                            WHERE parent_series_tmdb_id = %s 
                              AND season_number = %s 
                              AND item_type = 'Season'
                            """,
                            (parent_tmdb_id, season_num)
                        )
                        cursor.execute(
                            """
                            DELETE FROM resubscribe_index 
                            WHERE tmdb_id = %s 
                              AND item_type = 'Season' 
                              AND season_number = %s
                            """,
                            (parent_tmdb_id, season_num)
                        )

                    logger.info(f"  âœ æ­£åœ¨æ£€æŸ¥çˆ¶å‰§é›† (TMDB: {parent_tmdb_id}) æ˜¯å¦å·²ç©º...")
                    cursor.execute(
                        """
                        SELECT 1 
                        FROM media_metadata 
                        WHERE parent_series_tmdb_id = %s 
                          AND item_type = 'Episode' 
                          AND in_library = TRUE
                        LIMIT 1
                        """,
                        (parent_tmdb_id,)
                    )
                    has_episodes_in_series = cursor.fetchone()

                    if not has_episodes_in_series:
                        logger.warning(f"  âœ çˆ¶å‰§é›†å·²æ— ä»»ä½•åœ¨åº“åˆ†é›†ï¼Œå°†è§¦å‘æ•´å‰§æ¸…ç†ã€‚")
                        target_tmdb_id_for_full_cleanup = parent_tmdb_id
                        target_item_type_for_full_cleanup = 'Series'

                # ======================================================================
                # æ­¥éª¤ 2: æ‰§è¡Œç»Ÿä¸€çš„â€œå®Œå…¨æ¸…ç†â€ (é’ˆå¯¹æ•´éƒ¨å‰§/ç”µå½±ç¦»çº¿)
                # ======================================================================
                if target_tmdb_id_for_full_cleanup:
                    logger.info(f"--- å¼€å§‹å¯¹ TMDB ID: {target_tmdb_id_for_full_cleanup} (Type: {target_item_type_for_full_cleanup}) æ‰§è¡Œç»Ÿä¸€æ¸…ç† ---")
                    
                    cursor.execute(
                        "SELECT title, emby_item_ids_json FROM media_metadata WHERE tmdb_id = %s AND item_type = %s",
                        (target_tmdb_id_for_full_cleanup, target_item_type_for_full_cleanup)
                    )
                    row = cursor.fetchone()
                    item_title = row['title'] if row and row['title'] else "æœªçŸ¥æ ‡é¢˜"
                    parent_emby_ids = []
                    if row and row['emby_item_ids_json']:
                        raw_ids = row['emby_item_ids_json']
                        if isinstance(raw_ids, list):
                            parent_emby_ids = raw_ids
                        elif isinstance(raw_ids, str):
                            try:
                                parent_emby_ids = json.loads(raw_ids)
                            except Exception as e:
                                logger.warning(f"è§£æ Emby IDs JSON å¤±è´¥: {e}")
                    
                    if not isinstance(parent_emby_ids, list):
                        parent_emby_ids = []
                    
                    cascaded_cleanup_info = {
                        'tmdb_id': target_tmdb_id_for_full_cleanup,
                        'item_type': target_item_type_for_full_cleanup,
                        'item_name': item_title,
                        'emby_ids': parent_emby_ids
                    }

                    cursor.execute(
                        """
                        UPDATE media_metadata 
                        SET in_library = FALSE, 
                            emby_item_ids_json = '[]'::jsonb, 
                            asset_details_json = NULL
                        WHERE tmdb_id = %s AND item_type = %s
                        """,
                        (target_tmdb_id_for_full_cleanup, target_item_type_for_full_cleanup)
                    )

                    if target_item_type_for_full_cleanup == 'Series':
                        cursor.execute(
                            """
                            UPDATE media_metadata 
                            SET in_library = FALSE, 
                                emby_item_ids_json = '[]'::jsonb, 
                                asset_details_json = NULL
                            WHERE parent_series_tmdb_id = %s AND item_type IN ('Season', 'Episode')
                            """,
                            (target_tmdb_id_for_full_cleanup,)
                        )
                        logger.info(f"  âœ å·²çº§è”æ ‡è®°è¯¥å‰§é›†ä¸‹çš„ {cursor.rowcount} ä¸ªå­é¡¹(å­£/é›†)ä¸ºç¦»çº¿ã€‚")

                    if target_item_type_for_full_cleanup == 'Series':
                        sql_reset_watchlist = """
                            UPDATE media_metadata
                            SET watching_status = 'NONE'
                            WHERE tmdb_id = %s AND item_type = 'Series' AND watching_status != 'NONE'
                        """
                        cursor.execute(sql_reset_watchlist, (target_tmdb_id_for_full_cleanup,))
                        if cursor.rowcount > 0:
                            logger.info(f"  âœ å·²å°†è¯¥å‰§é›†ä»æ™ºèƒ½è¿½å‰§åˆ—è¡¨ç§»é™¤ã€‚")

                    if target_item_type_for_full_cleanup == 'Movie':
                        cursor.execute("DELETE FROM resubscribe_index WHERE tmdb_id = %s AND item_type = 'Movie'", (target_tmdb_id_for_full_cleanup,))
                    else:
                        cursor.execute("DELETE FROM resubscribe_index WHERE tmdb_id = %s AND item_type = 'Season'", (target_tmdb_id_for_full_cleanup,))
                    
                    if cursor.rowcount > 0: 
                        logger.info(f"  âœ å·²ä»åª’ä½“æ´—ç‰ˆç¼“å­˜ä¸­ç§»é™¤ {cursor.rowcount} æ¡è®°å½•ã€‚")

                    if target_item_type_for_full_cleanup == 'Movie':
                        cursor.execute("""
                            SELECT emby_collection_id, name, all_tmdb_ids_json
                            FROM collections_info
                            WHERE all_tmdb_ids_json @> %s::jsonb
                        """, (json.dumps([target_tmdb_id_for_full_cleanup]),))
                        
                        affected_collections = cursor.fetchall()
                        
                        for col in affected_collections:
                            c_id = col['emby_collection_id']
                            c_name = col['name']
                            tmdb_ids = col['all_tmdb_ids_json']
                            
                            if not tmdb_ids: continue

                            cursor.execute("""
                                SELECT 1 
                                FROM media_metadata 
                                WHERE tmdb_id = ANY(%s) 
                                  AND in_library = TRUE
                                LIMIT 1
                            """, (tmdb_ids,))
                            
                            has_remaining_items = cursor.fetchone()
                            
                            if not has_remaining_items:
                                logger.info(f"  ğŸ—‘ï¸ åŸç”Ÿåˆé›† '{c_name}' (ID: {c_id}) å†…æ‰€æœ‰åª’ä½“å‡å·²ç¦»çº¿ï¼Œæ­£åœ¨è‡ªåŠ¨æ¸…ç†è¯¥åˆé›†è®°å½•...")
                                cursor.execute("DELETE FROM collections_info WHERE emby_collection_id = %s", (c_id,))
                    
                    logger.info(f"--- å¯¹ TMDB ID: {target_tmdb_id_for_full_cleanup} çš„å®Œå…¨æ¸…ç†å·²å®Œæˆ ---")

                # æäº¤äº‹åŠ¡
                conn.commit()

        return cascaded_cleanup_info

    except Exception as e:
        logger.error(f"æ¸…ç†è¢«åˆ é™¤çš„åª’ä½“é¡¹ {item_id} æ—¶å‘ç”Ÿä¸¥é‡æ•°æ®åº“é”™è¯¯: {e}", exc_info=True)
        return None

def cleanup_offline_media() -> Dict[str, int]:
    """
    ã€æ–°å¢ã€‘æ¸…ç†æ‰€æœ‰â€œä¸åœ¨åº“â€ä¸”â€œæ— è®¢é˜…/è¿½å‰§çŠ¶æ€â€çš„åª’ä½“å…ƒæ•°æ®ã€‚
    ç”¨äºç»™æ•°æ®åº“ç˜¦èº«ï¼Œç§»é™¤ä¸å†éœ€è¦çš„ç¦»çº¿ç¼“å­˜ã€‚
    """
    results = {
        "media_metadata_deleted": 0,
        "resubscribe_index_cleaned": 0,
        "cleanup_index_cleaned": 0
    }
    
    try:
        with get_db_connection() as conn:
            with conn.cursor() as cursor:
                # 1. æ ¸å¿ƒæ¸…ç†ï¼šåˆ é™¤ media_metadata ä¸­ç¬¦åˆæ¡ä»¶çš„è®°å½•
                # æ¡ä»¶ï¼š
                #   - in_library = FALSE (ä¸åœ¨åº“)
                #   - subscription_status = 'NONE' (æ— è®¢é˜…)
                #   - watching_status = 'NONE' (æ— è¿½å‰§çŠ¶æ€ - é˜²æ­¢è¯¯åˆ æ­£åœ¨è¿½ä½†æš‚æ—¶ç¼ºé›†çš„å†…å®¹)
                logger.info("æ­£åœ¨æ‰§è¡Œç¦»çº¿åª’ä½“æ¸…ç†ä»»åŠ¡...")
                
                cursor.execute("""
                    DELETE FROM media_metadata
                    WHERE in_library = FALSE
                      AND subscription_status = 'NONE'
                      AND (watching_status IS NULL OR watching_status = 'NONE');
                """)
                results["media_metadata_deleted"] = cursor.rowcount
                logger.info(f"  âœ å·²ä» media_metadata åˆ é™¤ {results['media_metadata_deleted']} æ¡æ— æ•ˆç¦»çº¿è®°å½•ã€‚")

                # 2. çº§è”æ¸…ç†ï¼šæ¸…ç† resubscribe_index ä¸­çš„å­¤å„¿è®°å½•
                # (å³ï¼šä¸»è¡¨ä¸­å·²ç»ä¸å­˜åœ¨ï¼Œä½†æ´—ç‰ˆè¡¨ä¸­è¿˜æ®‹ç•™çš„è®°å½•)
                cursor.execute("""
                    DELETE FROM resubscribe_index ri
                    WHERE NOT EXISTS (
                        SELECT 1 FROM media_metadata mm
                        WHERE mm.tmdb_id = ri.tmdb_id AND mm.item_type = ri.item_type
                    );
                """)
                results["resubscribe_index_cleaned"] = cursor.rowcount
                
                # 3. çº§è”æ¸…ç†ï¼šæ¸…ç† cleanup_index ä¸­çš„å­¤å„¿è®°å½•
                cursor.execute("""
                    DELETE FROM cleanup_index ci
                    WHERE NOT EXISTS (
                        SELECT 1 FROM media_metadata mm
                        WHERE mm.tmdb_id = ci.tmdb_id AND mm.item_type = ci.item_type
                    );
                """)
                results["cleanup_index_cleaned"] = cursor.rowcount

            conn.commit()
            logger.info(f"ç¦»çº¿åª’ä½“æ¸…ç†å®Œæˆã€‚ç»Ÿè®¡: {results}")
            return results

    except Exception as e:
        logger.error(f"æ‰§è¡Œç¦»çº¿åª’ä½“æ¸…ç†æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        raise

def clear_all_vectors() -> int:
    """
    æ¸…ç©ºæ‰€æœ‰å·²ç”Ÿæˆçš„å‘é‡æ•°æ®ã€‚
    åœºæ™¯ï¼šç”¨æˆ·æ›´æ¢äº† Embedding æ¨¡å‹ï¼Œæ—§çš„å‘é‡æ•°æ®ä¸å†é€‚ç”¨ï¼Œå¿…é¡»æ¸…é™¤ã€‚
    """
    try:
        with get_db_connection() as conn:
            cursor = conn.cursor()
            # ä»…æ¸…ç©º embedding å­—æ®µï¼Œä¿ç•™å…¶ä»–å…ƒæ•°æ®
            cursor.execute("UPDATE media_metadata SET overview_embedding = NULL WHERE overview_embedding IS NOT NULL")
            count = cursor.rowcount
            conn.commit()
            logger.info(f"  âœ… å·²æ¸…ç©º {count} æ¡å‘é‡æ•°æ®ã€‚")
            return count
    except Exception as e:
        logger.error(f"æ¸…ç©ºå‘é‡æ•°æ®å¤±è´¥: {e}", exc_info=True)
        raise