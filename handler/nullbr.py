# handler/nullbr.py
import logging
import requests
import re
import time  
import threading 
from datetime import datetime
from database import settings_db, media_db, request_db
import config_manager

import constants
import utils
import handler.tmdb as tmdb
try:
    from p115client import P115Client
except ImportError:
    P115Client = None

logger = logging.getLogger(__name__)

# â˜…â˜…â˜… ç¡¬ç¼–ç é…ç½®ï¼šNullbr â˜…â˜…â˜…
NULLBR_APP_ID = "7DqRtfNX3"
NULLBR_API_BASE = "https://api.nullbr.com"

# å†…å­˜ç¼“å­˜ï¼Œç”¨äºå­˜å‚¨ç”¨æˆ·ç­‰çº§ä»¥æ§åˆ¶è¯·æ±‚é¢‘ç‡ï¼Œé¿å…æ¯æ¬¡éƒ½æŸ¥åº“
_user_level_cache = {
    "sub_name": "free",
    "daily_used": 0,
    "daily_quota": 0,
    "updated_at": 0
}

def get_config():
    return settings_db.get_setting('nullbr_config') or {}

def _get_headers():
    config = get_config()
    api_key = config.get('api_key')
    headers = {
        "Content-Type": "application/json",
        "X-APP-ID": NULLBR_APP_ID,
        "User-Agent": f"EmbyToolkit/{constants.APP_VERSION}"
    }
    if api_key:
        headers["X-API-KEY"] = api_key
    return headers

def _parse_size_to_gb(size_str):
    """å°†å¤§å°å­—ç¬¦ä¸²è½¬æ¢ä¸º GB (float)"""
    if not size_str: return 0.0
    size_str = size_str.upper().replace(',', '')
    match = re.search(r'([\d\.]+)\s*(TB|GB|MB|KB)', size_str)
    if not match: return 0.0
    num = float(match.group(1))
    unit = match.group(2)
    if unit == 'TB': return num * 1024
    elif unit == 'GB': return num
    elif unit == 'MB': return num / 1024
    elif unit == 'KB': return num / 1024 / 1024
    return 0.0

def _is_resource_valid(item, filters, media_type='movie', episode_count=0):
    """æ ¹æ®é…ç½®è¿‡æ»¤èµ„æº (ä¿æŒåŸæœ‰é€»è¾‘)"""
    if not filters: return True
    
    # 1. åˆ†è¾¨ç‡
    if filters.get('resolutions'):
        res = item.get('resolution')
        if not res or res not in filters['resolutions']: return False

    # 2. è´¨é‡
    if filters.get('qualities'):
        item_quality = item.get('quality')
        if not item_quality: return False
        q_list = [item_quality] if isinstance(item_quality, str) else item_quality
        if not any(q in q_list for q in filters['qualities']): return False

    # 3. å¤§å°è¿‡æ»¤ (GB) 
    if media_type == 'tv':
        min_size = float(filters.get('tv_min_size') or filters.get('min_size') or 0)
        max_size = float(filters.get('tv_max_size') or filters.get('max_size') or 0)
    else:
        min_size = float(filters.get('movie_min_size') or filters.get('min_size') or 0)
        max_size = float(filters.get('movie_max_size') or filters.get('max_size') or 0)
    
    if min_size > 0 or max_size > 0:
        size_gb = _parse_size_to_gb(item.get('size'))
        
        # å¦‚æœæ˜¯å‰§é›†ä¸”è·å–åˆ°äº†é›†æ•°ï¼Œè®¡ç®—å•é›†å¹³å‡å¤§å°
        check_size = size_gb
        if media_type == 'tv' and episode_count > 0:
            check_size = size_gb / episode_count
            # ç®€å•çš„é˜²è¯¯åˆ¤ï¼šå¦‚æœè®¡ç®—å‡ºçš„å•é›†å¤ªå°ï¼ˆæ¯”å¦‚å°äº0.1Gï¼‰ï¼Œå¯èƒ½æ˜¯è·å–åˆ°äº†å•é›†èµ„æºè€Œä¸æ˜¯æ•´å­£åŒ…
            # ä½†è¿™é‡Œæˆ‘ä»¬ä¸»è¦ç›®çš„æ˜¯è¿‡æ»¤æ•´å­£åŒ…ï¼Œæ‰€ä»¥æŒ‰å¹³å‡å€¼ç®—æ²¡é—®é¢˜ã€‚
            # å¦‚æœèµ„æºæœ¬èº«å°±æ˜¯å•é›†ï¼ˆå¦‚S01E01ï¼‰ï¼Œé™¤ä»¥æ€»é›†æ•°åä¼šå˜å¾—éå¸¸å°ï¼Œè‡ªç„¶ä¼šè¢« min_size è¿‡æ»¤æ‰ï¼Œ
            # è¿™æ­£å¥½ç¬¦åˆâ€œè®¢é˜…æ•´å­£â€çš„éœ€æ±‚ï¼ˆä¸è¦å•é›†æ•£ä»¶ï¼‰ã€‚

        if min_size > 0 and check_size < min_size:
            return False
        if max_size > 0 and check_size > max_size:
            return False

    # 4. ä¸­å­—
    if filters.get('require_zh'):
        if item.get('is_zh_sub'): return True
        title = item.get('title', '').upper()
        zh_keywords = ['ä¸­å­—', 'ä¸­è‹±', 'å­—å¹•', 'CHS', 'CHT', 'CN', 'DIY', 'å›½è¯­', 'å›½ç²¤']
        if not any(k in title for k in zh_keywords): return False

    # 5. å®¹å™¨ (ä»…ç”µå½±)
    if media_type != 'tv' and filters.get('containers'):
        title = item.get('title', '').lower()
        link = item.get('link', '').lower()
        ext = None
        if 'mkv' in title or link.endswith('.mkv'): ext = 'mkv'
        elif 'mp4' in title or link.endswith('.mp4'): ext = 'mp4'
        elif 'iso' in title or link.endswith('.iso'): ext = 'iso'
        if not ext or ext not in filters['containers']: return False

    return True

# ==============================================================================
# â˜…â˜…â˜… æ–°å¢ï¼šç”¨æˆ· API äº¤äº’ä¸è‡ªåŠ¨æµæ§ â˜…â˜…â˜…
# ==============================================================================

def get_user_info():
    """è·å–ç”¨æˆ·ä¿¡æ¯"""
    url = f"{NULLBR_API_BASE}/user/info"
    try:
        proxies = config_manager.get_proxies_for_requests()
        response = requests.get(url, headers=_get_headers(), timeout=15, proxies=proxies)
        response.raise_for_status()
        data = response.json()
        
        if data.get('success'):
            user_data = data.get('data', {})
            _user_level_cache.update({
                'sub_name': user_data.get('sub_name', 'free').lower(),
                'daily_used': user_data.get('daily_used', 0),
                'daily_quota': user_data.get('daily_quota', 0),
                'updated_at': time.time()
            })
            return user_data
        else:
            raise Exception(data.get('message', 'è·å–ç”¨æˆ·ä¿¡æ¯å¤±è´¥'))
    except Exception as e:
        logger.error(f"  âš ï¸ è·å– NULLBR ç”¨æˆ·ä¿¡æ¯å¼‚å¸¸: {e}")
        raise e

def redeem_code(code):
    """
    ä½¿ç”¨å…‘æ¢ç 
    """
    url = f"{NULLBR_API_BASE}/user/redeem"
    payload = {"code": code}
    try:
        proxies = config_manager.get_proxies_for_requests()
        
        response = requests.post(url, json=payload, headers=_get_headers(), timeout=15, proxies=proxies)
        data = response.json()
        
        if response.status_code == 200 and data.get('success'):
            get_user_info()
            return data
        else:
            msg = data.get('message') or "å…‘æ¢å¤±è´¥"
            return {"success": False, "message": msg}
    except Exception as e:
        logger.error(f"  âœ å…‘æ¢è¯·æ±‚å¼‚å¸¸: {e}")
        return {"success": False, "message": str(e)}

def _wait_for_rate_limit():
    """
    æ ¹æ®ç”¨æˆ·ç­‰çº§è‡ªåŠ¨æ‰§è¡Œæµæ§ç¡çœ 
    Free: 25 req/min -> ~2.4s interval
    Silver: 60 req/min -> ~1.0s interval
    Golden: 100 req/min -> ~0.6s interval
    """
    # å¦‚æœç¼“å­˜è¿‡æœŸ(è¶…è¿‡1å°æ—¶)ï¼Œå°è¯•æ›´æ–°ä¸€ä¸‹ï¼Œä½†ä¸é˜»å¡ä¸»æµç¨‹
    if time.time() - _user_level_cache['updated_at'] > 3600:
        try:
            get_user_info()
        except:
            pass 

    level = _user_level_cache.get('sub_name', 'free')
    
    if 'golden' in level:
        time.sleep(0.6)
    elif 'silver' in level:
        time.sleep(1.0)
    else:
        # Free or unknown
        time.sleep(2.5)

def _enrich_items_with_status(items):
    """æ‰¹é‡æŸ¥è¯¢æœ¬åœ°åº“çŠ¶æ€ (ä¿æŒä¸å˜)"""
    if not items: return items
    tmdb_ids = [str(i.get('tmdbid') or i.get('id')) for i in items if (i.get('tmdbid') or i.get('id'))]
    if not tmdb_ids: return items

    library_map_movie = media_db.check_tmdb_ids_in_library(tmdb_ids, 'Movie')
    library_map_series = media_db.check_tmdb_ids_in_library(tmdb_ids, 'Series')
    sub_status_movie = request_db.get_global_subscription_statuses_by_tmdb_ids(tmdb_ids, 'Movie')
    sub_status_series = request_db.get_global_subscription_statuses_by_tmdb_ids(tmdb_ids, 'Series')

    for item in items:
        tid = str(item.get('tmdbid') or item.get('id') or '')
        mtype = item.get('media_type', 'movie')
        if not tid: continue
        
        in_lib = False
        sub_stat = None
        if mtype == 'tv':
            if f"{tid}_Series" in library_map_series: in_lib = True
            sub_stat = sub_status_series.get(tid)
        else:
            if f"{tid}_Movie" in library_map_movie: in_lib = True
            sub_stat = sub_status_movie.get(tid)
        
        item['in_library'] = in_lib
        item['subscription_status'] = sub_stat
    return items

def get_preset_lists():
    custom_presets = settings_db.get_setting('nullbr_presets')
    if custom_presets and isinstance(custom_presets, list) and len(custom_presets) > 0:
        return custom_presets
    return utils.DEFAULT_NULLBR_PRESETS

def fetch_list_items(list_id, page=1):
    _wait_for_rate_limit()
    url = f"{NULLBR_API_BASE}/list/{list_id}"
    params = {"page": page}
    try:
        proxies = config_manager.get_proxies_for_requests()
        response = requests.get(url, params=params, headers=_get_headers(), timeout=15, proxies=proxies)
        response.raise_for_status()
        data = response.json()
        items = data.get('items', [])
        enriched_items = _enrich_items_with_status(items)
        return {"code": 200, "data": {"list": enriched_items, "total": data.get('total_results', 0)}}
    except Exception as e:
        logger.error(f"è·å–ç‰‡å•å¤±è´¥: {e}")
        raise e

def search_media(keyword, page=1):
    _wait_for_rate_limit() # è‡ªåŠ¨æµæ§
    url = f"{NULLBR_API_BASE}/search"
    params = { "query": keyword, "page": page }
    try:
        proxies = config_manager.get_proxies_for_requests()
        response = requests.get(url, params=params, headers=_get_headers(), timeout=15, proxies=proxies)
        response.raise_for_status()
        data = response.json()
        items = data.get('items', [])
        enriched_items = _enrich_items_with_status(items)
        return { "code": 200, "data": { "list": enriched_items, "total": data.get('total_results', 0) } }
    except Exception as e:
        logger.error(f"  âœ NULLBR æœç´¢å¤±è´¥: {e}")
        raise e

def _fetch_single_source(tmdb_id, media_type, source_type, season_number=None):
    _wait_for_rate_limit() # è‡ªåŠ¨æµæ§
    
    url = ""
    if media_type == 'movie':
        url = f"{NULLBR_API_BASE}/movie/{tmdb_id}/{source_type}"
    elif media_type == 'tv':
        if season_number:
            url = f"{NULLBR_API_BASE}/tv/{tmdb_id}/season/{season_number}/{source_type}"
        else:
            if source_type == '115':
                url = f"{NULLBR_API_BASE}/tv/{tmdb_id}/115"
            elif source_type == 'magnet':
                url = f"{NULLBR_API_BASE}/tv/{tmdb_id}/season/1/magnet"
            else:
                return []
    else:
        return []

    try:
        proxies = config_manager.get_proxies_for_requests()
        response = requests.get(url, headers=_get_headers(), timeout=10, proxies=proxies)
        
        if response.status_code == 404: return []
        
        if response.status_code == 402:
            logger.warning("  âš ï¸ NULLBR æ¥å£è¿”å› 402: é…é¢å·²è€—å°½")
            if _user_level_cache['daily_quota'] > 0:
                _user_level_cache['daily_used'] = _user_level_cache['daily_quota']
            return []
            
        response.raise_for_status()
        
        _user_level_cache['daily_used'] = _user_level_cache.get('daily_used', 0) + 1
        
        data = response.json()
        raw_list = data.get(source_type, [])
        
        cleaned_list = []
        for item in raw_list:
            link = item.get('share_link') or item.get('magnet') or item.get('ed2k')
            title = item.get('title') or item.get('name')
            
            if link and title:
                if media_type == 'tv' and source_type == 'magnet' and not season_number:
                    title = f"[S1] {title}"
                
                is_zh = item.get('zh_sub') == 1
                if not is_zh:
                    t_upper = title.upper()
                    zh_keywords = ['ä¸­å­—', 'ä¸­è‹±', 'å­—å¹•', 'CHS', 'CHT', 'CN', 'DIY', 'å›½è¯­', 'å›½ç²¤']
                    if any(k in t_upper for k in zh_keywords): is_zh = True
                
                # å­£å·æ¸…æ´—é€»è¾‘ (ä¿æŒä¸å˜)
                if media_type == 'tv' and season_number:
                    try:
                        target_season = int(season_number)
                        match = re.search(r'(?:^|\.|\[|\s|-)S(\d{1,2})(?:\.|\]|\s|E|-|$)', title.upper())
                        if match and int(match.group(1)) != target_season: continue
                        match_zh = re.search(r'ç¬¬(\d{1,2})å­£', title)
                        if match_zh and int(match_zh.group(1)) != target_season: continue
                    except: pass

                cleaned_list.append({
                    "title": title,
                    "size": item.get('size', 'æœªçŸ¥'),
                    "resolution": item.get('resolution'),
                    "quality": item.get('quality'),
                    "link": link,
                    "source_type": source_type.upper(),
                    "is_zh_sub": is_zh
                })
        return cleaned_list
    except Exception as e:
        logger.warning(f"  âœ è·å– {source_type} èµ„æºå¤±è´¥: {e}")
        return []

def fetch_resource_list(tmdb_id, media_type='movie', specific_source=None, season_number=None):
    config = get_config()
    if specific_source:
        sources_to_fetch = [specific_source]
    else:
        sources_to_fetch = config.get('enabled_sources', ['115', 'magnet', 'ed2k'])

    if _user_level_cache.get('daily_quota', 0) > 0 and _user_level_cache.get('daily_used', 0) >= _user_level_cache.get('daily_quota', 0):
        logger.warning(f"  âš ï¸ ä»Šæ—¥ API é…é¢å·²ç”¨å®Œ ({_user_level_cache['daily_used']}/{_user_level_cache['daily_quota']})ï¼Œè·³è¿‡è¯·æ±‚")
        raise Exception("ä»Šæ—¥ API é…é¢å·²ç”¨å®Œï¼Œè¯·æ˜æ—¥å†è¯•æˆ–å‡çº§å¥—é¤ã€‚")
    
    all_resources = []
    
    if '115' in sources_to_fetch:
        try: all_resources.extend(_fetch_single_source(tmdb_id, media_type, '115', season_number))
        except: pass
    if 'magnet' in sources_to_fetch:
        try: all_resources.extend(_fetch_single_source(tmdb_id, media_type, 'magnet', season_number))
        except: pass
    if media_type == 'movie' and 'ed2k' in sources_to_fetch:
        try: all_resources.extend(_fetch_single_source(tmdb_id, media_type, 'ed2k'))
        except: pass
    
    filters = config.get('filters', {})
    # å¦‚æœ filters å…¨ä¸ºç©ºå€¼ï¼Œåˆ™ä¸è¿‡æ»¤
    has_filter = any(filters.values())
    if not has_filter:
        return all_resources
        
    # å¦‚æœæ˜¯å‰§é›†ï¼Œè·å–è¯¥å­£çš„æ€»é›†æ•°ï¼Œç”¨äºåç»­æŒ‰å•é›†å¹³å‡å¤§å°è¿‡æ»¤
    episode_count = 0
    if media_type == 'tv' and season_number:
        try:
            tmdb_api_key = config_manager.APP_CONFIG.get(constants.CONFIG_OPTION_TMDB_API_KEY)
            if tmdb_api_key:
                season_info = tmdb.get_tv_season_details(tmdb_id, season_number, tmdb_api_key)
                if season_info and 'episodes' in season_info:
                    episode_count = len(season_info['episodes'])
                    logger.info(f"  âœ [NULLBR] è·å–åˆ° ç¬¬ {season_number} å­£ æ€»é›†æ•°: {episode_count}ï¼Œå°†æŒ‰å•é›†å¹³å‡å¤§å°è¿‡æ»¤ã€‚")
        except Exception as e:
            logger.warning(f"  âš ï¸ è·å– TMDb å­£é›†æ•°å¤±è´¥ï¼Œå°†æŒ‰æ€»å¤§å°è¿‡æ»¤: {e}")

    # 5. æ‰§è¡Œè¿‡æ»¤ (ä¼ å…¥ episode_count)
    filtered_list = [
        res for res in all_resources 
        if _is_resource_valid(res, filters, media_type, episode_count=episode_count)
    ]
    logger.info(f"  âœ èµ„æºè¿‡æ»¤: åŸå§‹ {len(all_resources)} -> è¿‡æ»¤å {len(filtered_list)}")
    return filtered_list

# ==============================================================================
# â˜…â˜…â˜… 115 æ¨é€é€»è¾‘  â˜…â˜…â˜…
# ==============================================================================

def _clean_link(link):
    """
    æ¸…æ´—é“¾æ¥ï¼šå»é™¤é¦–å°¾ç©ºæ ¼ï¼Œå¹¶å®‰å…¨å»é™¤æœ«å°¾çš„ HTML è„å­—ç¬¦ (&#)
    """
    if not link:
        return ""
    link = link.strip()
    while link.endswith('&#') or link.endswith('&') or link.endswith('#'):
        if link.endswith('&#'):
            link = link[:-2]
        elif link.endswith('&') or link.endswith('#'):
            link = link[:-1]
    return link

def notify_cms_scan():
    """
    é€šçŸ¥ CMS æ‰§è¡Œç›®å½•æ•´ç† (ç”Ÿæˆ strm)
    æ¥å£: /api/sync/lift_by_token?type=auto_organize&token=...
    """
    config = get_config()
    cms_url = config.get('cms_url')
    cms_token = config.get('cms_token')

    if not cms_url or not cms_token:
        # ç”¨æˆ·æ²¡é…ç½® CMSï¼Œç›´æ¥å¿½ç•¥ï¼Œä¸æŠ¥é”™
        return

    cms_url = cms_url.rstrip('/')
    # æ„é€ é€šçŸ¥æ¥å£ URL
    api_url = f"{cms_url}/api/sync/lift_by_token"
    params = {
        "type": "auto_organize",
        "token": cms_token
    }

    try:
        logger.info(f"  âœ æ­£åœ¨é€šçŸ¥ CMS æ‰§è¡Œæ•´ç†...")
        response = requests.get(api_url, params=params, timeout=5)
        response.raise_for_status()
        
        res_json = response.json()
        if res_json.get('code') == 200 or res_json.get('success'):
            logger.info(f"  âœ… CMS é€šçŸ¥æˆåŠŸ: {res_json.get('msg', 'OK')}")
        else:
            logger.warning(f"  âš ï¸ CMS é€šçŸ¥è¿”å›å¼‚å¸¸: {res_json}")

    except Exception as e:
        logger.warning(f"  âš ï¸ CMS é€šçŸ¥å‘é€å¤±è´¥: {e}")
        raise e

def push_to_115(resource_link, title):
    """
    æ™ºèƒ½æ¨é€ï¼šæ”¯æŒ 115/115cdn/anxia è½¬å­˜ å’Œ ç£åŠ›ç¦»çº¿
    """
    if P115Client is None:
        raise ImportError("æœªå®‰è£… p115 åº“")

    config = get_config()
    cookies = config.get('p115_cookies')
    
    try:
        cid_val = config.get('p115_save_path_cid', 0)
        save_path_cid = int(cid_val) if cid_val else 0
    except:
        save_path_cid = 0

    if not cookies:
        raise ValueError("æœªé…ç½® 115 Cookies")

    clean_url = _clean_link(resource_link)
    logger.info(f"  âœ [NULLBR] å¾…å¤„ç†é“¾æ¥: {clean_url}")
    
    client = P115Client(cookies)
    
    try:
        # æ”¯æŒ 115.com, 115cdn.com, anxia.com
        target_domains = ['115.com', '115cdn.com', 'anxia.com']
        is_115_share = any(d in clean_url for d in target_domains) and ('magnet' not in clean_url)
        
        if is_115_share:
            logger.info(f"  âœ [NULLBR] è¯†åˆ«ä¸º 115 è½¬å­˜ä»»åŠ¡ -> CID: {save_path_cid}")
            share_code = None
            match = re.search(r'/s/([a-z0-9]+)', clean_url)
            if match: share_code = match.group(1)
            if not share_code: raise Exception("æ— æ³•ä»é“¾æ¥ä¸­æå–åˆ†äº«ç ")
            receive_code = ''
            pwd_match = re.search(r'password=([a-z0-9]+)', clean_url)
            if pwd_match: receive_code = pwd_match.group(1)
            
            resp = {} 
            try:
                if hasattr(client, 'fs_share_import_to_dir'):
                     resp = client.fs_share_import_to_dir(share_code, receive_code, save_path_cid)
                elif hasattr(client, 'fs_share_import'):
                    resp = client.fs_share_import(share_code, receive_code, save_path_cid)
                elif hasattr(client, 'share_import'):
                    resp = client.share_import(share_code, receive_code, save_path_cid)
                else:
                    api_url = "https://webapi.115.com/share/receive"
                    payload = {'share_code': share_code, 'receive_code': receive_code, 'cid': save_path_cid}
                    r = client.request(api_url, method='POST', data=payload)
                    resp = r.json() if hasattr(r, 'json') else r
            except Exception as e:
                raise Exception(f"è°ƒç”¨è½¬å­˜æ¥å£å¤±è´¥: {e}")

            if resp and resp.get('state'):
                logger.info(f"  âœ… 115 è½¬å­˜æˆåŠŸ: {title}")
                return True
            else:
                err = resp.get('error_msg') if resp else 'æ— å“åº”'
                err = err or resp.get('msg') or str(resp)
                raise Exception(f"è½¬å­˜å¤±è´¥: {err}")

        else:
            # ==================================================
            # â˜…â˜…â˜… ç£åŠ›/Ed2k ç¦»çº¿ä¸‹è½½ (æŒ‡çº¹å¯¹æ¯”ç‰ˆ) â˜…â˜…â˜…
            # ==================================================
            logger.info(f"  âœ [NULLBR] è¯†åˆ«ä¸ºç£åŠ›/ç¦»çº¿ä»»åŠ¡ -> CID: {save_path_cid}")
            
            # 1. ã€å…³é”®æ­¥éª¤ã€‘å»ºç«‹å¿«ç…§ï¼šè®°å½•å½“å‰ç›®å½•ä¸‹å·²å­˜åœ¨æ–‡ä»¶çš„ pick_code
            existing_pick_codes = set()
            try:
                # è·å–å‰50ä¸ªæ–‡ä»¶/æ–‡ä»¶å¤¹ (æŒ‰ä¸Šä¼ æ—¶é—´å€’åº)
                # æ³¨æ„ï¼š115 API è¿”å›çš„ pc (pick_code) æ˜¯å”¯ä¸€æ ‡è¯†
                files_res = client.fs_files({'cid': save_path_cid, 'limit': 50, 'o': 'user_ptime', 'asc': 0})
                if files_res.get('data'):
                    for item in files_res['data']:
                        if item.get('pc'):
                            existing_pick_codes.add(item.get('pc'))
            except Exception as e:
                logger.warning(f"  âš ï¸ è·å–ç›®å½•å¿«ç…§å¤±è´¥(å¯èƒ½æ˜¯ç©ºç›®å½•): {e}")
            
            logger.info(f"  âœ [NULLBR] å½“å‰ç›®å½•å·²æœ‰ {len(existing_pick_codes)} ä¸ªé¡¹ç›®")

            # 2. æ·»åŠ ä»»åŠ¡
            payload = {'url[0]': clean_url, 'wp_path_id': save_path_cid}
            resp = client.offline_add_urls(payload)
            
            if resp.get('state'):
                # è·å– info_hash ç”¨äºè¾…åŠ©æ£€æŸ¥æ­»é“¾
                result_list = resp.get('result', [])
                info_hash = None
                if result_list and isinstance(result_list, list):
                    info_hash = result_list[0].get('info_hash')

                # 3. è½®è¯¢æ£€æµ‹ç›®å½• (å»¶é•¿åˆ° 45ç§’)
                # æ–‡ä»¶å¤¹ç”Ÿæˆæ¯”è¾ƒæ…¢ï¼Œç»™è¶³æ—¶é—´
                max_retries = 3  # 15æ¬¡ * 3ç§’ = 45ç§’
                success_found = False
                
                logger.info(f"  âœ [NULLBR] ä»»åŠ¡å·²æäº¤ï¼Œæ­£åœ¨æ‰«ææ–°é¡¹ç›®...")

                for i in range(max_retries):
                    time.sleep(3) 
                    
                    # --- A. æ£€æŸ¥ç›®å½•æ˜¯å¦æœ‰ã€ä¸åœ¨å¿«ç…§é‡Œã€‘çš„æ–°é¡¹ç›® ---
                    try:
                        check_res = client.fs_files({'cid': save_path_cid, 'limit': 50, 'o': 'user_ptime', 'asc': 0})
                        if check_res.get('data'):
                            for item in check_res['data']:
                                current_pc = item.get('pc')
                                # å¦‚æœå‘ç°ä¸€ä¸ª pick_code ä¸åœ¨æ—§é›†åˆé‡Œï¼Œè¯´æ˜æ˜¯æ–°ç”Ÿæˆçš„
                                if current_pc and (current_pc not in existing_pick_codes):
                                    item_name = item.get('n', 'æœªçŸ¥')
                                    logger.info(f"  âœ… [ç¬¬{i+1}æ¬¡æ£€æŸ¥] å‘ç°æ–°é¡¹ç›®: {item_name}")
                                    success_found = True
                                    break
                        if success_found:
                            break
                    except Exception as e:
                        pass # ç½‘ç»œæ³¢åŠ¨å¿½ç•¥

                    # --- B. è¾…åŠ©æ£€æŸ¥ï¼šä»»åŠ¡æ˜¯å¦æŒ‚äº† ---
                    try:
                        list_resp = client.offline_list(page=1)
                        tasks = list_resp.get('tasks', [])
                        for task in tasks[:10]:
                            if info_hash and task.get('info_hash') == info_hash:
                                if task.get('status') == -1:
                                    try: client.offline_delete([task.get('info_hash')])
                                    except: pass
                                    raise Exception("115ä»»åŠ¡çŠ¶æ€å˜ä¸º[ä¸‹è½½å¤±è´¥]")
                    except Exception as task_err:
                        if "ä¸‹è½½å¤±è´¥" in str(task_err): raise task_err
                        pass

                if success_found:
                    logger.info(f"  âœ… [NULLBR] 115 ç¦»çº¿æˆåŠŸ: {title}")
                    return True
                else:
                    # è¶…æ—¶æœªå‘ç°æ–°æ–‡ä»¶
                    try: 
                        if info_hash: client.offline_delete([info_hash])
                    except: pass
                    
                    logger.warning(f"  [NULLBR] âŒ æœªåœ¨ç›®å½•å‘ç°æ–°é¡¹ç›®ï¼Œåˆ¤å®šä¸ºæ­»é“¾")
                    raise Exception("èµ„æºæ— æ•ˆï¼Œè¯·æ¢ä¸ªæºè¯•è¯•")

            else:
                err = resp.get('error_msg') or resp.get('msg') or 'æœªçŸ¥é”™è¯¯'
                if 'å·²å­˜åœ¨' in str(err):
                    logger.info(f"  âœ… ä»»åŠ¡å·²å­˜åœ¨: {title}")
                    return True
                raise Exception(f"ç¦»çº¿å¤±è´¥: {err}")

    except Exception as e:
        logger.error(f"  âœ 115 æ¨é€å¼‚å¸¸: {e}")
        if "Login" in str(e) or "cookie" in str(e).lower():
            raise Exception("115 Cookie æ— æ•ˆ")
        raise e

def get_115_account_info():
    """
    æç®€çŠ¶æ€æ£€æŸ¥ï¼šåªéªŒè¯ Cookie æ˜¯å¦æœ‰æ•ˆï¼Œä¸è·å–ä»»ä½•è¯¦æƒ…
    """
    if P115Client is None:
        raise Exception("æœªå®‰è£… p115client")
        
    config = get_config()
    cookies = config.get('p115_cookies')
    
    if not cookies:
        raise Exception("æœªé…ç½® Cookies")
        
    try:
        client = P115Client(cookies)
        
        # å°è¯•åˆ—å‡º 1 ä¸ªæ–‡ä»¶ï¼Œè¿™æ˜¯éªŒè¯ Cookie æœ€å¿«æœ€å‡†çš„æ–¹æ³•
        resp = client.fs_files({'limit': 1})
        
        if not resp.get('state'):
            raise Exception("Cookie å·²å¤±æ•ˆ")
            
        # åªè¦æ²¡æŠ¥é”™ï¼Œå°±æ˜¯æœ‰æ•ˆ
        return {
            "valid": True,
            "msg": "Cookie çŠ¶æ€æ­£å¸¸ï¼Œå¯æ­£å¸¸æ¨é€"
        }

    except Exception as e:
        raise Exception("Cookie æ— æ•ˆæˆ–ç½‘ç»œä¸é€š")

def handle_push_request(link, title):
    """
    ç»Ÿä¸€æ¨é€å…¥å£
    """
    # 1. æ¨é€åˆ° 115 (å¦‚æœå¤±è´¥æˆ–æ­»é“¾ï¼Œè¿™é‡Œä¼šç›´æ¥æŠ›å‡ºå¼‚å¸¸ï¼Œä¸­æ–­æµç¨‹)
    push_to_115(link, title)
    
    # 2. 115 æˆåŠŸåï¼Œé€šçŸ¥ CMS æ•´ç†
    notify_cms_scan()
    
    return True

def auto_download_best_resource(tmdb_id, media_type, title, season_number=None):
    """
    [è‡ªåŠ¨ä»»åŠ¡ä¸“ç”¨] æœç´¢å¹¶ä¸‹è½½æœ€ä½³èµ„æº
    :param season_number: å­£å· (ä»… media_type='tv' æ—¶æœ‰æ•ˆ)
    """
    try:
        config = get_config()
        if not config.get('api_key'):
            logger.warning("NULLBR æœªé…ç½® API Keyï¼Œæ— æ³•æ‰§è¡Œè‡ªåŠ¨å…œåº•ã€‚")
            return False

        priority_sources = ['115', 'magnet', 'ed2k']
        user_enabled = config.get('enabled_sources', priority_sources)
        
        # æ„é€ æ—¥å¿—æ ‡é¢˜
        log_title = title
        if media_type == 'tv' and season_number:
            log_title = f"ã€Š{title}ã€‹ç¬¬ {season_number} å­£"

        logger.info(f"  âœ [NULLBR] å¼€å§‹æœç´¢èµ„æº: {log_title} (ID: {tmdb_id})")

        for source in priority_sources:
            if source not in user_enabled: continue
            if media_type == 'tv' and source == 'ed2k': continue

            resources = fetch_resource_list(tmdb_id, media_type, specific_source=source, season_number=season_number)
            
            if not resources:
                continue

            logger.info(f"  âœ [{source.upper()}] æ‰¾åˆ° {len(resources)} ä¸ªèµ„æºï¼Œå¼€å§‹å°è¯•æ¨é€...")

            for index, res in enumerate(resources):
                try:
                    logger.info(f"  ğŸ‘‰ å°è¯•ç¬¬ {index + 1} ä¸ªèµ„æº: {res['title']}")
                    
                    # è°ƒç”¨ç»Ÿä¸€æ¨é€å…¥å£ (115 -> CMS Notify)
                    handle_push_request(res['link'], title)
                    
                    logger.info(f"  âœ… èµ„æºæ¨é€æˆåŠŸï¼Œåœæ­¢åç»­å°è¯•ã€‚")
                    return True
                    
                except Exception as e:
                    logger.warning(f"  âŒ ç¬¬ {index + 1} ä¸ªèµ„æºæ¨é€å¤±è´¥: {e}")
                    logger.info("  ğŸ”„ æ­£åœ¨å°è¯•ä¸‹ä¸€ä¸ªèµ„æº...")
                    continue
            
            logger.info(f"  âš ï¸ [{source.upper()}] æ‰€æœ‰èµ„æºå‡å°è¯•å¤±è´¥ï¼Œåˆ‡æ¢ä¸‹ä¸€æº...")

        logger.info(f"  âŒ æ‰€æœ‰æºçš„æ‰€æœ‰èµ„æºå‡å°è¯•å¤±è´¥: {log_title}")
        return False

    except Exception as e:
        logger.error(f"  âœ NULLBR è‡ªåŠ¨å…œåº•å¤±è´¥: {e}")
        return False