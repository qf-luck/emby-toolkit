# handler/nullbr.py
import logging
import requests
import re
import time  
import os 
from datetime import datetime
from database import settings_db, media_db, request_db
import config_manager

import constants
import utils
import handler.tmdb as tmdb
try:
    from p115client import P115Client
except ImportError:
    P115Client = None

logger = logging.getLogger(__name__)

# â˜…â˜…â˜… ç¡¬ç¼–ç é…ç½®ï¼šNullbr â˜…â˜…â˜…
NULLBR_APP_ID = "7DqRtfNX3"
NULLBR_API_BASE = "https://api.nullbr.com"

# å†…å­˜ç¼“å­˜ï¼Œç”¨äºå­˜å‚¨ç”¨æˆ·ç­‰çº§ä»¥æ§åˆ¶è¯·æ±‚é¢‘ç‡ï¼Œé¿å…æ¯æ¬¡éƒ½æŸ¥åº“
_user_level_cache = {
    "sub_name": "free",
    "daily_used": 0,
    "daily_quota": 0,
    "updated_at": 0
}

def get_config():
    return settings_db.get_setting('nullbr_config') or {}

def _get_headers():
    config = get_config()
    api_key = config.get('api_key')
    headers = {
        "Content-Type": "application/json",
        "X-APP-ID": NULLBR_APP_ID,
        "User-Agent": f"EmbyToolkit/{constants.APP_VERSION}"
    }
    if api_key:
        headers["X-API-KEY"] = api_key
    return headers

def _parse_size_to_gb(size_str):
    """å°†å¤§å°å­—ç¬¦ä¸²è½¬æ¢ä¸º GB (float)"""
    if not size_str: return 0.0
    size_str = size_str.upper().replace(',', '')
    match = re.search(r'([\d\.]+)\s*(TB|GB|MB|KB)', size_str)
    if not match: return 0.0
    num = float(match.group(1))
    unit = match.group(2)
    if unit == 'TB': return num * 1024
    elif unit == 'GB': return num
    elif unit == 'MB': return num / 1024
    elif unit == 'KB': return num / 1024 / 1024
    return 0.0

def _is_resource_valid(item, filters, media_type='movie', episode_count=0):
    """æ ¹æ®é…ç½®è¿‡æ»¤èµ„æº"""
    if not filters:
        return True

    # 1. åˆ†è¾¨ç‡è¿‡æ»¤
    allowed_resolutions = filters.get('resolutions', [])
    if allowed_resolutions:
        res = item.get('resolution')
        if not res or res not in allowed_resolutions:
            logger.debug(f"  âœ èµ„æºã€Š{item.get('title')}ã€‹è¢«è¿‡æ»¤æ‰äº†ï¼Œå› ä¸ºåˆ†è¾¨ç‡ {res} ä¸åœ¨å…è®¸åˆ—è¡¨ä¸­")
            return False

    # 2. è´¨é‡è¿‡æ»¤
    allowed_qualities = filters.get('qualities', [])
    if allowed_qualities:
        item_quality = item.get('quality')
        if not item_quality: return False
        q_list = [item_quality] if isinstance(item_quality, str) else item_quality
        if not any(q in q_list for q in allowed_qualities): 
            logger.debug(f"  âœ èµ„æºã€Š{item.get('title')}ã€‹è¢«è¿‡æ»¤æ‰äº†ï¼Œå› ä¸ºè´¨é‡ {item_quality} ä¸åœ¨å…è®¸åˆ—è¡¨ä¸­")
            return False

    # 3. å¤§å°è¿‡æ»¤ (GB) 
    min_size = 0.0
    max_size = 0.0

    if media_type == 'tv':
        # ä¼˜å…ˆå– tv_min_sizeï¼Œå–ä¸åˆ°(None)åˆ™å°è¯•å– min_sizeï¼Œæœ€åé»˜è®¤ä¸º 0
        v_min = filters.get('tv_min_size')
        if v_min is None: v_min = filters.get('min_size')
        min_size = float(v_min or 0)

        v_max = filters.get('tv_max_size')
        if v_max is None: v_max = filters.get('max_size')
        max_size = float(v_max or 0)
    else:
        v_min = filters.get('movie_min_size')
        if v_min is None: v_min = filters.get('min_size')
        min_size = float(v_min or 0)

        v_max = filters.get('movie_max_size')
        if v_max is None: v_max = filters.get('max_size')
        max_size = float(v_max or 0)
    
    if min_size > 0 or max_size > 0:
        size_gb = _parse_size_to_gb(item.get('size'))
        
        # è®¡ç®—æ£€æŸ¥ç”¨çš„æ•°å€¼
        check_size = size_gb
        
        # åªæœ‰å½“æ˜¯å‰§é›†ã€ä¸”æˆåŠŸè·å–åˆ°äº†é›†æ•°ã€ä¸”é›†æ•°å¤§äº0æ—¶ï¼Œæ‰è®¡ç®—å¹³å‡å¤§å°
        if media_type == 'tv' and episode_count > 0:
            check_size = size_gb / episode_count
            # è°ƒè¯•æ—¥å¿— (å¯é€‰å¼€å¯)
            # logger.debug(f"  [å¤§å°æ£€æŸ¥] æ€»å¤§å°: {size_gb}G, é›†æ•°: {episode_count}, å¹³å‡: {check_size:.2f}G (é™åˆ¶: {min_size}-{max_size})")

        if min_size > 0 and check_size < min_size:
            logger.debug(f"  âœ èµ„æºã€Š{item.get('title')}ã€‹è¢«è¿‡æ»¤æ‰äº†ï¼Œå› ä¸ºå¤§å° {check_size:.2f}G å°äºæœ€å°é™åˆ¶ {min_size}G")
            return False
        if max_size > 0 and check_size > max_size:
            logger.debug(f"  âœ èµ„æºã€Š{item.get('title')}ã€‹è¢«è¿‡æ»¤æ‰äº†ï¼Œå› ä¸ºå¤§å° {check_size:.2f}G å¤§äºæœ€å¤§é™åˆ¶ {max_size}G")
            return False

    # 4. ä¸­å­—è¿‡æ»¤
    if filters.get('require_zh'):
        if item.get('is_zh_sub'): return True
        title = item.get('title', '').upper()
        zh_keywords = ['ä¸­å­—', 'ä¸­è‹±', 'å­—å¹•', 'CHS', 'CHT', 'CN', 'DIY', 'å›½è¯­', 'å›½ç²¤']
        if not any(k in title for k in zh_keywords): 
            logger.debug(f"  âœ èµ„æºã€Š{item.get('title')}ã€‹è¢«è¿‡æ»¤æ‰äº†ï¼Œå› ä¸ºæœªæ£€æµ‹åˆ°ä¸­æ–‡å­—å¹•")
            return False
            

    # 5. å®¹å™¨è¿‡æ»¤
    allowed_containers = filters.get('containers', [])
    if allowed_containers:
        if media_type == 'tv': return True
        title = item.get('title', '').lower()
        link = item.get('link', '').lower()
        ext = None

        if link.startswith('ed2k://'):
            # Ed2k æ ¼å¼: ed2k://|file|æ–‡ä»¶å|å¤§å°|å“ˆå¸Œ|/
            # ä½¿ç”¨ | åˆ†å‰²ï¼Œæ–‡ä»¶åé€šå¸¸åœ¨ç¬¬ 3 éƒ¨åˆ† (ç´¢å¼• 2)
            try:
                parts = link.split('|')
                if len(parts) >= 3:
                    file_name_in_link = parts[2].lower()
                    if file_name_in_link.endswith('.mkv'): ext = 'mkv'
                    elif file_name_in_link.endswith('.mp4'): ext = 'mp4'
                    elif file_name_in_link.endswith('.iso'): ext = 'iso'
                    elif file_name_in_link.endswith('.ts'): ext = 'ts'
                    elif file_name_in_link.endswith('.avi'): ext = 'avi'
            except:
                pass # è§£æå¤±è´¥åˆ™å¿½ç•¥ï¼Œå›é€€åˆ°ä¸‹æ–¹é€»è¾‘

        # å¦‚æœä¸Šé¢æ²¡æå–åˆ° (æ¯”å¦‚æ˜¯ç£åŠ›é“¾æˆ– 115 ç )ï¼Œåˆ™èµ°åŸæœ‰é€»è¾‘
        if not ext:
            if 'mkv' in title or link.endswith('.mkv'): ext = 'mkv'
            elif 'mp4' in title or link.endswith('.mp4'): ext = 'mp4'
            elif 'iso' in title or link.endswith('.iso'): ext = 'iso'
            elif 'ts' in title or link.endswith('.ts'): ext = 'ts'
            elif 'avi' in title or link.endswith('.avi'): ext = 'avi'
            
        if not ext or ext not in allowed_containers: 
            logger.debug(f"  âœ èµ„æºã€Š{item.get('title')}ã€‹è¢«è¿‡æ»¤æ‰äº†ï¼Œå› ä¸ºå®¹å™¨ {ext} ä¸åœ¨å…è®¸åˆ—è¡¨ä¸­")
            return False

    return True

# ==============================================================================
# â˜…â˜…â˜… æ–°å¢ï¼šç”¨æˆ· API äº¤äº’ä¸è‡ªåŠ¨æµæ§ â˜…â˜…â˜…
# ==============================================================================

def get_user_info():
    """è·å–ç”¨æˆ·ä¿¡æ¯"""
    url = f"{NULLBR_API_BASE}/user/info"
    try:
        proxies = config_manager.get_proxies_for_requests()
        response = requests.get(url, headers=_get_headers(), timeout=15, proxies=proxies)
        response.raise_for_status()
        data = response.json()
        
        if data.get('success'):
            user_data = data.get('data', {})
            _user_level_cache.update({
                'sub_name': user_data.get('sub_name', 'free').lower(),
                'daily_used': user_data.get('daily_used', 0),
                'daily_quota': user_data.get('daily_quota', 0),
                'updated_at': time.time()
            })
            return user_data
        else:
            raise Exception(data.get('message', 'è·å–ç”¨æˆ·ä¿¡æ¯å¤±è´¥'))
    except Exception as e:
        logger.error(f"  âš ï¸ è·å– NULLBR ç”¨æˆ·ä¿¡æ¯å¼‚å¸¸: {e}")
        raise e

def redeem_code(code):
    """
    ä½¿ç”¨å…‘æ¢ç 
    """
    url = f"{NULLBR_API_BASE}/user/redeem"
    payload = {"code": code}
    try:
        proxies = config_manager.get_proxies_for_requests()
        
        response = requests.post(url, json=payload, headers=_get_headers(), timeout=15, proxies=proxies)
        data = response.json()
        
        if response.status_code == 200 and data.get('success'):
            get_user_info()
            return data
        else:
            msg = data.get('message') or "å…‘æ¢å¤±è´¥"
            return {"success": False, "message": msg}
    except Exception as e:
        logger.error(f"  âœ å…‘æ¢è¯·æ±‚å¼‚å¸¸: {e}")
        return {"success": False, "message": str(e)}

def _wait_for_rate_limit():
    """
    æ ¹æ®ç”¨æˆ·ç­‰çº§è‡ªåŠ¨æ‰§è¡Œæµæ§ç¡çœ 
    Free: 25 req/min -> ~2.4s interval
    Silver: 60 req/min -> ~1.0s interval
    Golden: 100 req/min -> ~0.6s interval
    """
    # å¦‚æœç¼“å­˜è¿‡æœŸ(è¶…è¿‡1å°æ—¶)ï¼Œå°è¯•æ›´æ–°ä¸€ä¸‹ï¼Œä½†ä¸é˜»å¡ä¸»æµç¨‹
    if time.time() - _user_level_cache['updated_at'] > 3600:
        try:
            get_user_info()
        except:
            pass 

    level = _user_level_cache.get('sub_name', 'free')
    
    if 'golden' in level:
        time.sleep(0.6)
    elif 'silver' in level:
        time.sleep(1.0)
    else:
        # Free or unknown
        time.sleep(2.5)

def _enrich_items_with_status(items):
    """æ‰¹é‡æŸ¥è¯¢æœ¬åœ°åº“çŠ¶æ€ (ä¿æŒä¸å˜)"""
    if not items: return items
    tmdb_ids = [str(i.get('tmdbid') or i.get('id')) for i in items if (i.get('tmdbid') or i.get('id'))]
    if not tmdb_ids: return items

    library_map_movie = media_db.check_tmdb_ids_in_library(tmdb_ids, 'Movie')
    library_map_series = media_db.check_tmdb_ids_in_library(tmdb_ids, 'Series')
    sub_status_movie = request_db.get_global_subscription_statuses_by_tmdb_ids(tmdb_ids, 'Movie')
    sub_status_series = request_db.get_global_subscription_statuses_by_tmdb_ids(tmdb_ids, 'Series')

    for item in items:
        tid = str(item.get('tmdbid') or item.get('id') or '')
        mtype = item.get('media_type', 'movie')
        if not tid: continue
        
        in_lib = False
        sub_stat = None
        if mtype == 'tv':
            if f"{tid}_Series" in library_map_series: in_lib = True
            sub_stat = sub_status_series.get(tid)
        else:
            if f"{tid}_Movie" in library_map_movie: in_lib = True
            sub_stat = sub_status_movie.get(tid)
        
        item['in_library'] = in_lib
        item['subscription_status'] = sub_stat
    return items

def get_preset_lists():
    custom_presets = settings_db.get_setting('nullbr_presets')
    if custom_presets and isinstance(custom_presets, list) and len(custom_presets) > 0:
        return custom_presets
    return utils.DEFAULT_NULLBR_PRESETS

def fetch_list_items(list_id, page=1):
    _wait_for_rate_limit()
    url = f"{NULLBR_API_BASE}/list/{list_id}"
    params = {"page": page}
    try:
        proxies = config_manager.get_proxies_for_requests()
        response = requests.get(url, params=params, headers=_get_headers(), timeout=15, proxies=proxies)
        response.raise_for_status()
        data = response.json()
        items = data.get('items', [])
        enriched_items = _enrich_items_with_status(items)
        return {"code": 200, "data": {"list": enriched_items, "total": data.get('total_results', 0)}}
    except Exception as e:
        logger.error(f"è·å–ç‰‡å•å¤±è´¥: {e}")
        raise e

def search_media(keyword, page=1):
    _wait_for_rate_limit() # è‡ªåŠ¨æµæ§
    url = f"{NULLBR_API_BASE}/search"
    params = { "query": keyword, "page": page }
    try:
        proxies = config_manager.get_proxies_for_requests()
        response = requests.get(url, params=params, headers=_get_headers(), timeout=15, proxies=proxies)
        response.raise_for_status()
        data = response.json()
        items = data.get('items', [])
        enriched_items = _enrich_items_with_status(items)
        return { "code": 200, "data": { "list": enriched_items, "total": data.get('total_results', 0) } }
    except Exception as e:
        logger.error(f"  âœ NULLBR æœç´¢å¤±è´¥: {e}")
        raise e

def _fetch_single_source(tmdb_id, media_type, source_type, season_number=None, episode_number=None):
    _wait_for_rate_limit() # è‡ªåŠ¨æµæ§
    
    url = ""
    if media_type == 'movie':
        url = f"{NULLBR_API_BASE}/movie/{tmdb_id}/{source_type}"
    elif media_type == 'tv':
        # â˜…â˜…â˜… æ ¸å¿ƒä¿®æ”¹ï¼šæ”¯æŒå•é›† URL æ‹¼æ¥ â˜…â˜…â˜…
        if season_number is not None:
            if episode_number is not None:
                # æ¥å£: /tv/{id}/season/{s}/episode/{e}/{source}
                url = f"{NULLBR_API_BASE}/tv/{tmdb_id}/season/{season_number}/episode/{episode_number}/{source_type}"
            else:
                # æ¥å£: /tv/{id}/season/{s}/{source}
                url = f"{NULLBR_API_BASE}/tv/{tmdb_id}/season/{season_number}/{source_type}"
        else:
            # æ•´å‰§æœç´¢ (é€šå¸¸åªæœ‰ 115 æ”¯æŒï¼Œæˆ–è€… magnet æœç¬¬ä¸€å­£)
            if source_type == '115':
                url = f"{NULLBR_API_BASE}/tv/{tmdb_id}/115"
            elif source_type == 'magnet':
                # å¦‚æœæ²¡ä¼ å­£å·ï¼Œé»˜è®¤æœç¬¬1å­£ç£åŠ›ï¼Œæˆ–è€…ä½ å¯ä»¥é€‰æ‹©ä¸æœ
                url = f"{NULLBR_API_BASE}/tv/{tmdb_id}/season/1/magnet"
            else:
                return []
    else:
        return []

    try:
        proxies = config_manager.get_proxies_for_requests()
        response = requests.get(url, headers=_get_headers(), timeout=10, proxies=proxies)
        
        if response.status_code == 404: return []
        
        if response.status_code == 402:
            logger.warning("  âš ï¸ NULLBR æ¥å£è¿”å› 402: é…é¢å·²è€—å°½")
            if _user_level_cache['daily_quota'] > 0:
                _user_level_cache['daily_used'] = _user_level_cache['daily_quota']
            return []
            
        response.raise_for_status()
        
        _user_level_cache['daily_used'] = _user_level_cache.get('daily_used', 0) + 1
        
        data = response.json()
        raw_list = data.get(source_type, [])
        
        cleaned_list = []
        for item in raw_list:
            link = item.get('share_link') or item.get('magnet') or item.get('ed2k')
            title = item.get('title') or item.get('name')
            
            if link and title:
                if media_type == 'tv' and source_type == 'magnet' and not season_number:
                    title = f"[S1] {title}"
                
                is_zh = item.get('zh_sub') == 1
                if not is_zh:
                    t_upper = title.upper()
                    zh_keywords = ['ä¸­å­—', 'ä¸­è‹±', 'å­—å¹•', 'CHS', 'CHT', 'CN', 'DIY', 'å›½è¯­', 'å›½ç²¤']
                    if any(k in t_upper for k in zh_keywords): is_zh = True
                
                # å­£å·æ¸…æ´—é€»è¾‘
                if media_type == 'tv' and season_number:
                    try:
                        target_season = int(season_number)
                        match = re.search(r'(?:^|\.|\[|\s|-)S(\d{1,2})(?:\.|\]|\s|E|-|$)', title.upper())
                        if match and int(match.group(1)) != target_season: continue
                        match_zh = re.search(r'ç¬¬(\d{1,2})å­£', title)
                        if match_zh and int(match_zh.group(1)) != target_season: continue
                    except: pass

                cleaned_list.append({
                    "title": title,
                    "size": item.get('size', 'æœªçŸ¥'),
                    "resolution": item.get('resolution'),
                    "quality": item.get('quality'),
                    "link": link,
                    "source_type": source_type.upper(),
                    "is_zh_sub": is_zh
                })
        return cleaned_list
    except Exception as e:
        logger.warning(f"  âœ è·å– {source_type} èµ„æºå¤±è´¥: {e}")
        return []

def fetch_resource_list(tmdb_id, media_type='movie', specific_source=None, season_number=None, episode_number=None):
    config = get_config()
    
    # 1. ç¡®å®šè¦æœç´¢çš„æº
    if specific_source:
        sources_to_fetch = [specific_source]
    else:
        # å¿…é¡»æ‹·è´ä¸€ä»½ï¼Œé˜²æ­¢ä¿®æ”¹åŸé…ç½®
        sources_to_fetch = list(config.get('enabled_sources', ['115', 'magnet', 'ed2k']))
    
    # 2. è·å–è¿‡æ»¤é…ç½® (æå‰è·å–)
    filters = config.get('filters', {})
    
    # å¦‚æœå¼€å¯äº†å®¹å™¨è¿‡æ»¤ï¼Œå¼ºåˆ¶è·³è¿‡ç£åŠ›é“¾ æœç´¢ä»¥èŠ‚çœé…é¢
    allowed_containers = filters.get('containers', [])
    if allowed_containers and 'magnet' in sources_to_fetch:
        logger.debug(f"  âœ [NULLBR] æ£€æµ‹åˆ°å¼€å¯äº†å®¹å™¨è¿‡æ»¤ ({allowed_containers})ï¼Œå·²è·³è¿‡ç£åŠ›é“¾æœç´¢ä»¥èŠ‚çœé…é¢ã€‚")
        sources_to_fetch.remove('magnet')
    
    # é…é¢æ£€æŸ¥
    if _user_level_cache.get('daily_quota', 0) > 0 and _user_level_cache.get('daily_used', 0) >= _user_level_cache.get('daily_quota', 0):
        logger.warning(f"  âš ï¸ ä»Šæ—¥é…é¢å·²ç”¨å®Œï¼Œæ— æ³•è¯·æ±‚APIæœç´¢èµ„æºã€‚")
        raise Exception("ä»Šæ—¥ API é…é¢å·²ç”¨å®Œï¼Œè¯·æ˜æ—¥å†è¯•æˆ–å‡çº§å¥—é¤ã€‚")

    # ==============================================================================
    # â˜…â˜…â˜… æå‰è®¡ç®—é›†æ•° (ç”¨äºå¤§å°è¿‡æ»¤) â˜…â˜…â˜…
    # ==============================================================================
    episode_count = 0
    should_fetch_ep_count = False
    
    # åªæœ‰æ˜¯å‰§é›†ä¸”æœ‰å­£å·æ—¶æ‰è€ƒè™‘
    if media_type == 'tv' and season_number is not None:
        # æ£€æŸ¥æ˜¯å¦é…ç½®äº†å¤§å°é™åˆ¶
        t_min = filters.get('tv_min_size')
        if t_min is None: t_min = filters.get('min_size')
        
        t_max = filters.get('tv_max_size')
        if t_max is None: t_max = filters.get('max_size')
        
        try:
            if (t_min and float(t_min) > 0) or (t_max and float(t_max) > 0):
                should_fetch_ep_count = True
        except:
            pass 

    if should_fetch_ep_count:
        try:
            tmdb_api_key = config_manager.APP_CONFIG.get(constants.CONFIG_OPTION_TMDB_API_KEY)
            if tmdb_api_key:
                season_info = tmdb.get_tv_season_details(tmdb_id, season_number, tmdb_api_key)
                if season_info and 'episodes' in season_info:
                    episode_count = len(season_info['episodes'])
                    logger.info(f"  âœ [NULLBR] è·å–åˆ° ï¼ˆç¬¬ {season_number} å­£ï¼‰ æ€»é›†æ•°: {episode_count}ï¼Œå°†æŒ‰å•é›†å¹³å‡å¤§å°è¿‡æ»¤ã€‚")
        except Exception as e:
            logger.warning(f"  âš ï¸ è·å– TMDb å­£é›†æ•°å¤±è´¥: {e}")

    # ==============================================================================
    # â˜…â˜…â˜… å¾ªç¯è·å–å¹¶åˆ†åˆ«è¿‡æ»¤ â˜…â˜…â˜…
    # ==============================================================================
    final_filtered_list = []
    
    # å®šä¹‰æºåç§°æ˜ å°„
    source_name_map = {
        '115': '115åˆ†äº«',
        'magnet': 'ç£åŠ›é“¾',
        'ed2k': 'ç”µé©´(Ed2k)'
    }

    for source in sources_to_fetch:
        try:
            # é’ˆå¯¹ ed2k çš„ç‰¹æ®Šåˆ¤æ–­ (TV ä¸æœ ed2k)
            if media_type == 'tv' and source == 'ed2k':
                if episode_number is None:
                    continue
                
            # 1. è·å–åŸå§‹èµ„æº
            raw_res = _fetch_single_source(tmdb_id, media_type, source, season_number, episode_number)
            
            if not raw_res:
                continue

            # 2. ç«‹å³æ‰§è¡Œè¿‡æ»¤
            current_filtered = [
                res for res in raw_res 
                if _is_resource_valid(res, filters, media_type, episode_count=episode_count)
            ]
            
            # 3. æ‰“å°å¸¦æºåç§°çš„æ—¥å¿—
            cn_name = source_name_map.get(source, source.upper())
            logger.info(f"  âœ {cn_name} èµ„æºè¿‡æ»¤: åŸå§‹ {len(raw_res)} -> è¿‡æ»¤å {len(current_filtered)}")
            
            # 4. åŠ å…¥æœ€ç»ˆåˆ—è¡¨
            if current_filtered:
                final_filtered_list.extend(current_filtered)

        except Exception as e:
            logger.warning(f"  âœ è·å– {source} èµ„æºå¼‚å¸¸: {e}")

    return final_filtered_list

# ==============================================================================
# â˜…â˜…â˜… æ™ºèƒ½æ•´ç†æ ¸å¿ƒé€»è¾‘ (Smart Organizer) â˜…â˜…â˜…
# ==============================================================================

class SmartOrganizer:
    def __init__(self, client, tmdb_id, media_type, original_title):
        self.client = client
        self.tmdb_id = tmdb_id
        self.media_type = media_type
        self.original_title = original_title
        self.api_key = config_manager.APP_CONFIG.get(constants.CONFIG_OPTION_TMDB_API_KEY)
        
        # åŠ è½½æ˜ å°„è¡¨ (ç”¨äºå·¥ä½œå®¤/å…³é”®è¯/åˆ†çº§çš„é€»è¾‘åŒ¹é…)
        self.studio_map = settings_db.get_setting('studio_mapping') or utils.DEFAULT_STUDIO_MAPPING
        self.keyword_map = settings_db.get_setting('keyword_mapping') or utils.DEFAULT_KEYWORD_MAPPING
        self.rating_map = settings_db.get_setting('rating_mapping') or utils.DEFAULT_RATING_MAPPING
        self.rating_priority = settings_db.get_setting('rating_priority') or utils.DEFAULT_RATING_PRIORITY
        
        # æå–åŸå§‹æ•°æ®
        self.raw_metadata = self._fetch_raw_metadata()
        self.details = self.raw_metadata
        self.rules = settings_db.get_setting('nullbr_sorting_rules') or []

    def _fetch_raw_metadata(self):
        """
        è·å– TMDb åŸå§‹å…ƒæ•°æ® (ID/Code)ï¼Œä¸è¿›è¡Œä»»ä½•ä¸­æ–‡è½¬æ¢ã€‚
        """
        if not self.api_key: return {}
        
        data = {
            'genre_ids': [], 
            'country_codes': [], 
            'lang_code': None, 
            'company_ids': [], 
            'network_ids': [],
            'keyword_ids': [], 
            'rating_label': 'æœªçŸ¥' # åˆ†çº§æ˜¯ç‰¹ä¾‹ï¼Œå¿…é¡»è®¡ç®—å‡ºæ ‡ç­¾æ‰èƒ½åŒ¹é…
        }

        try:
            raw_details = {}
            if self.media_type == 'tv':
                raw_details = tmdb.get_tv_details(
                    self.tmdb_id, self.api_key, 
                    append_to_response="keywords,content_ratings,networks"
                )
            else:
                raw_details = tmdb.get_movie_details(
                    self.tmdb_id, self.api_key, 
                    append_to_response="keywords,release_dates"
                )

            if not raw_details: return {}

            # 1. åŸºç¡€ ID/Code æå–
            data['genre_ids'] = [g.get('id') for g in raw_details.get('genres', [])]
            data['country_codes'] = [c.get('iso_3166_1') for c in raw_details.get('production_countries', [])]
            if not data['country_codes'] and raw_details.get('origin_country'):
                data['country_codes'] = raw_details.get('origin_country')
            
            data['lang_code'] = raw_details.get('original_language')
            
            data['company_ids'] = [c.get('id') for c in raw_details.get('production_companies', [])]
            data['network_ids'] = [n.get('id') for n in raw_details.get('networks', [])] if self.media_type == 'tv' else []

            # 2. å…³é”®è¯ ID æå–
            kw_container = raw_details.get('keywords', {})
            raw_kw_list = kw_container.get('keywords', []) if self.media_type == 'movie' else kw_container.get('results', [])
            data['keyword_ids'] = [k.get('id') for k in raw_kw_list]

            # 3. åˆ†çº§è®¡ç®— (è¿™æ˜¯å”¯ä¸€éœ€è¦é¢„å¤„ç†æˆ Label çš„ï¼Œå› ä¸ºå®ƒæ˜¯æŠ½è±¡æ¦‚å¿µ)
            # ... (ä¿ç•™åŸæœ‰çš„åˆ†çº§è®¡ç®—é€»è¾‘ï¼Œè®¡ç®—å‡º rating_label) ...
            rating_code = None
            rating_country = None
            if self.media_type == 'tv':
                results = raw_details.get('content_ratings', {}).get('results', [])
                for country in self.rating_priority:
                    if country == 'ORIGIN': continue 
                    found = next((r['rating'] for r in results if r['iso_3166_1'] == country), None)
                    if found:
                        rating_code = found
                        rating_country = country
                        break
            else:
                results = raw_details.get('release_dates', {}).get('results', [])
                for country in self.rating_priority:
                    if country == 'ORIGIN': continue
                    country_release = next((r for r in results if r['iso_3166_1'] == country), None)
                    if country_release:
                        cert = next((x['certification'] for x in country_release.get('release_dates', []) if x.get('certification')), None)
                        if cert:
                            rating_code = cert
                            rating_country = country
                            break
            
            if rating_code and rating_country:
                country_map_list = self.rating_map.get(rating_country, [])
                label_match = next((r['label'] for r in country_map_list if r['code'] == rating_code), None)
                if label_match:
                    data['rating_label'] = label_match

            # è¡¥å……æ ‡é¢˜æ—¥æœŸä¾›é‡å‘½å
            data['title'] = raw_details.get('title') or raw_details.get('name')
            data['date'] = raw_details.get('release_date') or raw_details.get('first_air_date')

            return data

        except Exception as e:
            logger.warning(f"  âš ï¸ [æ•´ç†] è·å–åŸå§‹å…ƒæ•°æ®å¤±è´¥: {e}", exc_info=True)
            return {}

    def _match_rule(self, rule):
        """
        è§„åˆ™åŒ¹é…é€»è¾‘ï¼š
        - æ ‡å‡†å­—æ®µï¼šç›´æ¥æ¯”å¯¹ ID/Code
        - é›†åˆå­—æ®µï¼ˆå·¥ä½œå®¤/å…³é”®è¯ï¼‰ï¼šé€šè¿‡ Label åæŸ¥ Config ä¸­çš„ ID åˆ—è¡¨ï¼Œå†æ¯”å¯¹ TMDb ID
        """
        if not self.raw_metadata: return False
        
        # 1. åª’ä½“ç±»å‹
        if rule.get('media_type') and rule['media_type'] != 'all':
            if rule['media_type'] != self.media_type: return False

        # 2. ç±»å‹ (Genres) - ID åŒ¹é…
        if rule.get('genres'):
            # rule['genres'] å­˜çš„æ˜¯ ID åˆ—è¡¨ (å¦‚ [16, 35])
            # self.raw_metadata['genre_ids'] æ˜¯ TMDb ID åˆ—è¡¨
            # åªè¦æœ‰ä¸€ä¸ªäº¤é›†å°±ç®—å‘½ä¸­
            rule_ids = [int(x) for x in rule['genres']]
            if not any(gid in self.raw_metadata['genre_ids'] for gid in rule_ids): return False

        # 3. å›½å®¶ (Countries) - Code åŒ¹é…
        if rule.get('countries'):
            # rule['countries'] å­˜çš„æ˜¯ Code (å¦‚ ['US', 'CN'])
            if not any(c in self.raw_metadata['country_codes'] for c in rule['countries']): return False

        # 4. è¯­è¨€ (Languages) - Code åŒ¹é…
        if rule.get('languages'):
            if self.raw_metadata['lang_code'] not in rule['languages']: return False

        # 5. å·¥ä½œå®¤ (Studios) - Label -> ID åŒ¹é…
        if rule.get('studios'):
            # rule['studios'] å­˜çš„æ˜¯ Label (å¦‚ ['æ¼«å¨', 'Netflix'])
            # æˆ‘ä»¬éœ€è¦éå†è¿™äº› Labelï¼Œå» self.studio_map é‡Œæ‰¾å¯¹åº”çš„ ID
            target_ids = set()
            for label in rule['studios']:
                # æ‰¾åˆ°é…ç½®é¡¹
                config_item = next((item for item in self.studio_map if item['label'] == label), None)
                if config_item:
                    target_ids.update(config_item.get('company_ids', []))
                    target_ids.update(config_item.get('network_ids', []))
            
            # æ£€æŸ¥ TMDb çš„ company/network ID æ˜¯å¦åœ¨ target_ids ä¸­
            has_company = any(cid in target_ids for cid in self.raw_metadata['company_ids'])
            has_network = any(nid in target_ids for nid in self.raw_metadata['network_ids'])
            
            if not (has_company or has_network): return False
            
        # 6. å…³é”®è¯ (Keywords) - Label -> ID åŒ¹é…
        if rule.get('keywords'):
            target_ids = set()
            for label in rule['keywords']:
                config_item = next((item for item in self.keyword_map if item['label'] == label), None)
                if config_item:
                    target_ids.update(config_item.get('ids', []))
            
            # å…¼å®¹å­—ç¬¦ä¸²/æ•°å­— ID
            tmdb_kw_ids = [int(k) for k in self.raw_metadata['keyword_ids']]
            target_ids_int = [int(k) for k in target_ids]
            
            if not any(kid in target_ids_int for kid in tmdb_kw_ids): return False

        # 7. åˆ†çº§ (Rating) - Label åŒ¹é…
        if rule.get('ratings'):
            if self.raw_metadata['rating_label'] not in rule['ratings']: return False

        return True

    def get_target_cid(self):
        """éå†è§„åˆ™ï¼Œè¿”å›å‘½ä¸­çš„ CIDã€‚æœªå‘½ä¸­è¿”å› None"""
        for rule in self.rules:
            if not rule.get('enabled', True): continue
            if self._match_rule(rule):
                logger.info(f"  ğŸ¯ [æ•´ç†] å‘½ä¸­è§„åˆ™: {rule.get('name')} -> CID: {rule.get('cid')}")
                return rule.get('cid')
        return None

    def _extract_video_info(self, filename):
        """
        ä»æ–‡ä»¶åæå–è§†é¢‘ä¿¡æ¯ (æ¥æº Â· åˆ†è¾¨ç‡ Â· ç¼–ç  Â· éŸ³é¢‘ Â· åˆ¶ä½œç»„)
        å‚è€ƒæ ¼å¼: BluRay Â· 1080p Â· X264 Â· DDP 7.1 Â· CMCT
        """
        info_tags = []
        name_upper = filename.upper()
        
        # 1. æ¥æº/è´¨é‡ (Source)
        source = ""
        if re.search(r'REMUX', name_upper): source = 'Remux'
        elif re.search(r'BLU-?RAY|BD', name_upper): source = 'BluRay'
        elif re.search(r'WEB-?DL', name_upper): source = 'WEB-DL'
        elif re.search(r'WEB-?RIP', name_upper): source = 'WEBRip'
        elif re.search(r'HDTV', name_upper): source = 'HDTV'
        elif re.search(r'DVD', name_upper): source = 'DVD'
        
        # 2. ç‰¹æ•ˆ (Effect: HDR/DV)
        effect = ""
        is_dv = re.search(r'\b(DV|DOVI|DOLBY\s?VISION)\b', name_upper)
        is_hdr = re.search(r'\b(HDR|HDR10\+?)\b', name_upper)
        
        if is_dv and is_hdr: effect = "HDR" # é€šå¸¸æ–‡ä»¶åå†™ WEB-DL HDR DVï¼Œè¿™é‡Œç®€åŒ–æ˜¾ç¤ºï¼Œæˆ–è€…ç»„åˆ
        elif is_dv: effect = "DV"
        elif is_hdr: effect = "HDR"
        
        # ç»„åˆ Source å’Œ Effect (å¦‚ WEB-DL HDR)
        if source:
            info_tags.append(f"{source} {effect}".strip())
        elif effect:
            info_tags.append(effect)

        # 3. åˆ†è¾¨ç‡ (Resolution)
        res_match = re.search(r'(2160|1080|720|480)[pP]', filename)
        if res_match:
            info_tags.append(res_match.group(0).lower())
        elif '4K' in name_upper:
            info_tags.append('2160p')

        # 4. ç¼–ç  (Codec)
        if re.search(r'[HX]265|HEVC', name_upper): info_tags.append('H265')
        elif re.search(r'[HX]264|AVC', name_upper): info_tags.append('H264')
        elif re.search(r'AV1', name_upper): info_tags.append('AV1')
        elif re.search(r'MPEG-?2', name_upper): info_tags.append('MPEG2')

        # 5. éŸ³é¢‘ (Audio)
        audio_info = []
        # éŸ³é¢‘ç¼–ç 
        if re.search(r'ATMOS', name_upper): audio_info.append('Atmos')
        elif re.search(r'TRUEHD', name_upper): audio_info.append('TrueHD')
        elif re.search(r'DTS-?HD(\s?MA)?', name_upper): audio_info.append('DTS-HD')
        elif re.search(r'DTS', name_upper): audio_info.append('DTS')
        elif re.search(r'DDP|EAC3|DOLBY\s?DIGITAL\+', name_upper): audio_info.append('DDP')
        elif re.search(r'AC3|DD', name_upper): audio_info.append('AC3')
        elif re.search(r'AAC', name_upper): audio_info.append('AAC')
        elif re.search(r'FLAC', name_upper): audio_info.append('FLAC')
        
        # å£°é“
        chan_match = re.search(r'\b(7\.1|5\.1|2\.0)\b', filename)
        if chan_match:
            audio_info.append(chan_match.group(1))
            
        if audio_info:
            info_tags.append(" ".join(audio_info))

        # 6. å‘å¸ƒç»„ (Release Group) - è°ƒç”¨ helpers.RELEASE_GROUPS
        # é€»è¾‘ï¼šéå†æ‰€æœ‰æ­£åˆ™ï¼Œå¦‚æœåŒ¹é…åˆ°ï¼Œæå–æ–‡ä»¶åä¸­çš„åŸå§‹å­—ç¬¦ä¸²
        group_found = False
        for group_key, patterns in utils.RELEASE_GROUPS.items() if hasattr(utils, 'RELEASE_GROUPS') else {}.items():
             # æ³¨æ„ï¼šè¿™é‡Œå‡è®¾ helpers è¢« import ä¸º utils æˆ–è€… helpersï¼Œæ ¹æ®æ–‡ä»¶å¤´ import æƒ…å†µè°ƒæ•´
             # åŸæ–‡ä»¶ import utils, ä½† RELEASE_GROUPS åœ¨ helpers.pyã€‚
             # å¦‚æœ nullbr.py æ²¡æœ‰ import helpersï¼Œéœ€è¦ç¡®ä¿èƒ½è®¿é—®åˆ°ã€‚
             # å‡è®¾ helpers.py çš„å†…å®¹åœ¨ helpers æ¨¡å—ä¸­ï¼Œæˆ–è€…è¢« utils å¼•ç”¨ã€‚
             # æ—¢ç„¶ä½ æä¾›äº† helpers.pyï¼Œä¸” nullbr.py å¤´éƒ¨æ²¡æœ‰ import helpersï¼Œ
             # **è¯·ç¡®ä¿åœ¨ nullbr.py å¤´éƒ¨æ·»åŠ : import handler.helpers as helpers æˆ– from tasks import helpers**
             pass

        # ä¿®æ­£ï¼šç›´æ¥ä½¿ç”¨ helpers æ¨¡å— (éœ€è¦åœ¨æ–‡ä»¶å¤´ import tasks.helpers as helpers)
        # è€ƒè™‘åˆ°åŸæ–‡ä»¶ç»“æ„ï¼Œè¿™é‡Œå°è¯•ä» helpers åŒ¹é…
        try:
            from tasks import helpers # å»¶è¿Ÿå¯¼å…¥é˜²æ­¢å¾ªç¯å¼•ç”¨ï¼Œæˆ–è€…æ”¾åœ¨æ–‡ä»¶å¤´
            for group_name, patterns in helpers.RELEASE_GROUPS.items():
                for pattern in patterns:
                    try:
                        # ä½¿ç”¨æ­£åˆ™æŸ¥æ‰¾æ–‡ä»¶åä¸­çš„ç»„å
                        match = re.search(pattern, filename, re.IGNORECASE)
                        if match:
                            # åŒ¹é…åˆ°äº†ï¼Œä¿ç•™æ–‡ä»¶åä¸­çš„åŸå§‹å†™æ³• (match.group(0))
                            info_tags.append(match.group(0))
                            group_found = True
                            break
                    except: pass
                if group_found: break
            
            # å¦‚æœæ²¡åœ¨å­—å…¸é‡Œæ‰¾åˆ°ï¼Œå°è¯•åŒ¹é…å¸¸è§çš„ -Group ç»“å°¾
            if not group_found:
                # åŒ¹é…æ–‡ä»¶åæœ«å°¾çš„ -Group (å¦‚ -CMCT.mkv)
                # å»æ‰æ‰©å±•å
                name_no_ext = os.path.splitext(filename)[0]
                match_suffix = re.search(r'-([a-zA-Z0-9]+)$', name_no_ext)
                if match_suffix:
                    possible_group = match_suffix.group(1)
                    # æ’é™¤å¸¸è§éç»„ååç¼€
                    if len(possible_group) > 2 and possible_group.upper() not in ['1080P', '2160P', '4K', 'HDR', 'H265', 'H264']:
                        info_tags.append(possible_group)
        except ImportError:
            pass

        return " Â· ".join(info_tags) if info_tags else ""

    def _rename_file_node(self, file_node, new_base_name, is_tv=False):
        """é‡å‘½åå•ä¸ªæ–‡ä»¶èŠ‚ç‚¹"""
        original_name = file_node.get('n', '')
        ext = original_name.split('.')[-1]
        
        # æå–æ ‡ç­¾ä¿¡æ¯
        video_info = self._extract_video_info(original_name)
        
        # æ„é€ åç¼€ï¼šæ³¨æ„è¿™é‡Œä½¿ç”¨ " Â· " ä½œä¸ºåˆ†éš”ç¬¦
        suffix = f" Â· {video_info}" if video_info else ""
        
        if is_tv:
            # å‰§é›†ï¼šå°è¯•æå– SxxExx
            # åŒ¹é… S01E01, S1E1, Ep01, ç¬¬01é›†
            pattern = r'(?:s|S)(\d{1,2})(?:e|E)(\d{1,2})|Ep?(\d{1,2})|ç¬¬(\d{1,3})[é›†è¯]'
            match = re.search(pattern, original_name)
            if match:
                s, e, ep_only, zh_ep = match.groups()
                season_num = int(s) if s else 1
                episode_num = int(e) if e else (int(ep_only) if ep_only else int(zh_ep))
                
                # æ ¼å¼åŒ–ä¸º S01E01
                s_str = f"S{season_num:02d}"
                e_str = f"E{episode_num:02d}"
                
                # å‰§é›†æ ¼å¼ï¼šTitle - S01E01 Â· Tags.ext
                new_name = f"{new_base_name} - {s_str}{e_str}{suffix}.{ext}"
                
                return new_name, season_num
            else:
                # æ²¡åŒ¹é…åˆ°é›†æ•°ï¼Œä¸æ”¹å
                return original_name, None
        else:
            # ç”µå½±æ ¼å¼ï¼šTitle (Year) Â· Tags.ext
            new_name = f"{new_base_name}{suffix}.{ext}"
            return new_name, None

    def execute(self, root_item, target_cid):
        """æ‰§è¡Œæ•´ç†ï¼šåŒºåˆ†å•æ–‡ä»¶å½’æ¡£ä¸æ–‡ä»¶å¤¹æ•´ç†"""
        # 1. å‡†å¤‡æ ‡å‡†åç§° (ä½œä¸ºæ–‡ä»¶å¤¹å)
        title = self.details.get('title') or self.original_title
        date_str = self.details.get('date') or ''
        year = date_str[:4] if date_str else ''
        
        # æ›¿æ¢éæ³•å­—ç¬¦
        safe_title = re.sub(r'[\\/:*?"<>|]', '', title).strip()
        std_root_name = f"{safe_title} ({year}) {{tmdb-{self.tmdb_id}}}" if year else f"{safe_title} {{tmdb-{self.tmdb_id}}}"
        
        # 2. è¯†åˆ«ç±»å‹
        root_id = root_item.get('fid') or root_item.get('cid')
        # 115 API: æœ‰ fid çš„æ˜¯æ–‡ä»¶ï¼Œæ²¡æœ‰ fid (åªæœ‰ cid) çš„æ˜¯æ–‡ä»¶å¤¹
        is_file = bool(root_item.get('fid'))
        
        # ==================================================
        # åˆ†æ”¯ A: å•æ–‡ä»¶å¤„ç† (åˆ›å»ºæ–‡ä»¶å¤¹ -> ç§»åŠ¨ -> æ”¹å)
        # ==================================================
        if is_file:
            logger.info(f"  ğŸ› ï¸ [æ•´ç†] è¯†åˆ«ä¸ºå•æ–‡ä»¶ï¼Œæ‰§è¡Œå½’æ¡£æ¨¡å¼...")
            
            # A1. ç¡®å®šæ–°æ–‡ä»¶å¤¹åˆ›å»ºåœ¨å“ªé‡Œ
            # å¦‚æœæœ‰ç›®æ ‡ target_cid (å‘½ä¸­è§„åˆ™)ï¼Œå°±å»é‚£é‡Œå»º
            # å¦‚æœæ²¡æœ‰ (æœªå‘½ä¸­è§„åˆ™)ï¼Œå°±åœ¨å½“å‰æ–‡ä»¶æ‰€åœ¨çš„ç›®å½•å»º (root_item['cid'] å³ä¸ºçˆ¶ç›®å½•id)
            dest_parent_cid = target_cid if (target_cid and str(target_cid) != '0') else root_item.get('cid')
            
            # A2. åˆ›å»ºæ ‡å‡†å‘½åçš„æ–‡ä»¶å¤¹
            mk_res = self.client.fs_mkdir(std_root_name, dest_parent_cid)
            new_folder_cid = mk_res.get('cid')
            
            if not new_folder_cid:
                logger.error(f"  âŒ [æ•´ç†] åˆ›å»ºæ–‡ä»¶å¤¹å¤±è´¥: {std_root_name}")
                return False
                
            # A3. å°†æ–‡ä»¶ç§»åŠ¨åˆ°æ–°æ–‡ä»¶å¤¹å†…
            self.client.fs_move(root_id, new_folder_cid)
            
            # A4. é‡å‘½åæ–‡ä»¶æœ¬èº« (åŠ ä¸Šåç¼€å’ŒTags)
            new_filename, _ = self._rename_file_node(root_item, safe_title, is_tv=(self.media_type=='tv'))
            
            if new_filename != root_item.get('n'):
                self.client.fs_rename((root_id, new_filename))
                logger.info(f"  âœ… [æ•´ç†] å•æ–‡ä»¶å½’æ¡£å®Œæˆ: {new_filename}")
            else:
                logger.info(f"  âœ… [æ•´ç†] å•æ–‡ä»¶å½’æ¡£å®Œæˆ (æ— éœ€æ”¹å)")

        # ==================================================
        # åˆ†æ”¯ B: æ–‡ä»¶å¤¹å¤„ç† (é‡å‘½åæ–‡ä»¶å¤¹ -> å†…éƒ¨æ•´ç† -> ç§»åŠ¨)
        # ==================================================
        else:
            logger.info(f"  ğŸ› ï¸ [æ•´ç†] è¯†åˆ«ä¸ºæ–‡ä»¶å¤¹ï¼Œæ‰§è¡Œé‡å‘½å: {root_item.get('n')} -> {std_root_name}")
            
            # B1. é‡å‘½åæ ¹æ–‡ä»¶å¤¹
            self.client.fs_rename((root_id, std_root_name))
            
            # B2. è¿›å…¥å†…éƒ¨å¤„ç† (é‡å‘½åè§†é¢‘æ–‡ä»¶ + å‰§é›†å½’ç±» + åƒåœ¾æ¸…ç†)
            files_res = self.client.fs_files({'cid': root_id, 'limit': 1000})
            if files_res.get('data'):
                season_folders_cache = {} # { season_num: folder_cid }
                
                # å®šä¹‰ç™½åå•åç¼€ (è§†é¢‘ + å­—å¹•)
                video_exts = ['mp4', 'mkv', 'avi', 'ts', 'iso', 'rmvb', 'wmv', 'mov', 'm2ts']
                sub_exts = ['srt', 'ass', 'ssa', 'sub', 'vtt', 'sup']
                
                for sub_file in files_res['data']:
                    fid = sub_file.get('fid')
                    if not fid: continue # å¿½ç•¥å­æ–‡ä»¶å¤¹
                    
                    file_name = sub_file.get('n', '')
                    ext = file_name.split('.')[-1].lower() if '.' in file_name else ''
                    
                    # --- åƒåœ¾æ¸…ç† ---
                    is_video = ext in video_exts
                    is_sub = ext in sub_exts
                    
                    if not (is_video or is_sub):
                        logger.info(f"  ğŸ—‘ï¸ [æ•´ç†] åˆ é™¤åƒåœ¾æ–‡ä»¶: {file_name}")
                        self.client.fs_delete([fid])
                        continue
                        
                    # è§†é¢‘å¤§å°æ£€æŸ¥ (<100MB åˆ é™¤)
                    if is_video:
                        should_delete = False
                        raw_size = sub_file.get('size')
                        try:
                            if isinstance(raw_size, (int, float)):
                                if raw_size < 100 * 1024 * 1024: should_delete = True
                            elif isinstance(raw_size, str):
                                s_upper = raw_size.upper().replace(',', '')
                                if 'GB' not in s_upper and 'TB' not in s_upper:
                                    if 'KB' in s_upper or 'BYTES' in s_upper: should_delete = True
                                    elif 'MB' in s_upper:
                                        match = re.search(r'([\d\.]+)', s_upper)
                                        if match and float(match.group(1)) < 100: should_delete = True
                        except: pass

                        if should_delete:
                            logger.info(f"  ğŸ—‘ï¸ [æ•´ç†] åˆ é™¤è¿‡å°è§†é¢‘: {file_name}")
                            self.client.fs_delete([fid])
                            continue
                    
                    # --- è§†é¢‘æ–‡ä»¶é‡å‘½å ---
                    if is_video:
                        new_filename, season_num = self._rename_file_node(sub_file, safe_title, is_tv=(self.media_type=='tv'))
                        
                        if new_filename != file_name:
                            self.client.fs_rename((fid, new_filename))
                        
                        # å‰§é›†ï¼šç§»åŠ¨åˆ° Season ç›®å½•
                        if self.media_type == 'tv' and season_num is not None:
                            s_folder_cid = season_folders_cache.get(season_num)
                            if not s_folder_cid:
                                s_name = f"Season {season_num:02d}"
                                found = False
                                for existing in files_res['data']:
                                    if existing.get('n') == s_name and existing.get('cid'):
                                        s_folder_cid = existing.get('cid')
                                        found = True
                                        break
                                if not found:
                                    mk_res = self.client.fs_mkdir(s_name, root_id)
                                    if mk_res.get('state'): s_folder_cid = mk_res.get('cid')
                                
                                if s_folder_cid: season_folders_cache[season_num] = s_folder_cid
                            
                            if s_folder_cid:
                                self.client.fs_move(fid, s_folder_cid)

            # B3. æ•´ä½“ç§»åŠ¨åˆ°ç›®æ ‡ CID
            if target_cid and str(target_cid) != '0':
                logger.info(f"  ğŸšš [æ•´ç†] ç§»åŠ¨æ–‡ä»¶å¤¹åˆ°åˆ†ç±»ç›®å½• CID: {target_cid}")
                self.client.fs_move(root_id, target_cid)
        
        return True

# ==============================================================================
# â˜…â˜…â˜… 115 æ¨é€é€»è¾‘  â˜…â˜…â˜…
# ==============================================================================

def _clean_link(link):
    """
    æ¸…æ´—é“¾æ¥ï¼šå»é™¤é¦–å°¾ç©ºæ ¼ï¼Œå¹¶å®‰å…¨å»é™¤æœ«å°¾çš„ HTML è„å­—ç¬¦ (&#)
    """
    if not link:
        return ""
    link = link.strip()
    while link.endswith('&#') or link.endswith('&') or link.endswith('#'):
        if link.endswith('&#'):
            link = link[:-2]
        elif link.endswith('&') or link.endswith('#'):
            link = link[:-1]
    return link

def notify_cms_scan():
    """
    é€šçŸ¥ CMS æ‰§è¡Œç›®å½•æ•´ç† (ç”Ÿæˆ strm)
    """
    config = get_config()
    cms_url = config.get('cms_url')
    cms_token = config.get('cms_token')

    if not cms_url or not cms_token:
        return

    cms_url = cms_url.rstrip('/')
    
    # â˜…â˜…â˜… æ ¸å¿ƒä¿®æ”¹ï¼šæ ¹æ®æ˜¯å¦å¯ç”¨æ™ºèƒ½æ•´ç†ï¼Œé€‰æ‹©ä¸åŒçš„æ¥å£ â˜…â˜…â˜…
    enable_smart_organize = config.get('enable_smart_organize', False)
    
    if enable_smart_organize:
        # æ™ºèƒ½æ•´ç†æ¨¡å¼ï¼šæ–‡ä»¶å·²å½’ä½ï¼Œæ‰§è¡Œå¢é‡åŒæ­¥ (lift_sync)
        api_url = f"{cms_url}/api/sync/lift_by_token"
        params = {
            "type": "lift_sync",
            "token": cms_token
        }
        logger.info(f"  âœ [CMS] é€šçŸ¥ CMS æ‰§è¡Œå¢é‡åŒæ­¥ ...")
    else:
        # é»˜è®¤æ¨¡å¼ï¼šæ–‡ä»¶åœ¨ä¸‹è½½ç›®å½•ï¼Œæ‰§è¡Œè‡ªåŠ¨æ•´ç† (auto_organize)
        api_url = f"{cms_url}/api/sync/lift_by_token"
        params = {
            "type": "auto_organize",
            "token": cms_token
        }
        logger.info(f"  âœ [CMS] é€šçŸ¥ CMS æ‰§è¡Œè‡ªåŠ¨æ•´ç† ...")

    try:
        response = requests.get(api_url, params=params, timeout=5)
        response.raise_for_status()
        
        res_json = response.json()
        if res_json.get('code') == 200 or res_json.get('success'):
            logger.info(f"  âœ… CMS é€šçŸ¥æˆåŠŸ: {res_json.get('msg', 'OK')}")
        else:
            logger.warning(f"  âš ï¸ CMS é€šçŸ¥è¿”å›å¼‚å¸¸: {res_json}")

    except Exception as e:
        logger.warning(f"  âš ï¸ CMS é€šçŸ¥å‘é€å¤±è´¥: {e}")
        # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œä»¥å…å½±å“ä¸»æµç¨‹

def _standardize_115_file(client, file_item, save_cid, raw_title, tmdb_id, media_type='movie'):
    """
    ä¿®å¤ç‰ˆï¼šå¯¹ 115 æ–°å…¥åº“çš„æ–‡ä»¶/æ–‡ä»¶å¤¹è¿›è¡Œæ ‡å‡†åŒ–é‡å‘½å
    """
    try:
        # ==================================================
        # 1. è·å–å®˜æ–¹å…ƒæ•°æ® (TMDb) - ä¿æŒåŸé€»è¾‘
        # ==================================================
        final_title = raw_title
        final_year = None
        
        try:
            tmdb_api_key = config_manager.APP_CONFIG.get(constants.CONFIG_OPTION_TMDB_API_KEY)
            if tmdb_api_key and tmdb_id:
                details = None
                if media_type == 'tv':
                    details = tmdb.get_tv_details(tmdb_id, tmdb_api_key)
                    if details:
                        final_title = details.get('name')
                        first_air_date = details.get('first_air_date')
                        if first_air_date: final_year = first_air_date[:4]
                else:
                    details = tmdb.get_movie_details(tmdb_id, tmdb_api_key)
                    if details:
                        final_title = details.get('title')
                        release_date = details.get('release_date')
                        if release_date: final_year = release_date[:4]
        except Exception as e:
            logger.warning(f"  âš ï¸ [æ•´ç†] TMDb è·å–å¤±è´¥: {e}")

        if not final_year:
            match = re.search(r'[(ï¼ˆ](\d{4})[)ï¼‰]', raw_title)
            if match: final_year = match.group(1)
        
        safe_title = re.sub(r'[\\/:*?"<>|]', '', final_title).strip()
        std_name = f"{safe_title} ({final_year}) {{tmdb-{tmdb_id}}}" if final_year else f"{safe_title} {{tmdb-{tmdb_id}}}"

        # ==================================================
        # 2. æ ¸å¿ƒä¿®å¤ï¼šåŒºåˆ† æ–‡ä»¶å¤¹é‡å‘½å ä¸ å•æ–‡ä»¶å½’æ¡£
        # ==================================================
        # 115 æ–‡ä»¶å¤¹æ ‡è¯†ï¼šico == 'folder' æˆ–è€…æ²¡æœ‰ fid (åªæœ‰ cid)
        is_directory = (file_item.get('ico') == 'folder') or (not file_item.get('fid'))
        current_name = file_item.get('n')

        if current_name == std_name:
            logger.info(f"  âœ… [æ•´ç†] åç§°å·²ç¬¦åˆæ ‡å‡†ï¼Œè·³è¿‡æ“ä½œã€‚")
            return

        if is_directory:
            folder_id = file_item.get('cid')
            logger.info(f"  ğŸ› ï¸ [æ•´ç†] è¯†åˆ«ä¸ºæ–‡ä»¶å¤¹ï¼Œæ‰§è¡Œé‡å‘½å: {current_name} -> {std_name}")
            
            # ä¿®å¤ï¼šå°†ä¸¤ä¸ªå‚æ•°å°è£…æˆä¸€ä¸ªå…ƒç»„ä¼ å…¥
            rename_res = client.fs_rename((folder_id, std_name)) 
            
            if isinstance(rename_res, dict) and rename_res.get('state'):
                logger.info(f"  âœ… [æ•´ç†] æ–‡ä»¶å¤¹é‡å‘½åæˆåŠŸ")
            else:
                logger.warning(f"  âš ï¸ [æ•´ç†] é‡å‘½åå¤±è´¥: {rename_res}")
        
        else:
            # === æƒ…å†µ B: å•æ–‡ä»¶å½’æ¡£ ===
            file_id = file_item.get('fid')
            logger.info(f"  ğŸ› ï¸ [æ•´ç†] è¯†åˆ«ä¸ºå•æ–‡ä»¶ï¼Œæ­£åœ¨å½’æ¡£è‡³ç›®å½•: {std_name}")
            
            # æ£€æŸ¥ç›®æ ‡æ–‡ä»¶å¤¹æ˜¯å¦å­˜åœ¨
            target_dir_cid = None
            # è¿™é‡Œçš„ search é€»è¾‘è¦å°å¿ƒï¼Œ115 çš„æœç´¢è¿”å›ç»“æ„å¯èƒ½ä¸åŒ
            search_res = client.fs_files({'cid': save_cid, 'search_value': std_name})
            if isinstance(search_res, dict) and search_res.get('data'):
                for item in search_res['data']:
                    if item.get('n') == std_name and (item.get('ico') == 'folder' or not item.get('fid')):
                        target_dir_cid = item.get('cid')
                        break
            
            if not target_dir_cid:
                mkdir_res = client.fs_mkdir(std_name, save_cid)
                if isinstance(mkdir_res, dict) and mkdir_res.get('state'):
                    target_dir_cid = mkdir_res.get('cid')
                else:
                    logger.error(f"  âŒ [æ•´ç†] åˆ›å»ºæ–‡ä»¶å¤¹å¤±è´¥")
                    return 

            # æ‰§è¡Œç§»åŠ¨
            move_res = client.fs_move([file_id], target_dir_cid)
            if isinstance(move_res, dict) and move_res.get('state'):
                logger.info(f"  âœ… [æ•´ç†] å•æ–‡ä»¶å·²å½’æ¡£æˆåŠŸ")
            else:
                logger.warning(f"  âš ï¸ [æ•´ç†] ç§»åŠ¨æ–‡ä»¶å¤±è´¥")

    except Exception as e:
        # è¿™é‡Œä¼šæ•è·åˆ° "not enough values to unpack" å¹¶æ‰“å°å…·ä½“ä½ç½®
        logger.error(f"  âš ï¸ æ ‡å‡†åŒ–é‡å‘½åæµç¨‹å¼‚å¸¸: {e}", exc_info=True)

def push_to_115(resource_link, title, tmdb_id=None, media_type=None):
    """
    æ™ºèƒ½æ¨é€ï¼šæ”¯æŒ 115/115cdn/anxia è½¬å­˜ å’Œ ç£åŠ›ç¦»çº¿
    å¹¶æ‰§è¡Œ æ™ºèƒ½æ•´ç† (Smart Organize)
    """
    if P115Client is None:
        raise ImportError("æœªå®‰è£… p115 åº“")

    config = get_config()
    cookies = config.get('p115_cookies')
    
    # é»˜è®¤ä¿å­˜è·¯å¾„ (ä¸­è½¬ç«™)
    try:
        cid_val = config.get('p115_save_path_cid', 0)
        save_path_cid = int(cid_val) if cid_val else 0
    except:
        save_path_cid = 0

    if not cookies:
        raise ValueError("æœªé…ç½® 115 Cookies")

    clean_url = _clean_link(resource_link)
    logger.info(f"  âœ [NULLBR] å¾…å¤„ç†é“¾æ¥: {clean_url}")
    
    client = P115Client(cookies)
    
    # ==================================================
    # â˜…â˜…â˜… æ­¥éª¤ 1: å»ºç«‹ç›®å½•å¿«ç…§ (ç”¨äºæ•è·æ–°æ–‡ä»¶) â˜…â˜…â˜…
    # ==================================================
    existing_ids = set()
    try:
        # æ‰«æå‰50ä¸ªæ–‡ä»¶å³å¯ï¼Œé€šå¸¸æ–°æ–‡ä»¶åœ¨æœ€å‰
        files_res = client.fs_files({'cid': save_path_cid, 'limit': 50, 'o': 'user_ptime', 'asc': 0})
        if files_res.get('data'):
            for item in files_res['data']:
                item_id = item.get('fid') or item.get('cid') 
                if item_id: existing_ids.add(str(item_id))
    except Exception as e:
        logger.warning(f"  âš ï¸ è·å–ç›®å½•å¿«ç…§å¤±è´¥: {e}")

    # ==================================================
    # â˜…â˜…â˜… æ­¥éª¤ 2: æ‰§è¡Œä»»åŠ¡ (è½¬å­˜ æˆ– ç¦»çº¿) â˜…â˜…â˜…
    # ==================================================
    # ... (è¿™éƒ¨åˆ†ä»£ç ä¿æŒä¸å˜ï¼Œè´Ÿè´£è°ƒç”¨ 115 API æ·»åŠ ä»»åŠ¡) ...
    target_domains = ['115.com', '115cdn.com', 'anxia.com']
    is_115_share = any(d in clean_url for d in target_domains) and ('magnet' not in clean_url)
    task_success = False
    
    try:
        if is_115_share:
            # ... (115 åˆ†äº«è½¬å­˜é€»è¾‘ï¼Œä¿æŒä¸å˜) ...
            logger.info(f"  âœ [NULLBR] è¯†åˆ«ä¸º 115 è½¬å­˜ä»»åŠ¡ -> CID: {save_path_cid}")
            share_code = None
            match = re.search(r'/s/([a-z0-9]+)', clean_url)
            if match: share_code = match.group(1)
            if not share_code: raise Exception("æ— æ³•æå–åˆ†äº«ç ")
            receive_code = ''
            pwd_match = re.search(r'password=([a-z0-9]+)', clean_url)
            if pwd_match: receive_code = pwd_match.group(1)
            
            resp = {} 
            if hasattr(client, 'fs_share_import_to_dir'):
                    resp = client.fs_share_import_to_dir(share_code, receive_code, save_path_cid)
            elif hasattr(client, 'fs_share_import'):
                resp = client.fs_share_import(share_code, receive_code, save_path_cid)
            elif hasattr(client, 'share_import'):
                resp = client.share_import(share_code, receive_code, save_path_cid)
            else:
                api_url = "https://webapi.115.com/share/receive"
                payload = {'share_code': share_code, 'receive_code': receive_code, 'cid': save_path_cid}
                r = client.request(api_url, method='POST', data=payload)
                resp = r.json() if hasattr(r, 'json') else r

            if resp and resp.get('state'):
                logger.info(f"  âœ… 115 è½¬å­˜è¯·æ±‚æˆåŠŸ")
                task_success = True
            else:
                err = resp.get('error_msg') or resp.get('msg') or str(resp)
                raise Exception(f"è½¬å­˜å¤±è´¥: {err}")
        else:
            # ... (ç£åŠ›ç¦»çº¿é€»è¾‘ï¼Œä¿æŒä¸å˜) ...
            logger.info(f"  âœ [NULLBR] è¯†åˆ«ä¸ºç£åŠ›/ç¦»çº¿ä»»åŠ¡ -> CID: {save_path_cid}")
            payload = {'url[0]': clean_url, 'wp_path_id': save_path_cid}
            resp = client.offline_add_urls(payload)
            if resp.get('state'):
                task_success = True
                logger.info(f"  âœ [NULLBR] ä»»åŠ¡å·²æäº¤ï¼Œç­‰å¾…æ–‡ä»¶ç”Ÿæˆ...")
            else:
                err = resp.get('error_msg') or resp.get('msg') or 'æœªçŸ¥é”™è¯¯'
                if 'å·²å­˜åœ¨' in str(err):
                    task_success = True
                    logger.info(f"  âœ… ä»»åŠ¡å·²å­˜åœ¨")
                else:
                    raise Exception(f"ç¦»çº¿å¤±è´¥: {err}")
    except Exception as e:
        raise e

    # ==================================================
    # â˜…â˜…â˜… æ­¥éª¤ 3: æ‰«ææ–°æ–‡ä»¶å¹¶æ‰§è¡Œæ™ºèƒ½æ•´ç† â˜…â˜…â˜…
    # ==================================================
    if task_success:
        # è½®è¯¢æŸ¥æ‰¾æ–°æ–‡ä»¶
        max_retries = 8 # ç¨å¾®å¢åŠ é‡è¯•æ¬¡æ•°
        found_item = None
        
        for i in range(max_retries):
            time.sleep(3)
            try:
                check_res = client.fs_files({'cid': save_path_cid, 'limit': 50, 'o': 'user_ptime', 'asc': 0})
                if check_res.get('data'):
                    for item in check_res['data']:
                        current_id = item.get('fid') or item.get('cid')
                        if current_id and (str(current_id) not in existing_ids):
                            found_item = item
                            break
                if found_item:
                    break
            except Exception as e:
                logger.debug(f"è½®è¯¢å‡ºé”™: {e}")
        
        if found_item:
            item_name = found_item.get('n', 'æœªçŸ¥')
            logger.info(f"  âœ… æ•è·åˆ°æ–°å…¥åº“é¡¹ç›®: {item_name}")
            
            # â˜…â˜…â˜… æ ¸å¿ƒä¿®æ”¹ï¼šè°ƒç”¨æ™ºèƒ½æ•´ç† â˜…â˜…â˜…
            if tmdb_id:
                try:
                    # æ£€æŸ¥æ˜¯å¦å¼€å¯äº†æ•´ç†åŠŸèƒ½
                    enable_organize = config.get('enable_smart_organize', False)
                    
                    if enable_organize:
                        logger.info("  ğŸ§  [æ•´ç†] æ™ºèƒ½æ•´ç†å·²å¼€å¯ï¼Œå¼€å§‹åˆ†æ...")
                        organizer = SmartOrganizer(client, tmdb_id, media_type, title)
                        target_cid = organizer.get_target_cid()
                        
                        # æ— è®ºæ˜¯å¦å‘½ä¸­è§„åˆ™ï¼Œåªè¦å¼€å¯äº†æ•´ç†ï¼Œå°±æ‰§è¡Œé‡å‘½å
                        # å¦‚æœæ²¡å‘½ä¸­è§„åˆ™ï¼Œtarget_cid ä¸º Noneï¼Œåˆ™åªé‡å‘½åä¸ç§»åŠ¨
                        organizer.execute(found_item, target_cid)
                    else:
                        # æ—§é€»è¾‘ï¼šä»…ç®€å•é‡å‘½å
                        _standardize_115_file(client, found_item, save_path_cid, title, tmdb_id, media_type)
                        
                except Exception as e:
                    logger.error(f"  âŒ [æ•´ç†] æ™ºèƒ½æ•´ç†æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
            else:
                logger.debug("  âš ï¸ æœªæä¾› TMDb IDï¼Œè·³è¿‡æ•´ç†")
            
            return True
        else:
            if is_115_share:
                logger.warning("  âš ï¸ è½¬å­˜æ˜¾ç¤ºæˆåŠŸä½†æœªæ•è·åˆ°æ–°æ–‡ä»¶ID (å¯èƒ½æ–‡ä»¶å·²å­˜åœ¨)")
                return True
            else:
                logger.warning("  âŒ ç¦»çº¿ä»»åŠ¡è¶…æ—¶ï¼Œæœªåœ¨ç›®å½•å‘ç°æ–°æ–‡ä»¶ (æ­»é“¾æˆ–ä¸‹è½½è¿‡æ…¢)")
                # ç£åŠ›é“¾å¯èƒ½éœ€è¦å¾ˆä¹…ï¼Œè¿™é‡Œä¸æŠ¥é”™ï¼Œåªæ˜¯æ— æ³•æ‰§è¡Œæ•´ç†
                return True

    return False

def get_115_account_info():
    """
    æç®€çŠ¶æ€æ£€æŸ¥ï¼šåªéªŒè¯ Cookie æ˜¯å¦æœ‰æ•ˆï¼Œä¸è·å–ä»»ä½•è¯¦æƒ…
    """
    if P115Client is None:
        raise Exception("æœªå®‰è£… p115client")
        
    config = get_config()
    cookies = config.get('p115_cookies')
    
    if not cookies:
        raise Exception("æœªé…ç½® Cookies")
        
    try:
        client = P115Client(cookies)
        
        # å°è¯•åˆ—å‡º 1 ä¸ªæ–‡ä»¶ï¼Œè¿™æ˜¯éªŒè¯ Cookie æœ€å¿«æœ€å‡†çš„æ–¹æ³•
        resp = client.fs_files({'limit': 1})
        
        if not resp.get('state'):
            raise Exception("Cookie å·²å¤±æ•ˆ")
            
        # åªè¦æ²¡æŠ¥é”™ï¼Œå°±æ˜¯æœ‰æ•ˆ
        return {
            "valid": True,
            "msg": "Cookie çŠ¶æ€æ­£å¸¸ï¼Œå¯æ­£å¸¸æ¨é€"
        }

    except Exception as e:
        raise Exception("Cookie æ— æ•ˆæˆ–ç½‘ç»œä¸é€š")

def handle_push_request(link, title, tmdb_id=None, media_type=None):
    """
    ç»Ÿä¸€æ¨é€å…¥å£
    """
    # 1. æ¨é€åˆ° 115 (ä¼ é€’ ID ä»¥ä¾¿é‡å‘½å)
    push_to_115(link, title, tmdb_id, media_type)
    
    # 2. 115 æˆåŠŸåï¼Œé€šçŸ¥ CMS æ•´ç†
    notify_cms_scan()
    
    return True

def auto_download_best_resource(tmdb_id, media_type, title, season_number=None, episode_number=None):
    """
    [è‡ªåŠ¨ä»»åŠ¡ä¸“ç”¨] æœç´¢å¹¶ä¸‹è½½æœ€ä½³èµ„æº
    :param season_number: å­£å· (ä»… media_type='tv' æ—¶æœ‰æ•ˆ)
    """
    try:
        config = get_config()
        if not config.get('api_key'):
            logger.warning("NULLBR æœªé…ç½® API Keyï¼Œæ— æ³•æ‰§è¡Œè‡ªåŠ¨å…œåº•ã€‚")
            return False

        priority_sources = ['115', 'magnet', 'ed2k']
        user_enabled = config.get('enabled_sources', priority_sources)
        
        # æ„é€ æ—¥å¿—æ ‡é¢˜
        log_title = title
        if media_type == 'tv' and season_number:
            log_title = f"ã€Š{title}ã€‹ç¬¬ {season_number} å­£"

        logger.info(f"  âœ [NULLBR] å¼€å§‹æœç´¢èµ„æº: {log_title} (ID: {tmdb_id})")

        for source in priority_sources:
            if source not in user_enabled: continue
            if media_type == 'tv' and source == 'ed2k': continue

            resources = fetch_resource_list(tmdb_id, media_type, specific_source=source, season_number=season_number, episode_number=episode_number)
            
            if not resources:
                continue

            logger.info(f"  âœ [{source.upper()}] æ‰¾åˆ° {len(resources)} ä¸ªèµ„æºï¼Œå¼€å§‹å°è¯•æ¨é€...")

            for index, res in enumerate(resources):
                try:
                    logger.info(f"  ğŸ‘‰ å°è¯•ç¬¬ {index + 1} ä¸ªèµ„æº: {res['title']}")
                    
                    # è°ƒç”¨ç»Ÿä¸€æ¨é€å…¥å£ (115 -> CMS Notify)
                    handle_push_request(res['link'], title, tmdb_id, media_type)
                    
                    logger.info(f"  âœ… èµ„æºæ¨é€æˆåŠŸï¼Œåœæ­¢åç»­å°è¯•ã€‚")
                    return True
                    
                except Exception as e:
                    logger.warning(f"  âŒ ç¬¬ {index + 1} ä¸ªèµ„æºæ¨é€å¤±è´¥: {e}")
                    logger.info("  ğŸ”„ æ­£åœ¨å°è¯•ä¸‹ä¸€ä¸ªèµ„æº...")
                    continue
            
            logger.info(f"  âš ï¸ [{source.upper()}] æ‰€æœ‰èµ„æºå‡å°è¯•å¤±è´¥ï¼Œåˆ‡æ¢ä¸‹ä¸€æº...")

        logger.info(f"  âŒ æ‰€æœ‰æºçš„æ‰€æœ‰èµ„æºå‡å°è¯•å¤±è´¥: {log_title}")
        return False

    except Exception as e:
        logger.error(f"  âœ NULLBR æœç´¢å¤±è´¥: {e}")
        return False