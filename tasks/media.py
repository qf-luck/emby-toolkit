# tasks/media.py
# æ ¸å¿ƒåª’ä½“å¤„ç†ã€å…ƒæ•°æ®ã€èµ„äº§åŒæ­¥ç­‰

import time
import json
import gc
import os
import re
import logging
from typing import List, Optional
import concurrent.futures
from collections import defaultdict
from gevent import spawn_later
# å¯¼å…¥éœ€è¦çš„åº•å±‚æ¨¡å—å’Œå…±äº«å®ä¾‹
import task_manager
import utils
import constants
import handler.tmdb as tmdb
import handler.emby as emby
import handler.telegram as telegram
from database import connection, settings_db, media_db, queries_db
from .helpers import parse_full_asset_details, reconstruct_metadata_from_db, translate_tmdb_metadata_recursively
from extensions import UPDATING_METADATA

logger = logging.getLogger(__name__)

# â˜…â˜…â˜… ä¸­æ–‡åŒ–è§’è‰²å â˜…â˜…â˜…
def task_role_translation(processor, force_full_update: bool = False):
    """
    æ ¹æ®ä¼ å…¥çš„ force_full_update å‚æ•°ï¼Œå†³å®šæ˜¯æ‰§è¡Œæ ‡å‡†æ‰«æè¿˜æ˜¯æ·±åº¦æ›´æ–°ã€‚
    """
    actor = processor.config.get(constants.CONFIG_OPTION_AI_TRANSLATE_ACTOR_ROLE)

    if not actor:
        logger.info("  ğŸš« AIç¿»è¯‘åŠŸèƒ½æœªå¯ç”¨ï¼Œè·³è¿‡ä»»åŠ¡ã€‚")
        return

    # 1. æ ¹æ®å‚æ•°å†³å®šæ—¥å¿—ä¿¡æ¯
    if force_full_update:
        logger.info("  âœ å³å°†æ‰§è¡Œæ·±åº¦æ¨¡å¼ï¼Œå°†å¤„ç†æ‰€æœ‰åª’ä½“é¡¹å¹¶ä»TMDbè·å–æœ€æ–°æ•°æ®...")
    else:
        logger.info("  âœ å³å°†æ‰§è¡Œå¿«é€Ÿæ¨¡å¼ï¼Œå°†è·³è¿‡å·²å¤„ç†é¡¹...")


    # 3. è°ƒç”¨æ ¸å¿ƒå¤„ç†å‡½æ•°ï¼Œå¹¶å°† force_full_update å‚æ•°é€ä¼ ä¸‹å»
    processor.process_full_library(
        update_status_callback=task_manager.update_status_from_thread,
        force_full_update=force_full_update 
    )

# --- ä½¿ç”¨æ‰‹åŠ¨ç¼–è¾‘çš„ç»“æœå¤„ç†åª’ä½“é¡¹ ---
def task_manual_update(processor, item_id: str, manual_cast_list: list, item_name: str):
    """ä»»åŠ¡ï¼šä½¿ç”¨æ‰‹åŠ¨ç¼–è¾‘çš„ç»“æœå¤„ç†åª’ä½“é¡¹"""
    processor.process_item_with_manual_cast(
        item_id=item_id,
        manual_cast_list=manual_cast_list,
        item_name=item_name
    )

def task_sync_images(processor, item_id: str, update_description: str, sync_timestamp_iso: str):
    """
    ä»»åŠ¡ï¼šä¸ºå•ä¸ªåª’ä½“é¡¹åŒæ­¥å›¾ç‰‡å’Œå…ƒæ•°æ®æ–‡ä»¶åˆ°æœ¬åœ° override ç›®å½•ã€‚
    """
    logger.trace(f"ä»»åŠ¡å¼€å§‹ï¼šå›¾ç‰‡å¤‡ä»½ for ID: {item_id} (åŸå› : {update_description})")
    try:
        # 1. æ ¹æ® item_id è·å–å®Œæ•´çš„åª’ä½“è¯¦æƒ…
        item_details = emby.get_emby_item_details(
            item_id, 
            processor.emby_url, 
            processor.emby_api_key, 
            processor.emby_user_id
        )
        if not item_details:
            logger.error(f"ä»»åŠ¡å¤±è´¥ï¼šæ— æ³•è·å– ID: {item_id} çš„åª’ä½“è¯¦æƒ…ï¼Œè·³è¿‡å›¾ç‰‡å¤‡ä»½ã€‚")
            return

        # 2. ä½¿ç”¨è·å–åˆ°çš„ item_details å­—å…¸æ¥è°ƒç”¨
        processor.sync_item_images(
            item_details=item_details, 
            update_description=update_description
            # episode_ids_to_sync å‚æ•°è¿™é‡Œä¸éœ€è¦ï¼Œsync_item_images ä¼šè‡ªå·±å¤„ç†
        )

        logger.trace(f"ä»»åŠ¡æˆåŠŸï¼šå›¾ç‰‡å¤‡ä»½ for ID: {item_id}")
    except Exception as e:
        logger.error(f"ä»»åŠ¡å¤±è´¥ï¼šå›¾ç‰‡å¤‡ä»½ for ID: {item_id} æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        raise

def task_sync_all_metadata(processor, item_id: str, item_name: str):
    """
    ã€ä»»åŠ¡ï¼šå…¨èƒ½å…ƒæ•°æ®åŒæ­¥å™¨ã€‚
    å½“æ”¶åˆ° metadata.update Webhook æ—¶ï¼Œæ­¤ä»»åŠ¡ä¼šï¼š
    1. ä» Emby è·å–æœ€æ–°æ•°æ®ã€‚
    2. å°†æ›´æ–°æŒä¹…åŒ–åˆ° override è¦†ç›–ç¼“å­˜æ–‡ä»¶ã€‚
    3. å°†æ›´æ–°åŒæ­¥åˆ° media_metadata æ•°æ®åº“ç¼“å­˜ã€‚
    """
    log_prefix = f"å…¨èƒ½å…ƒæ•°æ®åŒæ­¥ for '{item_name}'"
    logger.trace(f"  âœ ä»»åŠ¡å¼€å§‹ï¼š{log_prefix}")
    try:
        # æ­¥éª¤ 1: è·å–åŒ…å«äº†ç”¨æˆ·ä¿®æ”¹çš„ã€æœ€æ–°çš„å®Œæ•´åª’ä½“è¯¦æƒ…
        item_details = emby.get_emby_item_details(
            item_id, 
            processor.emby_url, 
            processor.emby_api_key, 
            processor.emby_user_id,
            # è¯·æ±‚æ‰€æœ‰å¯èƒ½è¢«ç”¨æˆ·ä¿®æ”¹çš„å­—æ®µ
            fields="ProviderIds,Type,Name,OriginalTitle,Overview,Tagline,CommunityRating,OfficialRating,Genres,Studios,Tags,PremiereDate"
        )
        if not item_details:
            logger.error(f"  âœ {log_prefix} å¤±è´¥ï¼šæ— æ³•è·å–é¡¹ç›® {item_id} çš„æœ€æ–°è¯¦æƒ…ã€‚")
            return

        # æ­¥éª¤ 2: è°ƒç”¨æ–½å·¥é˜Ÿï¼Œæ›´æ–° override æ–‡ä»¶
        processor.sync_emby_updates_to_override_files(item_details)

        # æ­¥éª¤ 3: è°ƒç”¨å¦ä¸€ä¸ªæ–½å·¥é˜Ÿï¼Œæ›´æ–°æ•°æ®åº“ç¼“å­˜
        processor.sync_single_item_to_metadata_cache(item_id, item_name=item_name)

        logger.trace(f"  âœ ä»»åŠ¡æˆåŠŸï¼š{log_prefix}")
    except Exception as e:
        logger.error(f"  âœ ä»»åŠ¡å¤±è´¥ï¼š{log_prefix} æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        raise

def _wait_for_items_recovery(processor, item_ids: list, max_retries=60, interval=10) -> bool:
    """
    è½®è¯¢æ£€æŸ¥æŒ‡å®šçš„ä¸€ç»„ Emby ID æ˜¯å¦éƒ½å·²å…·å¤‡æœ‰æ•ˆçš„è§†é¢‘æµæ•°æ®ã€‚
    ç”¨äºç­‰å¾…ç¥åŒ»æ’ä»¶å¤„ç†ç½‘ç›˜æ–‡ä»¶ã€‚
    """
    if not item_ids:
        return True

    logger.info(f"  â³ å¼€å§‹è½®è¯¢ç›‘æ§ {len(item_ids)} ä¸ªé¡¹ç›®çš„ä¿®å¤è¿›åº¦ (æœ€å¤§ç­‰å¾… {max_retries*interval}ç§’)...")
    
    # ä½¿ç”¨é›†åˆæ¥ç®¡ç†è¿˜éœ€è¦ç­‰å¾…çš„IDï¼Œä¿®å¤ä¸€ä¸ªç§»é™¤ä¸€ä¸ª
    pending_ids = set(item_ids)
    
    for i in range(max_retries):
        if processor.is_stop_requested(): return False
        
        # å¤åˆ¶ä¸€ä»½å½“å‰å¾…å¤„ç†åˆ—è¡¨è¿›è¡Œéå†
        current_check_list = list(pending_ids)
        
        for eid in current_check_list:
            try:
                # è·å–è¯¦æƒ… (åªæŸ¥ MediaSources å³å¯)
                item_details = emby.get_emby_item_details(
                    eid, processor.emby_url, processor.emby_api_key, processor.emby_user_id,
                    fields="MediaSources"
                )
                
                is_healed = False
                if item_details:
                    media_sources = item_details.get("MediaSources", [])
                    for source in media_sources:
                        # æ’é™¤æœªåˆ†æçš„ strm
                        if not source.get("Container") and not source.get("Path", "").endswith(".strm"):
                            continue
                            
                        for stream in source.get("MediaStreams", []):
                            if stream.get("Type") == "Video":
                                w = stream.get("Width")
                                h = stream.get("Height")
                                c = stream.get("Codec")
                                # ä½¿ç”¨ä¸¥æ ¼æ ‡å‡†æ£€æŸ¥
                                valid, _ = utils.check_stream_validity(w, h, c)
                                if valid:
                                    is_healed = True
                                    break
                        if is_healed: break
                
                if is_healed:
                    logger.debug(f"    âœ” é¡¹ç›® {eid} å·²æ£€æµ‹åˆ°å®Œæ•´åª’ä½“ä¿¡æ¯ï¼Œç§»é™¤ç›‘æ§é˜Ÿåˆ—ã€‚")
                    pending_ids.remove(eid)
                    
            except Exception:
                pass # ç½‘ç»œé”™è¯¯æš‚æ—¶å¿½ç•¥ï¼Œä¸‹æ¬¡é‡è¯•
        
        if not pending_ids:
            logger.info(f"  âœ… æ‰€æœ‰ç›®æ ‡é¡¹ç›®åª’ä½“ä¿¡æ¯å‡å·²æå–å®Œæˆ (è€—æ—¶ {i*interval}ç§’)ï¼")
            return True
            
        if i % 2 == 0: # æ¯20ç§’æ‰“å°ä¸€æ¬¡è¿›åº¦
            logger.info(f"  â³ ç­‰å¾…ç¥åŒ»æå–åª’ä½“ä¿¡æ¯ä¸­... å‰©ä½™ {len(pending_ids)}/{len(item_ids)} ä¸ªé¡¹ç›® (è½®è¯¢ {i+1}/{max_retries})")
            
        time.sleep(interval)

    logger.warning(f"  âš ï¸ ç­‰å¾…è¶…æ—¶ï¼ä»æœ‰ {len(pending_ids)} ä¸ªé¡¹ç›®æœªè·å–åˆ°å®Œæ•´ä¿¡æ¯ï¼Œå°†å¼ºåˆ¶ç»§ç»­å¤„ç†ã€‚")
    return False

# --- é‡æ–°å¤„ç†å•ä¸ªé¡¹ç›® ---
def task_reprocess_single_item(processor, item_id: str, item_name_for_ui: str, failure_reason: Optional[str] = None):
    """
    é‡æ–°å¤„ç†å•ä¸ªé¡¹ç›®çš„åå°ä»»åŠ¡ã€‚
    æ–°å¢ failure_reason å‚æ•°ï¼šç”¨äºåˆ¤æ–­æ˜¯å¦éœ€è¦è§¦å‘ç¥åŒ»æ’ä»¶ã€‚
    """
    logger.trace(f"  âœ åå°ä»»åŠ¡å¼€å§‹æ‰§è¡Œ ({item_name_for_ui})")
    
    try:
        task_manager.update_status_from_thread(0, f"æ­£åœ¨å¤„ç†: {item_name_for_ui}")
        
        # â˜…â˜…â˜… æ–°å¢é€»è¾‘ï¼šåˆ¤æ–­æ˜¯å¦éœ€è¦æ‰§è¡Œç¥åŒ»ä¿®å¤æµç¨‹ â˜…â˜…â˜…
        # é»˜è®¤éœ€è¦ä¿®å¤(True)ï¼Œé™¤éæ˜ç¡®æä¾›äº†åŸå› ä¸”åŸå› ä¸æ˜¯"ç¼ºå¤±åª’ä½“ä¿¡æ¯"
        need_media_info_healing = True
        
        if failure_reason:
            if "ç¼ºå¤±åª’ä½“ä¿¡æ¯" not in failure_reason:
                need_media_info_healing = False
                logger.info(f"  âœ å¤±è´¥åŸå› ('{failure_reason}')ä¸åª’ä½“ä¿¡æ¯æ— å…³ï¼Œè·³è¿‡ç¥åŒ»æå–æ­¥éª¤ã€‚")
            else:
                logger.info(f"  âœ æ£€æµ‹åˆ°åª’ä½“ä¿¡æ¯ç¼ºå¤±ï¼Œå‡†å¤‡è§¦å‘ç¥åŒ»æå–æµç¨‹ã€‚")

        if need_media_info_healing:
            try:
                item_basic = emby.get_emby_item_details(
                    item_id, processor.emby_url, processor.emby_api_key, processor.emby_user_id,
                    fields="Type,ProviderIds"
                )
                
                if item_basic:
                    item_type = item_basic.get('Type')
                    tmdb_id = item_basic.get('ProviderIds', {}).get('Tmdb')
                    
                    ids_to_heal = []
                    
                    # A. ç¡®å®šéœ€è¦æ²»ç–—çš„ç›®æ ‡ ID åˆ—è¡¨
                    if item_type == 'Movie':
                        ids_to_heal.append(item_id)
                    elif item_type == 'Series' and tmdb_id:
                        logger.info(f"  âœ æ­£åœ¨æ£€æŸ¥å‰§é›† '{item_name_for_ui}' ä¸‹çš„å¼‚å¸¸åˆ†é›†...")
                        bad_episode_ids = media_db.get_bad_episode_emby_ids(str(tmdb_id))
                        if bad_episode_ids:
                            logger.info(f"  âœ å‘ç° {len(bad_episode_ids)} ä¸ªåˆ†é›†ç¼ºå¤±åª’ä½“ä¿¡æ¯ã€‚")
                            ids_to_heal.extend(bad_episode_ids)
                        else:
                            logger.trace(f"  âœ æœªå‘ç°æ˜æ˜¾çš„ååˆ†é›†ï¼Œå°†è·³è¿‡è§¦å‘æ­¥éª¤ã€‚")

                    # B. æ‰§è¡Œæ²»ç–—ä¸ç­‰å¾…
                    if ids_to_heal:
                        # 1. è§¦å‘
                        task_manager.update_status_from_thread(10, f"æ­£åœ¨è§¦å‘ç¥åŒ»æ’ä»¶é‡æ–°æå– {len(ids_to_heal)} ä¸ªæ–‡ä»¶çš„åª’ä½“ä¿¡æ¯...")
                        for eid in ids_to_heal:
                            emby.trigger_media_info_refresh(
                                eid, processor.emby_url, processor.emby_api_key, processor.emby_user_id
                            )
                            time.sleep(0.2) # ç¨å¾®é—´éš”
                        
                        # 2. è½®è¯¢ç­‰å¾… (å…³é”®ä¿®æ”¹)
                        task_manager.update_status_from_thread(20, f"ç­‰å¾…åª’ä½“ä¿¡æ¯æå– (æœ€é•¿10åˆ†é’Ÿ)...")
                        _wait_for_items_recovery(processor, ids_to_heal, max_retries=60, interval=10)
                        
            except Exception as e_heal:
                logger.warning(f"  âš ï¸ æµç¨‹å‡ºç°å°æ’æ›² (ä¸å½±å“åç»­é‡æ‰«): {e_heal}")
        else:
            task_manager.update_status_from_thread(10, "è·³è¿‡åª’ä½“ä¿¡æ¯æå–ï¼Œç›´æ¥å¼€å§‹åˆ®å‰Š...")

        # 3. æ‰§è¡Œæ ‡å‡†å¤„ç†æµç¨‹ (éªŒæ”¶æˆæœ)
        task_manager.update_status_from_thread(50, f"æ­£åœ¨é‡æ–°åˆ®å‰Šå…ƒæ•°æ®: {item_name_for_ui}")
        
        processor.process_single_item(
            item_id, 
            force_full_update=True
        )
        
        logger.trace(f"  âœ åå°ä»»åŠ¡å®Œæˆ ({item_name_for_ui})")

    except Exception as e:
        logger.error(f"åå°ä»»åŠ¡å¤„ç† '{item_name_for_ui}' æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}", exc_info=True)
        task_manager.update_status_from_thread(-1, f"å¤„ç†å¤±è´¥: {item_name_for_ui}")

# --- é‡æ–°å¤„ç†æ‰€æœ‰å¾…å¤æ ¸é¡¹ ---
def task_reprocess_all_review_items(processor):
    """
    ã€å·²å‡çº§ã€‘åå°ä»»åŠ¡ï¼šéå†æ‰€æœ‰å¾…å¤æ ¸é¡¹å¹¶é€ä¸€ä»¥â€œå¼ºåˆ¶åœ¨çº¿è·å–â€æ¨¡å¼é‡æ–°å¤„ç†ã€‚
    """
    logger.trace("--- å¼€å§‹æ‰§è¡Œâ€œé‡æ–°å¤„ç†æ‰€æœ‰å¾…å¤æ ¸é¡¹â€ä»»åŠ¡ [å¼ºåˆ¶åœ¨çº¿è·å–æ¨¡å¼] ---")
    try:
        # +++ æ ¸å¿ƒä¿®æ”¹ 1ï¼šåŒæ—¶æŸ¥è¯¢ item_id, item_name å’Œ reason +++
        with connection.get_db_connection() as conn:
            cursor = conn.cursor()
            # ä» failed_log ä¸­è·å– ID, Name å’Œ Reason
            cursor.execute("SELECT item_id, item_name, reason FROM failed_log")
            # å°†ç»“æœä¿å­˜ä¸ºä¸€ä¸ªå­—å…¸åˆ—è¡¨ï¼Œæ–¹ä¾¿åç»­ä½¿ç”¨
            all_items = [{'id': row['item_id'], 'name': row['item_name'], 'reason': row['reason']} for row in cursor.fetchall()]
        
        total = len(all_items)
        if total == 0:
            logger.info("å¾…å¤æ ¸åˆ—è¡¨ä¸­æ²¡æœ‰é¡¹ç›®ï¼Œä»»åŠ¡ç»“æŸã€‚")
            task_manager.update_status_from_thread(100, "å¾…å¤æ ¸åˆ—è¡¨ä¸ºç©ºã€‚")
            return

        logger.info(f"å…±æ‰¾åˆ° {total} ä¸ªå¾…å¤æ ¸é¡¹éœ€è¦ä»¥â€œå¼ºåˆ¶åœ¨çº¿è·å–â€æ¨¡å¼é‡æ–°å¤„ç†ã€‚")

        # +++ æ ¸å¿ƒä¿®æ”¹ 2ï¼šåœ¨å¾ªç¯ä¸­è§£åŒ… item_id, item_name å’Œ reason +++
        for i, item in enumerate(all_items):
            if processor.is_stop_requested():
                logger.info("  ğŸš« ä»»åŠ¡è¢«ä¸­æ­¢ã€‚")
                break
            
            item_id = item['id']
            item_name = item['name'] or f"ItemID: {item_id}" # å¦‚æœåå­—ä¸ºç©ºï¼Œæä¾›ä¸€ä¸ªå¤‡ç”¨å
            failure_reason = item['reason'] # è·å–å¤±è´¥åŸå› 

            task_manager.update_status_from_thread(int((i/total)*100), f"æ­£åœ¨é‡æ–°å¤„ç† {i+1}/{total}: {item_name}")
            
            # +++ æ ¸å¿ƒä¿®æ”¹ 3ï¼šä¼ é€’ failure_reason å‚æ•° +++
            task_reprocess_single_item(processor, item_id, item_name, failure_reason=failure_reason)
            
            # æ¯ä¸ªé¡¹ç›®ä¹‹é—´ç¨ä½œåœé¡¿
            time.sleep(2) 

    except Exception as e:
        logger.error(f"é‡æ–°å¤„ç†æ‰€æœ‰å¾…å¤æ ¸é¡¹æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}", exc_info=True)
        task_manager.update_status_from_thread(-1, "ä»»åŠ¡å¤±è´¥")

# æå–æ ‡ç­¾
def extract_tag_names(item_data):
    """
    å…¼å®¹æ–°æ—§ç‰ˆ Emby API æå–æ ‡ç­¾åã€‚
    """
    tags_set = set()

    # 1. å°è¯•æå– TagItems (æ–°ç‰ˆ/è¯¦ç»†ç‰ˆ)
    tag_items = item_data.get('TagItems')
    if isinstance(tag_items, list):
        for t in tag_items:
            if isinstance(t, dict):
                name = t.get('Name')
                if name:
                    tags_set.add(name)
            elif isinstance(t, str) and t:
                tags_set.add(t)
    
    # 2. å°è¯•æå– Tags (æ—§ç‰ˆ/ç®€ç•¥ç‰ˆ)
    tags = item_data.get('Tags')
    if isinstance(tags, list):
        for t in tags:
            if t:
                tags_set.add(str(t))
    
    return list(tags_set)

# --- æå–åŸå§‹åˆ†çº§æ•°æ®ï¼Œä¸è¿›è¡Œä»»ä½•è®¡ç®— ---
def _extract_and_map_tmdb_ratings(tmdb_details, item_type):
    """
    ä» TMDb è¯¦æƒ…ä¸­æå–æ‰€æœ‰å›½å®¶çš„åˆ†çº§æ•°æ®ï¼Œå¹¶æ‰§è¡Œæ™ºèƒ½æ˜ å°„ï¼ˆè¡¥å…¨ US åˆ†çº§ï¼‰ã€‚
    è¿”å›: å­—å…¸ { 'US': 'R', 'CN': 'PG-13', ... }
    """
    if not tmdb_details:
        return {}

    ratings_map = {}
    origin_country = None

    # 1. æå–åŸå§‹æ•°æ®
    if item_type == 'Movie':
        # ç”µå½±ï¼šåœ¨ release_dates ä¸­æŸ¥æ‰¾
        results = tmdb_details.get('release_dates', {}).get('results', [])
        for r in results:
            country = r.get('iso_3166_1')
            if not country: continue
            cert = None
            for release in r.get('release_dates', []):
                if release.get('certification'):
                    cert = release.get('certification')
                    break 
            if cert:
                ratings_map[country] = cert
        
        # è·å–åŸäº§å›½
        p_countries = tmdb_details.get('production_countries', [])
        if p_countries:
            origin_country = p_countries[0].get('iso_3166_1')

    elif item_type == 'Series':
        # å‰§é›†ï¼šåœ¨ content_ratings ä¸­æŸ¥æ‰¾
        results = tmdb_details.get('content_ratings', {}).get('results', [])
        for r in results:
            country = r.get('iso_3166_1')
            rating = r.get('rating')
            if country and rating:
                ratings_map[country] = rating
        
        # è·å–åŸäº§å›½
        o_countries = tmdb_details.get('origin_country', [])
        if o_countries:
            origin_country = o_countries[0]

    # æ— è®ºåŸå§‹æ•°æ®é‡Œæœ‰æ²¡æœ‰ US åˆ†çº§ï¼Œåªè¦ TMDb è¯´æ˜¯æˆäººï¼Œå°±å¿…é¡»æ˜¯ AO
    if tmdb_details.get('adult') is True:
        ratings_map['US'] = 'XXX'
        return ratings_map # æ—¢ç„¶æ˜¯æˆäººï¼Œç›´æ¥è¿”å›ï¼Œä¸éœ€è¦åç»­çš„æ˜ å°„é€»è¾‘äº†

    # 2. â˜…â˜…â˜… æ‰§è¡Œæ˜ å°„é€»è¾‘ (æ ¸å¿ƒä¿®å¤) â˜…â˜…â˜…
    # å¦‚æœå·²ç»æœ‰ US åˆ†çº§ï¼Œç›´æ¥è¿”å›ï¼Œä¸åšæ˜ å°„ï¼ˆä»¥ TMDb åŸç”Ÿ US ä¸ºå‡†ï¼Œæˆ–è€…ä½ å¯ä»¥é€‰æ‹©è¦†ç›–ï¼‰
    # è¿™é‡Œæˆ‘ä»¬é€‰æ‹©ï¼šå¦‚æœåŸç”Ÿæ²¡æœ‰ USï¼Œæˆ–è€…æˆ‘ä»¬æƒ³å¼ºåˆ¶æ£€æŸ¥æ˜ å°„ï¼Œå°±æ‰§è¡Œæ˜ å°„ã€‚
    # ä¸ºäº†ä¿é™©ï¼Œæˆ‘ä»¬æ€»æ˜¯å°è¯•è®¡ç®—æ˜ å°„å€¼ï¼Œå¦‚æœè®¡ç®—å‡ºæ¥äº†ï¼Œå°±è¡¥å…¨è¿›å»ã€‚
    
    target_us_code = None
    
    # åŠ è½½é…ç½®
    rating_mapping = settings_db.get_setting('rating_mapping') or utils.DEFAULT_RATING_MAPPING
    priority_list = settings_db.get_setting('rating_priority') or utils.DEFAULT_RATING_PRIORITY

    # æŒ‰ä¼˜å…ˆçº§æŸ¥æ‰¾
    for p_country in priority_list:
        search_country = origin_country if p_country == 'ORIGIN' else p_country
        if not search_country: continue
        
        if search_country in ratings_map:
            source_rating = ratings_map[search_country]
            
            # å°è¯•æ˜ å°„
            if isinstance(rating_mapping, dict) and search_country in rating_mapping and 'US' in rating_mapping:
                current_val = None
                # æ‰¾æºåˆ†çº§å¯¹åº”çš„ Value
                for rule in rating_mapping[search_country]:
                    if str(rule['code']).strip().upper() == str(source_rating).strip().upper():
                        current_val = rule.get('emby_value')
                        break
                
                # æ‰¾ US å¯¹åº”çš„ Code
                if current_val is not None:
                    valid_us_rules = []
                    for rule in rating_mapping['US']:
                        r_code = rule.get('code', '')
                        
                        is_tv_code = r_code.upper().startswith('TV-') or r_code.upper() == 'TV-Y7' # ç¡®ä¿æ¶µç›–æ‰€æœ‰TVæ ¼å¼
                        
                        # 1. å¦‚æœæ˜¯ç”µå½±ï¼Œè·³è¿‡ TV åˆ†çº§
                        if item_type == 'Movie' and is_tv_code:
                            continue
                            
                        # 2. å¦‚æœæ˜¯å‰§é›†ï¼Œè·³è¿‡é TV åˆ†çº§ (å¼ºåˆ¶è¦æ±‚ TV- å¼€å¤´)
                        # æ³¨æ„ï¼šUSåˆ†çº§ä¸­ï¼Œç”µè§†å‰§é€šå¸¸ä¸¥æ ¼ä½¿ç”¨ TV-Y, TV-G, TV-14 ç­‰
                        if item_type == 'Series' and not is_tv_code:
                            continue

                        valid_us_rules.append(rule)
                    
                    for rule in valid_us_rules:
                        # å°è¯•ç²¾ç¡®åŒ¹é…
                        try:
                            if int(rule.get('emby_value')) == int(current_val):
                                target_us_code = rule['code']
                                break
                        except: continue
                    
                    # å¦‚æœæ²¡ç²¾ç¡®åŒ¹é…ï¼Œå°è¯•å‘ä¸Šå…¼å®¹ (+1)
                    if not target_us_code:
                        for rule in valid_us_rules:
                            try:
                                if int(rule.get('emby_value')) == int(current_val) + 1:
                                    target_us_code = rule['code']
                                    break
                            except: continue

            if target_us_code:
                break
            # å¦‚æœæ²¡æ˜ å°„æˆåŠŸï¼Œä½†è¿™æ˜¯é«˜ä¼˜å…ˆçº§å›½å®¶ï¼Œä¸”æ²¡æœ‰ US åˆ†çº§ï¼Œä¹Ÿå¯ä»¥è€ƒè™‘ç›´æ¥ç”¨å®ƒçš„åˆ†çº§åšå…œåº•ï¼ˆè§†éœ€æ±‚è€Œå®šï¼‰
            # è¿™é‡Œæˆ‘ä»¬åªåšæ˜ å°„è¡¥å…¨

    # 3. è¡¥å…¨ US åˆ†çº§
    if target_us_code:
        # å¼ºåˆ¶è¦†ç›–/æ·»åŠ  US åˆ†çº§
        ratings_map['US'] = target_us_code

    return ratings_map

# â˜…â˜…â˜… é‡é‡çº§çš„å…ƒæ•°æ®ç¼“å­˜å¡«å……ä»»åŠ¡ (å†…å­˜ä¼˜åŒ–ç‰ˆ) â˜…â˜…â˜…
def task_populate_metadata_cache(processor, batch_size: int = 10, force_full_update: bool = False):
    """
    - é‡é‡çº§çš„å…ƒæ•°æ®ç¼“å­˜å¡«å……ä»»åŠ¡ (ç±»å‹å®‰å…¨ç‰ˆ)ã€‚
    - ä¿®å¤ï¼šå½»åº•è§£å†³ TMDb ID åœ¨ç”µå½±å’Œå‰§é›†é—´å†²çªçš„é—®é¢˜ã€‚
    - ä¿®å¤ï¼šå®Œå–„ç¦»çº¿æ£€æµ‹é€»è¾‘ï¼Œç¡®ä¿æ¶ˆå¤±çš„ç”µå½±/å‰§é›†èƒ½è¢«æ­£ç¡®æ ‡è®°ä¸ºç¦»çº¿ã€‚
    - ä¼˜åŒ–ï¼šç§»é™¤æ— ç”¨çš„ä¸­é—´æ•°æ®ç¼“å­˜ï¼Œå¤§å¹…é™ä½å†…å­˜å ç”¨ã€‚
    """
    task_name = "åŒæ­¥åª’ä½“å…ƒæ•°æ®"
    sync_mode = "æ·±åº¦åŒæ­¥ (å…¨é‡)" if force_full_update else "å¿«é€ŸåŒæ­¥ (å¢é‡)"
    logger.info(f"--- æ¨¡å¼: {sync_mode} (åˆ†æ‰¹å¤§å°: {batch_size}) ---")
    
    total_updated_count = 0
    total_offline_count = 0

    try:
        task_manager.update_status_from_thread(0, f"é˜¶æ®µ1/3: å»ºç«‹å·®å¼‚åŸºå‡† ({sync_mode})...")
        
        libs_to_process_ids = processor.config.get("libraries_to_process", [])
        if not libs_to_process_ids:
            raise ValueError("æœªåœ¨é…ç½®ä¸­æŒ‡å®šè¦å¤„ç†çš„åª’ä½“åº“ã€‚")

        # --- 1. å‡†å¤‡åŸºç¡€æ•°æ® ---
        # â˜…â˜…â˜… å†…å­˜ä¼˜åŒ– 1: æ”¹ç”¨ Set åªå­˜ IDï¼Œä¸å­˜ True/Falseï¼ŒèŠ‚çœä¸€åŠå†…å­˜ â˜…â˜…â˜…
        known_online_emby_ids = set() 
        emby_sid_to_tmdb_id = {}    # {emby_series_id: tmdb_id}
        tmdb_key_to_emby_ids = defaultdict(set) 
        
        with connection.get_db_connection() as conn:
            cursor = conn.cursor()
            
            # A. é¢„åŠ è½½æ˜ å°„
            cursor.execute("""
                SELECT tmdb_id, item_type, jsonb_array_elements_text(emby_item_ids_json) as eid 
                FROM media_metadata 
                WHERE item_type IN ('Movie', 'Series')
            """)
            for row in cursor.fetchall():
                e_id, t_id, i_type = row['eid'], row['tmdb_id'], row['item_type']
                if i_type == 'Series':
                    emby_sid_to_tmdb_id[e_id] = t_id
                if t_id:
                    tmdb_key_to_emby_ids[(t_id, i_type)].add(e_id)

            # B. è·å–åœ¨çº¿çŠ¶æ€
            if not force_full_update:
                # â˜…â˜…â˜… å†…å­˜ä¼˜åŒ– 1: åªæŸ¥è¯¢åœ¨çº¿çš„ ID â˜…â˜…â˜…
                cursor.execute("""
                    SELECT jsonb_array_elements_text(emby_item_ids_json) AS emby_id
                    FROM media_metadata 
                    WHERE in_library = TRUE
                """)
                for row in cursor.fetchall():
                    known_online_emby_ids.add(row['emby_id'])
                
                cursor.execute("""
                    SELECT COUNT(*) as total, SUM(CASE WHEN in_library THEN 1 ELSE 0 END) as online 
                    FROM media_metadata
                """)
                stat_row = cursor.fetchone()
                total_items = stat_row['total'] if stat_row else 0
                online_items = stat_row['online'] if stat_row and stat_row['online'] is not None else 0
                
                logger.info(f"  âœ æœ¬åœ°æ•°æ®åº“å…±å­˜å‚¨ {total_items} ä¸ªåª’ä½“é¡¹ (å…¶ä¸­åœ¨çº¿: {online_items})ã€‚")

        logger.info("  âœ æ­£åœ¨é¢„åŠ è½½ Emby æ–‡ä»¶å¤¹è·¯å¾„æ˜ å°„...")
        folder_map = emby.get_all_folder_mappings(processor.emby_url, processor.emby_api_key)
        logger.info(f"  âœ åŠ è½½äº† {len(folder_map)} ä¸ªæ–‡ä»¶å¤¹è·¯å¾„èŠ‚ç‚¹ã€‚")

        # --- 2. æ‰«æ Emby (æµå¼å¤„ç†) ---
        task_manager.update_status_from_thread(10, f"é˜¶æ®µ2/3: æ‰«æ Emby å¹¶è®¡ç®—å·®å¼‚...")
        
        # â˜…â˜…â˜… å†…å­˜ä¼˜åŒ– 2: å½»åº•ç§»é™¤æ— ç”¨çš„ç´¯ç§¯å­—å…¸ (top_level_items_map ç­‰) â˜…â˜…â˜…
        # è¿™äº›å­—å…¸ä¹‹å‰åªå­˜ä¸å–ï¼Œæ˜¯å¯¼è‡´çˆ†å†…å­˜çš„å…ƒå‡¶
        
        emby_id_to_lib_id = {}
        id_to_parent_map = {}
        lib_id_to_guid_map = {}
        
        try:
            import requests
            lib_resp = requests.get(f"{processor.emby_url}/Library/VirtualFolders", params={"api_key": processor.emby_api_key})
            if lib_resp.status_code == 200:
                for lib in lib_resp.json():
                    l_id = str(lib.get('ItemId'))
                    l_guid = str(lib.get('Guid'))
                    if l_id and l_guid:
                        lib_id_to_guid_map[l_id] = l_guid
        except Exception as e:
            logger.error(f"è·å–åº“ GUID æ˜ å°„å¤±è´¥: {e}")

        dirty_keys = set() 
        current_scan_emby_ids = set() 
        pending_children = [] 

        # â˜…â˜…â˜… æ–°å¢è®¡æ•°å™¨ â˜…â˜…â˜…
        scan_count = 0
        skipped_no_tmdb = 0
        skipped_other_type = 0
        skipped_clean = 0

        req_fields = "ProviderIds,Type,DateCreated,Name,OriginalTitle,PremiereDate,CommunityRating,Genres,Studios,Tags,TagItems,DateModified,OfficialRating,ProductionYear,Path,PrimaryImageAspectRatio,Overview,MediaStreams,Container,Size,SeriesId,ParentIndexNumber,IndexNumber,ParentId,RunTimeTicks,_SourceLibraryId"

        item_generator = emby.fetch_all_emby_items_generator(
            base_url=processor.emby_url, 
            api_key=processor.emby_api_key, 
            library_ids=libs_to_process_ids, 
            fields=req_fields
        )

        for item in item_generator:
            scan_count += 1
            if scan_count % 5000 == 0:
                task_manager.update_status_from_thread(10, f"æ­£åœ¨ç´¢å¼• Emby åº“ ({scan_count} å·²æ‰«æ)...")
            
            item_id = str(item.get("Id"))
            parent_id = str(item.get("ParentId"))
            if item_id and parent_id:
                id_to_parent_map[item_id] = parent_id
            
            if not item_id: 
                continue

            emby_id_to_lib_id[item_id] = item.get('_SourceLibraryId')
            
            item_type = item.get("Type")
            tmdb_id = item.get("ProviderIds", {}).get("Tmdb")

            # 1. è®°å½•æ‰€æœ‰æ‰«æåˆ°çš„ ID (ç”¨äºåå‘æ£€æµ‹ç¦»çº¿)
            if item_type in ["Movie", "Series", "Season", "Episode"]:
                current_scan_emby_ids.add(item_id)
            else:
                skipped_other_type += 1
                continue 

            # å®æ—¶æ›´æ–°æ˜ å°„
            if item_type == "Series" and tmdb_id:
                emby_sid_to_tmdb_id[item_id] = str(tmdb_id)
            
            if item_type in ["Movie", "Series"] and tmdb_id:
                tmdb_key_to_emby_ids[(str(tmdb_id), item_type)].add(item_id)

            # è·³è¿‡åˆ¤æ–­ (å·²å­˜åœ¨ä¸”åœ¨çº¿)
            is_clean = False
            if not force_full_update:
                # â˜…â˜…â˜… å†…å­˜ä¼˜åŒ– 1: ä½¿ç”¨ Set æŸ¥æ‰¾ â˜…â˜…â˜…
                if item_id in known_online_emby_ids:
                    is_clean = True
            
            if is_clean:
                skipped_clean += 1
                continue 

            # â˜…â˜…â˜… è„æ•°æ®å¤„ç† (å†…å­˜ä¼˜åŒ–ç‰ˆ) â˜…â˜…â˜…
            # ä¸å†å­˜å‚¨ item å¯¹è±¡ï¼Œåªè®°å½• ID å…³ç³»
            
            # A. é¡¶å±‚åª’ä½“
            if item_type in ["Movie", "Series"]:
                if tmdb_id:
                    composite_key = (str(tmdb_id), item_type)
                    # top_level_items_map[composite_key].append(item) # <--- åˆ é™¤è¿™è¡Œ
                    dirty_keys.add(composite_key)
                else:
                    skipped_no_tmdb += 1 

            # B. å­é›†åª’ä½“
            elif item_type in ['Season', 'Episode']:
                s_id = str(item.get('SeriesId') or item.get('ParentId')) if item_type == 'Season' else str(item.get('SeriesId'))
                
                # series_to_seasons_map/series_to_episode_map ä¹Ÿä¸éœ€è¦äº†ï¼Œå› ä¸ºåé¢ä¼šé‡æ–° fetch
                
                if s_id and s_id in emby_sid_to_tmdb_id:
                    dirty_keys.add((emby_sid_to_tmdb_id[s_id], 'Series'))
                elif s_id:
                    pending_children.append((s_id, item_type))

        # å¤„ç†å­¤å„¿åˆ†é›†
        for s_id, _ in pending_children:
            if s_id in emby_sid_to_tmdb_id:
                dirty_keys.add((emby_sid_to_tmdb_id[s_id], 'Series'))

        gc.collect()

        # --- 3. åå‘å·®å¼‚æ£€æµ‹ (åˆ é™¤) ---
        if not force_full_update:
            # known_online_emby_ids æœ¬èº«å°±æ˜¯ active_db_ids
            missing_emby_ids = known_online_emby_ids - current_scan_emby_ids
            
            del known_online_emby_ids # é‡Šæ”¾å†…å­˜
            del current_scan_emby_ids
            gc.collect()

            if missing_emby_ids:
                # ... (ä¿ç•™åŸæœ‰çš„ç¦»çº¿å¤„ç†é€»è¾‘) ...
                logger.info(f"  âœ æ£€æµ‹åˆ° {len(missing_emby_ids)} ä¸ª Emby ID å·²æ¶ˆå¤±ï¼Œæ­£åœ¨å¤„ç†ç¦»çº¿æ ‡è®°...")
                missing_ids_list = list(missing_emby_ids)
                
                with connection.get_db_connection() as conn:
                    cursor = conn.cursor()
                    cursor.execute("""
                        SELECT tmdb_id, item_type, parent_series_tmdb_id
                        FROM media_metadata 
                        WHERE in_library = TRUE 
                          AND EXISTS (
                              SELECT 1 
                              FROM jsonb_array_elements_text(emby_item_ids_json) as eid 
                              WHERE eid = ANY(%s)
                          )
                    """, (missing_ids_list,))
                    
                    rows = cursor.fetchall()
                    direct_offline_tmdb_ids = []
                    affected_parent_ids = set()
                    
                    for row in rows:
                        r_type = row['item_type']
                        r_tmdb = row['tmdb_id']
                        r_parent = row['parent_series_tmdb_id']
                        
                        if r_type in ['Movie', 'Series']:
                            direct_offline_tmdb_ids.append(r_tmdb)
                        elif r_type in ['Season', 'Episode'] and r_parent:
                            affected_parent_ids.add(r_parent)

                    if direct_offline_tmdb_ids:
                        logger.info(f"  âœ æ­£åœ¨æ ‡è®° {len(direct_offline_tmdb_ids)} ä¸ªé¡¶å±‚é¡¹ç›®ä¸ºç¦»çº¿...")
                        cursor.execute("""
                            UPDATE media_metadata
                            SET in_library = FALSE, emby_item_ids_json = '[]'::jsonb, asset_details_json = '[]'::jsonb
                            WHERE tmdb_id = ANY(%s) AND item_type IN ('Movie', 'Series')
                        """, (direct_offline_tmdb_ids,))
                        total_offline_count += cursor.rowcount
                        
                    if affected_parent_ids:
                        logger.info(f"  âœ å› åˆ†é›†æ¶ˆå¤±ï¼Œå°† {len(affected_parent_ids)} ä¸ªçˆ¶å‰§é›†åŠ å…¥åˆ·æ–°é˜Ÿåˆ—...")
                        for pid in affected_parent_ids:
                            dirty_keys.add((pid, 'Series'))
                    
                    conn.commit()

        # â˜…â˜…â˜… æ‰“å°è¯¦ç»†ç»Ÿè®¡æ—¥å¿— â˜…â˜…â˜…
        logger.info(f"  âœ Emby æ‰«æå®Œæˆï¼Œå…±æ‰«æ {scan_count} ä¸ªé¡¹ã€‚")
        logger.info(f"    - å·²å…¥åº“: {skipped_clean}")
        logger.info(f"    - å·²è·³è¿‡: {skipped_no_tmdb + skipped_other_type} (å« {skipped_no_tmdb} ä¸ªæ— ID, {skipped_other_type} ä¸ªéåª’ä½“)")
        logger.info(f"    - éœ€åŒæ­¥: {len(dirty_keys)}")

        # --- 4. ç¡®å®šå¤„ç†é˜Ÿåˆ— (æ— éœ€çŒœæµ‹ç±»å‹) ---
        items_to_process = []
        
        # ç›´æ¥éå† dirty_keysï¼Œé‡Œé¢å·²ç»åŒ…å«äº†å‡†ç¡®çš„ (ID, Type)
        for (tmdb_id, item_type) in dirty_keys:
            
            # ä½¿ç”¨å¤åˆé”®æŸ¥æ‰¾å…³è”çš„ Emby IDs
            related_emby_ids = tmdb_key_to_emby_ids.get((tmdb_id, item_type), set())
            
            if not related_emby_ids:
                continue

            items_to_process.append({
                'tmdb_id': tmdb_id,
                'emby_ids': list(related_emby_ids),
                'type': item_type, # ç›´æ¥ä½¿ç”¨ key é‡Œçš„ typeï¼Œç»å¯¹å‡†ç¡®
                'refetch': True 
            })

        total_to_process = len(items_to_process)
        task_manager.update_status_from_thread(20, f"é˜¶æ®µ3/3: æ­£åœ¨åŒæ­¥ {total_to_process} ä¸ªå˜æ›´é¡¹ç›®...")
        logger.info(f"  âœ æœ€ç»ˆå¤„ç†é˜Ÿåˆ—: {total_to_process} ä¸ªé¡¶å±‚é¡¹ç›®")

        # --- 5. æ‰¹é‡å¤„ç† ---
        processed_count = 0
        for i in range(0, total_to_process, batch_size):
            if processor.is_stop_requested(): break
            batch_tasks = items_to_process[i:i + batch_size]
            
            batch_item_groups = []

            series_to_seasons_map = defaultdict(list)
            series_to_episode_map = defaultdict(list)
            
            # é¢„å¤„ç†ï¼šæ‹‰å– refetch çš„æ•°æ®
            for task in batch_tasks:
                try:
                    target_emby_ids = task['emby_ids']
                    item_type = task['type']
                    
                    # 1. æ‰¹é‡è·å–è¿™äº› Emby ID çš„è¯¦æƒ…
                    top_items = emby.get_emby_items_by_id(
                        base_url=processor.emby_url,
                        api_key=processor.emby_api_key,
                        user_id=processor.emby_user_id,
                        item_ids=target_emby_ids,
                        fields=req_fields
                    )
                    
                    if not top_items: continue

                    # å› ä¸º get_emby_items_by_id é‡æ–°æ‹‰å–çš„æ•°æ®æ²¡æœ‰è¿™ä¸ªå­—æ®µï¼Œæˆ‘ä»¬éœ€è¦ä»ä¹‹å‰çš„æ˜ å°„ä¸­è¡¥å›å»
                    for item in top_items:
                        eid = str(item.get('Id'))
                        if eid in emby_id_to_lib_id:
                            item['_SourceLibraryId'] = emby_id_to_lib_id[eid]

                    # 2. å¦‚æœæ˜¯å‰§é›†ï¼Œè¿˜éœ€è¦æ‹‰å–æ¯ä¸ªå‰§é›†çš„å­é›†
                    if item_type == 'Series':
                        full_group = []
                        full_group.extend(top_items)
                        
                        # æ¸…ç©ºæ—§çš„å­é›†ç¼“å­˜ï¼Œé˜²æ­¢é‡å¤
                        for e_id in target_emby_ids:
                            series_to_seasons_map[e_id] = []
                            series_to_episode_map[e_id] = []
                        
                        children_gen = emby.fetch_all_emby_items_generator(
                            base_url=processor.emby_url,
                            api_key=processor.emby_api_key,
                            library_ids=target_emby_ids, 
                            fields=req_fields
                        )
                        
                        children_list = list(children_gen)
                        for child in children_list:
                            parent_series_id = str(child.get('SeriesId') or child.get('ParentId'))
                            if parent_series_id and parent_series_id in emby_id_to_lib_id:
                                real_lib_id = emby_id_to_lib_id[parent_series_id]
                                child['_SourceLibraryId'] = real_lib_id 
                        full_group.extend(children_list)
                        
                        # é‡æ–°å¡«å…… map
                        for child in children_list:
                            ct = child.get('Type')
                            pid = str(child.get('SeriesId') or child.get('ParentId'))
                            if pid:
                                if ct == 'Season': series_to_seasons_map[pid].append(child)
                                elif ct == 'Episode': series_to_episode_map[pid].append(child)
                        
                        batch_item_groups.append(full_group)
                    
                    else:
                        # ç”µå½±ç›´æ¥æ·»åŠ 
                        batch_item_groups.append(top_items)

                except Exception as e:
                    logger.error(f"å¤„ç†é¡¹ç›® {task.get('tmdb_id')} å¤±è´¥: {e}")

            # --- ä»¥ä¸‹é€»è¾‘ä¿æŒä¸å˜ (å¹¶å‘è·å– TMDB å’Œ å†™å…¥ DB) ---
            
            tmdb_details_map = {}
            def fetch_tmdb_details(item_group):
                if not item_group: return None, None
                item = item_group[0]
                t_id = item.get("ProviderIds", {}).get("Tmdb")
                i_type = item.get("Type")
                if not t_id: return None, None
                details = None
                try:
                    if i_type == 'Movie': 
                        details = tmdb.get_movie_details(t_id, processor.tmdb_api_key)
                    elif i_type == 'Series': 
                        # ä½¿ç”¨èšåˆå‡½æ•°ï¼Œå¹¶å‘è·å–æ‰€æœ‰å­£ä¿¡æ¯
                        # æ³¨æ„ï¼šå¤–å±‚å·²ç»æ˜¯å¹¶å‘äº†ï¼Œè¿™é‡Œ max_workers è®¾å°ä¸€ç‚¹ï¼ˆå¦‚ 3ï¼‰ï¼Œé˜²æ­¢ç¬é—´è¯·æ±‚è¿‡å¤šè§¦å‘ 429
                        details = tmdb.aggregate_full_series_data_from_tmdb(t_id, processor.tmdb_api_key, max_workers=2)
                except Exception: pass
                return str(t_id), details

            with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
                futures = {executor.submit(fetch_tmdb_details, grp): grp for grp in batch_item_groups}
                for future in concurrent.futures.as_completed(futures):
                    t_id_str, details = future.result()
                    if t_id_str and details: tmdb_details_map[t_id_str] = details

            # åœ¨å†™å…¥æ•°æ®åº“ä¹‹å‰ï¼Œå¯¹è·å–åˆ°çš„ TMDb æ•°æ®è¿›è¡Œç¿»è¯‘
            if processor.ai_translator and processor.config.get("ai_translate_episode_overview", False):
                for item_group in batch_item_groups:
                    if not item_group: continue
                    item = item_group[0]
                    t_id = str(item.get("ProviderIds", {}).get("Tmdb"))
                    i_type = item.get("Type")
                    
                    # è·å–åˆšæ‰ä¸‹è½½çš„æ•°æ®
                    data_to_translate = tmdb_details_map.get(t_id)
                    if data_to_translate:
                        # è°ƒç”¨ helper è¿›è¡ŒåŸåœ°ä¿®æ”¹
                        translate_tmdb_metadata_recursively(
                            item_type=i_type,
                            tmdb_data=data_to_translate,
                            ai_translator=processor.ai_translator,
                            item_name=item.get('Name', '')
                        )

            metadata_batch = []
            series_ids_processed_in_batch = set()

            for item_group in batch_item_groups:
                if not item_group: continue
                item = item_group[0]
                tmdb_id_str = str(item.get("ProviderIds", {}).get("Tmdb"))
                item_type = item.get("Type")

                full_aggregated_data = tmdb_details_map.get(tmdb_id_str)
                tmdb_details = None
                pre_fetched_episodes = {} # ç”¨äºå­˜å‚¨é¢„è·å–çš„åˆ†é›†ä¿¡æ¯

                if item_type == 'Series' and full_aggregated_data:
                    # å¦‚æœæ˜¯ Seriesï¼Œfull_aggregated_data æ˜¯ä¸€ä¸ªåŒ…å« series_details, seasons_details, episodes_details çš„å­—å…¸
                    tmdb_details = full_aggregated_data.get('series_details')
                    pre_fetched_episodes = full_aggregated_data.get('episodes_details', {})
                else:
                    # Movie æˆ–å…¶ä»–æƒ…å†µï¼Œä¿æŒåŸæ ·
                    tmdb_details = full_aggregated_data
                
                # --- 1. æ„å»ºé¡¶å±‚è®°å½• ---
                asset_details_list = []
                if item_type in ["Movie", "Series"]:
                    for v in item_group:
                        # ä»…å¤„ç†å½“å‰ç±»å‹çš„é¡¹ç›® (é˜²æ­¢ Series ç»„é‡Œæ··å…¥ Season/Episode)
                        if v.get('Type') != item_type:
                            continue
                            
                        source_lib_id = str(v.get('_SourceLibraryId'))
                        current_lib_guid = lib_id_to_guid_map.get(source_lib_id)

                        details = parse_full_asset_details(
                            v, 
                            id_to_parent_map=id_to_parent_map, 
                            library_guid=current_lib_guid
                        )
                        details['source_library_id'] = source_lib_id
                        asset_details_list.append(details)

                emby_runtime = round(item['RunTimeTicks'] / 600000000) if item.get('RunTimeTicks') else None

                # æå–å‘è¡Œæ—¥æœŸ 
                emby_date = item.get('PremiereDate') or None
                tmdb_date = None
                tmdb_last_date = None
                if tmdb_details:
                    if item_type == 'Movie': 
                        tmdb_date = tmdb_details.get('release_date')
                    elif item_type == 'Series': 
                        tmdb_date = tmdb_details.get('first_air_date')
                        tmdb_last_date = tmdb_details.get('last_air_date')
                
                final_release_date = emby_date or tmdb_date
                # æå–å…¨é‡åˆ†çº§æ•°æ®
                raw_ratings_map = _extract_and_map_tmdb_ratings(tmdb_details, item_type)
                # åºåˆ—åŒ–ä¸º JSON å­—ç¬¦ä¸²ï¼Œå‡†å¤‡å­˜å…¥æ•°æ®åº“
                rating_json_str = json.dumps(raw_ratings_map, ensure_ascii=False)
                # æ„å»º Genres æ•°æ® 
                # é»˜è®¤ä½¿ç”¨ Emby æ•°æ® (æ ¼å¼åŒ–ä¸ºå¯¹è±¡åˆ—è¡¨)
                final_genres_list = []
                for g in item.get('Genres', []):
                    name = g
                    if name in utils.GENRE_TRANSLATION_PATCH:
                        name = utils.GENRE_TRANSLATION_PATCH[name]
                    final_genres_list.append({"id": 0, "name": name})
                
                # å¦‚æœæœ‰ TMDb è¯¦æƒ…ï¼Œä¼˜å…ˆä½¿ç”¨ TMDb çš„ Genres (å¸¦ ID)
                if tmdb_details and tmdb_details.get('genres'):
                    final_genres_list = []
                    for g in tmdb_details.get('genres', []):
                        if isinstance(g, dict):
                            name = g.get('name')
                            if name in utils.GENRE_TRANSLATION_PATCH:
                                name = utils.GENRE_TRANSLATION_PATCH[name]
                            final_genres_list.append({"id": g.get('id', 0), "name": name})
                        elif isinstance(g, str):
                            name = g
                            if name in utils.GENRE_TRANSLATION_PATCH:
                                name = utils.GENRE_TRANSLATION_PATCH[name]
                            final_genres_list.append({"id": 0, "name": name})
                # 1. å¤„ç†åˆ¶ä½œå…¬å¸ & 2. å¤„ç†ç”µè§†ç½‘ 
                fmt_companies = []
                fmt_networks = []
                
                if tmdb_details:
                    raw_companies = tmdb_details.get('production_companies') or []
                    fmt_companies = [{'id': c.get('id'), 'name': c.get('name')} for c in raw_companies if c.get('name')]
                    
                    raw_networks = tmdb_details.get('networks') or []
                    fmt_networks = [{'id': n.get('id'), 'name': n.get('name')} for n in raw_networks if n.get('name')]
                top_record = {
                    "tmdb_id": tmdb_id_str, "item_type": item_type, "title": item.get('Name'),
                    "original_title": item.get('OriginalTitle'), "release_year": item.get('ProductionYear'),
                    "original_language": tmdb_details.get('original_language') if tmdb_details else None,
                    "watchlist_tmdb_status": tmdb_details.get('status') if tmdb_details else None,
                    "in_library": True, 
                    "subscription_status": "NONE",
                    "emby_item_ids_json": json.dumps(list(set(v.get('Id') for v in item_group if v.get('Id') and v.get('Type') == item_type)), ensure_ascii=False),
                    "asset_details_json": json.dumps(asset_details_list, ensure_ascii=False),
                    "rating": item.get('CommunityRating'),
                    "date_added": item.get('DateCreated') or None,
                    "release_date": final_release_date,
                    "last_air_date": tmdb_last_date,
                    "genres_json": json.dumps(final_genres_list, ensure_ascii=False),
                    "production_companies_json": json.dumps(fmt_companies, ensure_ascii=False), 
                    "networks_json": json.dumps(fmt_networks, ensure_ascii=False),
                    "tags_json": json.dumps(extract_tag_names(item), ensure_ascii=False),
                    "official_rating_json": rating_json_str,
                    "runtime_minutes": emby_runtime if (item_type == 'Movie' and emby_runtime) else tmdb_details.get('runtime') if (item_type == 'Movie' and tmdb_details) else None
                }
                if tmdb_details:
                    top_record['poster_path'] = tmdb_details.get('poster_path')
                    top_record['backdrop_path'] = tmdb_details.get('backdrop_path') 
                    top_record['homepage'] = tmdb_details.get('homepage')
                    top_record['overview'] = tmdb_details.get('overview')
                    if tmdb_details.get('vote_average') is not None:
                        top_record['rating'] = tmdb_details.get('vote_average')
                    # é‡‡é›†æ€»é›†æ•°
                    if item_type == 'Series':
                        top_record['total_episodes'] = tmdb_details.get('number_of_episodes', 0)
                    if item_type == 'Movie':
                        top_record['runtime_minutes'] = tmdb_details.get('runtime')
                    
                    directors, countries, keywords = [], [], []
                    if item_type == 'Movie':
                        credits_data = tmdb_details.get("credits", {}) or tmdb_details.get("casts", {})
                        directors = [{'id': p.get('id'), 'name': p.get('name')} for p in credits_data.get('crew', []) if p.get('job') == 'Director']
                        countries = [c.get('iso_3166_1') for c in tmdb_details.get('production_countries', []) if c.get('iso_3166_1')]
                        keywords_data = tmdb_details.get('keywords', {})
                        keyword_list = keywords_data.get('keywords', []) if isinstance(keywords_data, dict) else []
                        keywords = [{'id': k.get('id'), 'name': k.get('name')} for k in keyword_list if k.get('name')]
                    elif item_type == 'Series':
                        directors = [{'id': c.get('id'), 'name': c.get('name')} for c in tmdb_details.get('created_by', [])]
                        countries = tmdb_details.get('origin_country', [])
                        keywords_data = tmdb_details.get('keywords', {})
                        keyword_list = keywords_data.get('results', []) if isinstance(keywords_data, dict) else []
                        keywords = [{'id': k.get('id'), 'name': k.get('name')} for k in keyword_list if k.get('name')]
                    top_record['directors_json'] = json.dumps(directors, ensure_ascii=False)
                    top_record['countries_json'] = json.dumps(countries, ensure_ascii=False)
                    top_record['keywords_json'] = json.dumps(keywords, ensure_ascii=False)
                else:
                    top_record['poster_path'] = None
                    top_record['backdrop_path'] = None 
                    top_record['homepage'] = None
                    top_record['directors_json'] = '[]'; top_record['countries_json'] = '[]'; top_record['keywords_json'] = '[]'

                metadata_batch.append(top_record)

                # --- 2. å¤„ç† Series çš„å­é›† ---
                if item_type == "Series":
                    series_ids_processed_in_batch.add(tmdb_id_str)
                    
                    series_emby_ids = [str(v.get('Id')) for v in item_group if v.get('Id')]
                    my_seasons = []
                    my_episodes = []
                    for s_id in series_emby_ids:
                        my_seasons.extend(series_to_seasons_map.get(s_id, []))
                        my_episodes.extend(series_to_episode_map.get(s_id, []))
                    
                    tmdb_children_map = {}
                    processed_season_numbers = set()
                    
                    if tmdb_details and 'seasons' in tmdb_details:
                        for s_info in tmdb_details.get('seasons', []):
                            try:
                                s_num = int(s_info.get('season_number'))
                            except (ValueError, TypeError):
                                continue
                            
                            matched_emby_seasons = []
                            for s in my_seasons:
                                try:
                                    if int(s.get('IndexNumber')) == s_num:
                                        matched_emby_seasons.append(s)
                                except (ValueError, TypeError):
                                    continue
                            
                            if matched_emby_seasons:
                                processed_season_numbers.add(s_num)
                                real_season_tmdb_id = str(s_info.get('id'))
                                season_poster = s_info.get('poster_path')
                                if not season_poster and tmdb_details:
                                    season_poster = tmdb_details.get('poster_path')

                                # æå–å­£å‘è¡Œæ—¥æœŸ
                                s_release_date = s_info.get('air_date') or None
                                
                                if not s_release_date and matched_emby_seasons:
                                    s_release_date = matched_emby_seasons[0].get('PremiereDate') or None
                                
                                # æ ¸å¿ƒé€»è¾‘ï¼šå¦‚æœè¿˜æ²¡æ‰¾åˆ°ï¼Œéå†è¯¥å­£ä¸‹çš„åˆ†é›†æ‰¾æœ€æ—©çš„
                                if not s_release_date:
                                    # ç­›é€‰å‡ºå±äºå½“å‰å­£(s_num)ä¸”æœ‰æ—¥æœŸçš„åˆ†é›†
                                    ep_dates = [
                                        e.get('PremiereDate') for e in my_episodes 
                                        if e.get('ParentIndexNumber') == s_num and e.get('PremiereDate')
                                    ]
                                    if ep_dates:
                                        # å–æœ€æ—©çš„æ—¥æœŸä½œä¸ºå­£çš„å‘è¡Œæ—¥æœŸ
                                        s_release_date = min(ep_dates)
                                season_record = {
                                    "tmdb_id": real_season_tmdb_id,
                                    "item_type": "Season",
                                    "parent_series_tmdb_id": tmdb_id_str,
                                    "season_number": s_num,
                                    "title": s_info.get('name'),
                                    "overview": s_info.get('overview'),
                                    "poster_path": season_poster,
                                    "rating": s_info.get('vote_average'),
                                    "total_episodes": s_info.get('episode_count', 0),
                                    "in_library": True,
                                    "release_date": s_release_date,
                                    "subscription_status": "NONE",
                                    "emby_item_ids_json": json.dumps([s.get('Id') for s in matched_emby_seasons]),
                                    "tags_json": json.dumps(extract_tag_names(matched_emby_seasons[0]) if matched_emby_seasons else [], ensure_ascii=False),
                                    "ignore_reason": None
                                }
                                metadata_batch.append(season_record)
                                tmdb_children_map[f"S{s_num}"] = s_info

                                for key, ep_data in pre_fetched_episodes.items():
                                    # key æ ¼å¼ä¸º S1E1
                                    if key.startswith(f"S{s_num}E"):
                                        tmdb_children_map[key] = ep_data

                    # B. å…œåº•å¤„ç†
                    for s in my_seasons:
                        try:
                            s_num = int(s.get('IndexNumber'))
                        except (ValueError, TypeError):
                            continue

                        if s_num not in processed_season_numbers:
                            # å…œåº•é€»è¾‘ä¹ŸåŠ ä¸Šåˆ†é›†æ—¥æœŸæ¨æ–­ 
                            s_release_date = s.get('PremiereDate') or None
                            if not s_release_date:
                                ep_dates = [
                                    e.get('PremiereDate') for e in my_episodes 
                                    if e.get('ParentIndexNumber') == s_num and e.get('PremiereDate')
                                ]
                                if ep_dates:
                                    s_release_date = min(ep_dates)
                            fallback_season_tmdb_id = f"{tmdb_id_str}-S{s_num}"
                            season_record = {
                                "tmdb_id": fallback_season_tmdb_id,
                                "item_type": "Season",
                                "parent_series_tmdb_id": tmdb_id_str,
                                "season_number": s_num,
                                "title": s.get('Name') or f"Season {s_num}",
                                "overview": None,
                                "poster_path": tmdb_details.get('poster_path') if tmdb_details else None,
                                "in_library": True,
                                "release_date": s_release_date,
                                "subscription_status": "NONE",
                                "emby_item_ids_json": json.dumps([s.get('Id')]),
                                "tags_json": json.dumps(extract_tag_names(s), ensure_ascii=False),
                                "ignore_reason": "Local Season Only"
                            }
                            metadata_batch.append(season_record)
                            processed_season_numbers.add(s_num)

                    # C. å¤„ç†åˆ†é›†
                    ep_grouped = defaultdict(list)
                    for ep in my_episodes:
                        s_n, e_n = ep.get('ParentIndexNumber'), ep.get('IndexNumber')
                        if s_n is not None and e_n is not None:
                            ep_grouped[(s_n, e_n)].append(ep)
                    
                    for (s_n, e_n), versions in ep_grouped.items():
                        emby_ep = versions[0]
                        emby_ep_runtime = round(emby_ep['RunTimeTicks'] / 600000000) if emby_ep.get('RunTimeTicks') else None
                        lookup_key = f"S{s_n}E{e_n}"
                        tmdb_ep_info = tmdb_children_map.get(lookup_key)
                        
                        ep_asset_details_list = []
                        for v in versions:
                            details = parse_full_asset_details(v) 
                            ep_asset_details_list.append(details)

                        # æå–åˆ†é›†å‘è¡Œæ—¥æœŸ 
                        ep_release_date = emby_ep.get('PremiereDate')
                        if not ep_release_date and tmdb_ep_info:
                            ep_release_date = tmdb_ep_info.get('air_date') or None
                        child_record = {
                            "item_type": "Episode",
                            "parent_series_tmdb_id": tmdb_id_str,
                            "season_number": s_n,
                            "episode_number": e_n,
                            "in_library": True,
                            "release_date": ep_release_date,
                            "rating": emby_ep.get('CommunityRating'),
                            "emby_item_ids_json": json.dumps([v.get('Id') for v in versions]),
                            "asset_details_json": json.dumps(ep_asset_details_list, ensure_ascii=False),
                            "tags_json": json.dumps(extract_tag_names(versions[0]), ensure_ascii=False),
                            "ignore_reason": None
                        }

                        if tmdb_ep_info and tmdb_ep_info.get('id'):
                            child_record['tmdb_id'] = str(tmdb_ep_info.get('id'))
                            child_record['title'] = tmdb_ep_info.get('name')
                            child_record['overview'] = tmdb_ep_info.get('overview')
                            child_record['poster_path'] = tmdb_ep_info.get('still_path')
                            child_record['runtime_minutes'] = emby_ep_runtime if emby_ep_runtime else tmdb_ep_info.get('runtime')
                            if tmdb_ep_info.get('vote_average') is not None:
                                child_record['rating'] = tmdb_ep_info.get('vote_average')
                        else:
                            child_record['tmdb_id'] = f"{tmdb_id_str}-S{s_n}E{e_n}"
                            child_record['title'] = versions[0].get('Name')
                            child_record['overview'] = versions[0].get('Overview')
                            child_record['runtime_minutes'] = emby_ep_runtime
                        
                        metadata_batch.append(child_record)

            # 7. å†™å…¥æ•°æ®åº“ & å­é›†ç¦»çº¿å¯¹è´¦
            if metadata_batch:
                total_updated_count += len(metadata_batch)

                with connection.get_db_connection() as conn:
                    cursor = conn.cursor()
                    
                    # --- A. æ‰§è¡Œå†™å…¥ ---
                    for idx, metadata in enumerate(metadata_batch):
                        savepoint_name = f"sp_{idx}"
                        try:
                            cursor.execute(f"SAVEPOINT {savepoint_name};")
                            columns = [k for k, v in metadata.items() if v is not None]
                            values = [v for v in metadata.values() if v is not None]
                            cols_str = ', '.join(columns)
                            vals_str = ', '.join(['%s'] * len(values))
                            
                            update_clauses = []
                            current_type = metadata.get('item_type')
                        
                            for col in columns:
                                # â˜…â˜…â˜… 2. å®šä¹‰åŸºç¡€æ’é™¤åˆ—è¡¨ â˜…â˜…â˜…
                                # è¿™äº›å­—æ®µæ°¸è¿œä¸æ›´æ–°
                                exclude_cols = {'tmdb_id', 'item_type', 'subscription_sources_json', 'subscription_status'}
                                
                                # â˜…â˜…â˜… 3. åŠ¨æ€åˆ¤æ–­æ˜¯å¦æ’é™¤æ ‡é¢˜ â˜…â˜…â˜…
                                # åªæœ‰å½“ç±»å‹æ˜¯ ç”µå½±(Movie) æˆ– å‰§é›†(Series) æ—¶ï¼Œæ‰æ’é™¤ title
                                # è¿™æ · å­£(Season) å’Œ é›†(Episode) çš„æ ‡é¢˜ä¾ç„¶å¯ä»¥æ­£å¸¸åŒæ­¥æ›´æ–°
                                if current_type in ['Movie', 'Series']:
                                    exclude_cols.add('title')

                                if col in exclude_cols: 
                                    continue
                                
                                # é’ˆå¯¹ total_episodes å­—æ®µï¼Œæ£€æŸ¥é”å®šçŠ¶æ€
                                # é€»è¾‘ï¼šå¦‚æœ total_episodes_locked ä¸º TRUEï¼Œåˆ™ä¿æŒåŸå€¼ï¼›å¦åˆ™ä½¿ç”¨æ–°å€¼ (EXCLUDED.total_episodes)
                                if col == 'total_episodes':
                                    update_clauses.append(
                                        "total_episodes = CASE WHEN media_metadata.total_episodes_locked IS TRUE THEN media_metadata.total_episodes ELSE EXCLUDED.total_episodes END"
                                    )
                                else:
                                    # å…¶ä»–å­—æ®µæ­£å¸¸æ›´æ–°
                                    update_clauses.append(f"{col} = EXCLUDED.{col}")
                            
                            sql = f"""
                                INSERT INTO media_metadata ({cols_str}, last_synced_at) 
                                VALUES ({vals_str}, NOW()) 
                                ON CONFLICT (tmdb_id, item_type) 
                                DO UPDATE SET {', '.join(update_clauses)}, last_synced_at = NOW()
                            """
                            cursor.execute(sql, tuple(values))
                        except Exception as e:
                            cursor.execute(f"ROLLBACK TO SAVEPOINT {savepoint_name};")
                            logger.error(f"å†™å…¥å¤±è´¥ {metadata.get('tmdb_id')}: {e}")
                    
                    # --- B. æ‰§è¡Œå­é›†ç¦»çº¿å¯¹è´¦ ---
                    if series_ids_processed_in_batch:
                        active_child_ids = {
                            m['tmdb_id'] for m in metadata_batch 
                            if m['item_type'] in ('Season', 'Episode')
                        }
                        active_child_ids_list = list(active_child_ids)
                        
                        if active_child_ids_list:
                            cursor.execute("""
                                UPDATE media_metadata
                                SET in_library = FALSE, emby_item_ids_json = '[]'::jsonb, asset_details_json = '[]'::jsonb
                                WHERE parent_series_tmdb_id = ANY(%s)
                                  AND item_type IN ('Season', 'Episode')
                                  AND in_library = TRUE
                                  AND tmdb_id != ALL(%s)
                            """, (list(series_ids_processed_in_batch), active_child_ids_list))
                            total_offline_count += cursor.rowcount
                        else:
                            cursor.execute("""
                                UPDATE media_metadata
                                SET in_library = FALSE, emby_item_ids_json = '[]'::jsonb, asset_details_json = '[]'::jsonb
                                WHERE parent_series_tmdb_id = ANY(%s)
                                  AND item_type IN ('Season', 'Episode')
                                  AND in_library = TRUE
                            """, (list(series_ids_processed_in_batch),))
                            total_offline_count += cursor.rowcount

                    conn.commit()
            
            del batch_item_groups
            del tmdb_details_map
            del metadata_batch
            gc.collect()

            processed_count += len(batch_tasks)
            task_manager.update_status_from_thread(20 + int((processed_count / total_to_process) * 80), f"å¤„ç†è¿›åº¦ {processed_count}/{total_to_process}...")

        # 8. æ‰§è¡Œå¤§æ‰«é™¤ï¼šç‰©ç†åˆ é™¤åºŸå¼ƒçš„å†…éƒ¨ ID æ¡ç›®
        logger.info("  âœ [è‡ªåŠ¨ç»´æŠ¤] æ­£åœ¨æ¸…ç†åºŸå¼ƒçš„å†…éƒ¨IDå…œåº•è®°å½•...")
        cleaned_zombies = media_db.cleanup_offline_internal_ids()
        if cleaned_zombies > 0:
            logger.info(f"  ğŸ§¹ [å¤§æ‰«é™¤] æˆåŠŸç‰©ç†åˆ é™¤äº† {cleaned_zombies} æ¡å·²åºŸå¼ƒçš„å†…éƒ¨IDè®°å½• (å¦‚ xxx-S1E1)ã€‚")
            
        final_msg = f"åŒæ­¥å®Œæˆï¼æ–°å¢/æ›´æ–°: {total_updated_count} ä¸ªåª’ä½“é¡¹, æ ‡è®°ç¦»çº¿: {total_offline_count} ä¸ªåª’ä½“é¡¹ã€‚"
        logger.info(f"  âœ… {final_msg}")
        # è‡ªåŠ¨è§¦å‘åˆ†çº§åŒæ­¥ 
        logger.info("  âœ [è‡ªåŠ¨è§¦å‘] å…ƒæ•°æ®æ›´æ–°å®Œæ¯•ï¼Œå¼€å§‹åŒæ­¥åˆ†çº§ä¿¡æ¯åˆ° Emby...")
        # ä¸ºäº†é˜²æ­¢åˆ†çº§åŒæ­¥çš„è¿›åº¦æ¡è¦†ç›–æ‰ä¸»ä»»åŠ¡çš„è¿›åº¦ï¼Œæˆ‘ä»¬å¯ä»¥é€‰æ‹©ä¸ä¼  update_status_callback æˆ–è€…æ¥å—å®ƒé‡ç½®è¿›åº¦
        # è¿™é‡Œç›´æ¥è°ƒç”¨ï¼Œæ—¥å¿—ä¼šè®°å½•è¿‡ç¨‹
        try:
            task_sync_ratings_to_emby(processor)
        except Exception as e:
            logger.error(f"  âš ï¸ è‡ªåŠ¨åˆ†çº§åŒæ­¥å¤±è´¥ (ä¸å½±å“ä¸»ä»»åŠ¡å®Œæˆ): {e}")
        task_manager.update_status_from_thread(100, final_msg)

    except Exception as e:
        logger.error(f"æ‰§è¡Œ '{task_name}' ä»»åŠ¡æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}", exc_info=True)
        task_manager.update_status_from_thread(-1, f"ä»»åŠ¡å¤±è´¥: {e}")

# --- è¾…åŠ©å‡½æ•°ï¼šæ£€æŸ¥åˆ†çº§æ˜¯å¦åŒ¹é… (å¸¦æ—¥å¿—è°ƒè¯•ç‰ˆ) ---
def _is_rating_match(item_name: str, item_rating: str, rating_filters: List[str]) -> bool:
    """
    æ£€æŸ¥ Emby çš„ OfficialRating æ˜¯å¦åŒ¹é…æŒ‡å®šçš„ä¸­åˆ†çº§æ ‡ç­¾åˆ—è¡¨ã€‚
    """
    if not rating_filters:
        return True # æœªè®¾ç½®è¿‡æ»¤å™¨ï¼Œé»˜è®¤åŒ¹é…æ‰€æœ‰
    
    # 1. å¦‚æœé¡¹ç›®æ²¡æœ‰åˆ†çº§ï¼Œç›´æ¥ä¸åŒ¹é…
    if not item_rating:
        # logger.trace(f"  [åˆ†çº§è¿‡æ»¤] '{item_name}' æ— åˆ†çº§ä¿¡æ¯ -> è·³è¿‡")
        return False 

    # 2. å°†ä¸­æ–‡æ ‡ç­¾ï¼ˆå¦‚"é™åˆ¶çº§"ï¼‰å±•å¼€ä¸ºæ‰€æœ‰å¯èƒ½çš„ä»£ç ï¼ˆå¦‚"R", "NC-17"ï¼‰
    target_codes = queries_db._expand_rating_labels(rating_filters)
    
    # 3. æ£€æŸ¥åŒ¹é…
    # Emby çš„ OfficialRating å¯èƒ½æ˜¯ "R" ä¹Ÿå¯èƒ½æ˜¯ "US: R"ï¼Œè¿™é‡Œåšå®½æ¾åŒ¹é…
    is_match = item_rating in target_codes or \
               (item_rating.split(':')[-1].strip() in target_codes)
    
    # logger.trace(f"  [åˆ†çº§è¿‡æ»¤] '{item_name}' åˆ†çº§: {item_rating} | ç›®æ ‡: {target_codes} | åŒ¹é…: {is_match}")
    return is_match

# --- æ‰§è¡Œè‡ªåŠ¨æ‰“æ ‡è§„åˆ™ä»»åŠ¡ ---
def task_execute_auto_tagging_rules(processor):
    """
    ä»»åŠ¡ï¼šè¯»å–æ•°æ®åº“ä¸­çš„è‡ªåŠ¨æ‰“æ ‡è§„åˆ™ï¼Œå¹¶ä¾æ¬¡æ‰§è¡Œã€‚
    """
    rules = settings_db.get_setting('auto_tagging_rules') or []
    if not rules:
        logger.info("  âœ [è‡ªåŠ¨æ‰“æ ‡] æœªé…ç½®ä»»ä½•è§„åˆ™ï¼Œä»»åŠ¡ç»“æŸã€‚")
        return

    total_rules = len(rules)
    logger.info(f"  âœ [è‡ªåŠ¨æ‰“æ ‡] å¼€å§‹æ‰§è¡Œ {total_rules} æ¡è§„åˆ™...")

    for idx, rule in enumerate(rules):
        if processor.is_stop_requested(): 
            logger.info("  ğŸš« ä»»åŠ¡è¢«ä¸­æ­¢ã€‚")
            break

        tags = rule.get('tags')
        if not tags: continue
        
        library_ids = rule.get('library_ids', [])
        rating_filters = rule.get('rating_filters', [])
        
        # ç›´æ¥è°ƒç”¨ç°æœ‰çš„æ‰¹é‡æ‰“æ ‡é€»è¾‘
        # æ³¨æ„ï¼štask_bulk_auto_tag å†…éƒ¨ä¼šå¤„ç†è¿›åº¦æ›´æ–°å’Œå¼‚å¸¸æ•è·
        task_bulk_auto_tag(processor, library_ids, tags, rating_filters)

    task_manager.update_status_from_thread(100, "è‡ªåŠ¨æ‰“æ ‡è§„åˆ™æ‰§è¡Œå®Œæ¯•")

# --- è‡ªåŠ¨æ‰“æ ‡ (ä¿®å¤è¿›åº¦æ¡å¡é¡¿ç‰ˆ) ---
def task_bulk_auto_tag(processor, library_ids: List[str], tags: List[str], rating_filters: Optional[List[str]] = None):
    """
    åå°ä»»åŠ¡ï¼šæ”¯æŒä¸ºå¤šä¸ªåª’ä½“åº“æ‰¹é‡æ‰“æ ‡ç­¾ (æ”¯æŒåˆ†çº§è¿‡æ»¤ï¼Œä¼˜å…ˆä½¿ç”¨è‡ªå®šä¹‰åˆ†çº§)ã€‚
    """
    try:
        if not library_ids:
            logger.info("  âœ æœªæŒ‡å®šåª’ä½“åº“ï¼Œå°†æ‰«ææ‰€æœ‰åº“...")
            all_libs = emby.get_emby_libraries(processor.emby_url, processor.emby_api_key, processor.emby_user_id)
            if all_libs:
                # è¿‡æ»¤æ‰åˆé›†ã€æ’­æ”¾åˆ—è¡¨ç­‰éå†…å®¹åº“
                library_ids = [l['Id'] for l in all_libs if l.get('CollectionType') not in ['boxsets', 'playlists', 'music']]
        
        total_libs = len(library_ids)
        filter_msg = f" (åˆ†çº§é™åˆ¶: {','.join(rating_filters)})" if rating_filters else ""
        
        for lib_idx, lib_id in enumerate(library_ids):
            # åˆå§‹çŠ¶æ€æ›´æ–°
            task_manager.update_status_from_thread(int((lib_idx/total_libs)*100), f"æ­£åœ¨è¯»å–ç¬¬ {lib_idx+1}/{total_libs} ä¸ªåª’ä½“åº“...")
            
            # â˜…â˜…â˜… 2. è¯·æ±‚ OfficialRating å’Œ CustomRating å­—æ®µ â˜…â˜…â˜…
            items = emby.get_emby_library_items(
                base_url=processor.emby_url,
                api_key=processor.emby_api_key,
                library_ids=[lib_id],
                media_type_filter="Movie,Series,Episode",
                user_id=processor.emby_user_id,
                fields="Id,Name,OfficialRating,CustomRating" 
            )
            
            if not items: 
                logger.info(f"  åª’ä½“åº“ {lib_id} ä¸ºç©ºæˆ–æ— æ³•è®¿é—®ã€‚")
                continue

            total_items = len(items)
            logger.info(f"  åª’ä½“åº“ {lib_id} æ‰«æåˆ° {total_items} ä¸ªé¡¹ç›®ï¼Œå¼€å§‹è¿‡æ»¤...")
            
            processed_count = 0
            skipped_count = 0

            for i, item in enumerate(items):
                if processor.is_stop_requested(): return
                
                item_name = item.get('Name', 'æœªçŸ¥')
                
                # â˜…â˜…â˜… ä¿®å¤ç‚¹ï¼šå°†è¿›åº¦æ›´æ–°ç§»åˆ°è¿‡æ»¤é€»è¾‘ä¹‹å‰ï¼Œå¹¶æé«˜é¢‘ç‡ â˜…â˜…â˜…
                if i % 5 == 0:
                    # è®¡ç®—å…¨å±€è¿›åº¦
                    current_progress = int((lib_idx/total_libs)*100 + (i/total_items)*(100/total_libs))
                    task_manager.update_status_from_thread(
                        current_progress, 
                        f"åº“({lib_idx+1}/{total_libs}) æ­£åœ¨æ‰«æ: {item_name}"
                    )

                # â˜…â˜…â˜… 3. åˆ†çº§è¿‡æ»¤é€»è¾‘ (è‡ªå®šä¹‰åˆ†çº§ä¼˜å…ˆ) â˜…â˜…â˜…
                if rating_filters:
                    # ä¼˜å…ˆå– CustomRatingï¼Œå¦‚æœæ²¡æœ‰åˆ™å– OfficialRating
                    item_rating = item.get('CustomRating') or item.get('OfficialRating')
                    
                    if not _is_rating_match(item_name, item_rating, rating_filters):
                        skipped_count += 1
                        continue # åˆ†çº§ä¸åŒ¹é…ï¼Œè·³è¿‡

                
                # æ‰§è¡Œæ‰“æ ‡
                success = emby.add_tags_to_item(item.get("Id"), tags, processor.emby_url, processor.emby_api_key, processor.emby_user_id)
                if success:
                    processed_count += 1

            logger.info(f"  åª’ä½“åº“ {lib_id} å¤„ç†å®Œæˆ: æ‰“æ ‡ {processed_count} ä¸ª, è·³è¿‡ {skipped_count} ä¸ª (ä¸ç¬¦åˆ†çº§)ã€‚")
        
        task_manager.update_status_from_thread(100, "æ‰€æœ‰é€‰å®šåº“æ‰¹é‡æ‰“æ ‡å®Œæˆ")
    except Exception as e:
        logger.error(f"æ‰¹é‡æ‰“æ ‡ä»»åŠ¡å¤±è´¥: {e}", exc_info=True)
        task_manager.update_status_from_thread(-1, "ä»»åŠ¡å¼‚å¸¸ä¸­æ­¢")

def task_bulk_remove_tags(processor, library_ids: List[str], tags: List[str], rating_filters: Optional[List[str]] = None):
    """
    åå°ä»»åŠ¡ï¼šä»æŒ‡å®šåª’ä½“åº“ä¸­æ‰¹é‡ç§»é™¤ç‰¹å®šæ ‡ç­¾ (æ”¯æŒåˆ†çº§è¿‡æ»¤ï¼Œä¼˜å…ˆä½¿ç”¨è‡ªå®šä¹‰åˆ†çº§)ã€‚
    """
    try:
        if not library_ids:
            logger.info("  âœ æœªæŒ‡å®šåª’ä½“åº“ï¼Œå°†æ‰«ææ‰€æœ‰åº“...")
            all_libs = emby.get_emby_libraries(processor.emby_url, processor.emby_api_key, processor.emby_user_id)
            if all_libs:
                library_ids = [l['Id'] for l in all_libs if l.get('CollectionType') not in ['boxsets', 'playlists', 'music']]
        logger.info(f"å¯åŠ¨æ‰¹é‡ç§»é™¤ä»»åŠ¡ | ç›®æ ‡åº“: {len(library_ids)}ä¸ª | æ ‡ç­¾: {tags} | åˆ†çº§é™åˆ¶: {rating_filters if rating_filters else 'æ—  (å…¨é‡)'}")
        
        total_libs = len(library_ids)
        filter_msg = f" (åˆ†çº§é™åˆ¶: {','.join(rating_filters)})" if rating_filters else ""

        for lib_idx, lib_id in enumerate(library_ids):
            # åˆå§‹çŠ¶æ€æ›´æ–°
            task_manager.update_status_from_thread(int((lib_idx/total_libs)*100), f"æ­£åœ¨è¯»å–ç¬¬ {lib_idx+1}/{total_libs} ä¸ªåª’ä½“åº“...")

            items = emby.get_emby_library_items(
                base_url=processor.emby_url, api_key=processor.emby_api_key,
                library_ids=[lib_id], media_type_filter="Movie,Series,Episode",
                user_id=processor.emby_user_id,
                fields="Id,Name,OfficialRating,CustomRating" 
            )
            if not items: continue

            total_items = len(items)
            processed_count = 0
            skipped_count = 0

            for i, item in enumerate(items):
                if processor.is_stop_requested(): return
                
                item_name = item.get('Name', 'æœªçŸ¥')

                # â˜…â˜…â˜… ä¿®å¤ç‚¹ï¼šå°†è¿›åº¦æ›´æ–°ç§»åˆ°è¿‡æ»¤é€»è¾‘ä¹‹å‰ï¼Œå¹¶æé«˜é¢‘ç‡ â˜…â˜…â˜…
                if i % 5 == 0:
                    current_progress = int((lib_idx/total_libs)*100 + (i/total_items)*(100/total_libs))
                    task_manager.update_status_from_thread(
                        current_progress, 
                        f"åº“({lib_idx+1}/{total_libs}) æ­£åœ¨æ‰«æ: {item_name}"
                    )

                # â˜…â˜…â˜… åˆ†çº§è¿‡æ»¤é€»è¾‘ (è‡ªå®šä¹‰åˆ†çº§ä¼˜å…ˆ) â˜…â˜…â˜…
                if rating_filters:
                    # ä¼˜å…ˆå– CustomRatingï¼Œå¦‚æœæ²¡æœ‰åˆ™å– OfficialRating
                    item_rating = item.get('CustomRating') or item.get('OfficialRating')
                    
                    if not _is_rating_match(item.get('Name'), item_rating, rating_filters):
                        skipped_count += 1
                        continue 

                
                # æ‰§è¡Œç§»é™¤ 
                success = emby.remove_tags_from_item(item.get("Id"), tags, processor.emby_url, processor.emby_api_key, processor.emby_user_id)
                if success:
                    processed_count += 1
            
            logger.info(f"  åª’ä½“åº“ {lib_id} å¤„ç†å®Œæˆ: ç§»é™¤ {processed_count} ä¸ª, è·³è¿‡ {skipped_count} ä¸ªã€‚")
        
        task_manager.update_status_from_thread(100, "æ‰¹é‡æ ‡ç­¾ç§»é™¤å®Œæˆ")
    except Exception as e:
        logger.error(f"æ‰¹é‡æ¸…ç†ä»»åŠ¡å¤±è´¥: {e}")
        task_manager.update_status_from_thread(-1, "æ¸…ç†ä»»åŠ¡å¼‚å¸¸ä¸­æ­¢")

# --- åˆ†çº§åŒæ­¥ä»»åŠ¡ ---
def task_sync_ratings_to_emby(processor):
    """
    ã€åˆ†çº§åŒæ­¥ä»»åŠ¡ã€‘
    ä¸å†åŒºåˆ†æ¨¡å¼ï¼Œæ¯æ¬¡æ‰§è¡Œéƒ½ç¡®ä¿ï¼š
    1. CustomRating: åŒå‘äº’è¡¥ (ä»¥DBä¸ºå‡†)ã€‚
    2. OfficialRating: å•å‘å¼ºåˆ¶ (DB US -> Emby)ã€‚
    """
    logger.trace(f"--- å¼€å§‹æ‰§è¡Œåˆ†çº§åŒæ­¥ä»»åŠ¡ (å…¨é‡æ¯”å¯¹) ---")
    
    # 1. ä»æ•°æ®åº“è·å–æ‰€æœ‰åœ¨åº“é¡¹ç›®
    with connection.get_db_connection() as conn:
        cursor = conn.cursor()
        cursor.execute("""
            SELECT tmdb_id, item_type, emby_item_ids_json, custom_rating, official_rating_json 
            FROM media_metadata 
            WHERE in_library = TRUE 
              AND emby_item_ids_json IS NOT NULL 
              AND jsonb_array_length(emby_item_ids_json) > 0
        """)
        all_items = cursor.fetchall()

    total_items = len(all_items)
    logger.info(f"  âœ æ‰«æåˆ° {total_items} ä¸ªåœ¨åº“é¡¹ç›®ï¼Œå‡†å¤‡è¿›è¡Œå·®å¼‚æ¯”å¯¹...")
    
    BATCH_SIZE = 200
    updated_emby_count = 0
    updated_db_count = 0
    
    for i in range(0, total_items, BATCH_SIZE):
        if processor.is_stop_requested(): break
        
        batch = all_items[i : i + BATCH_SIZE]
        
        emby_id_map = {} 
        emby_ids_to_fetch = []
        
        for row in batch:
            try:
                e_ids = row['emby_item_ids_json']
                if e_ids:
                    eid = e_ids[0]
                    emby_id_map[eid] = row
                    emby_ids_to_fetch.append(eid)
            except: continue

        if not emby_ids_to_fetch: continue

        # æ‰¹é‡è·å– Emby ç°çŠ¶
        emby_items = emby.get_emby_items_by_id(
            base_url=processor.emby_url,
            api_key=processor.emby_api_key,
            user_id=processor.emby_user_id,
            item_ids=emby_ids_to_fetch,
            fields="OfficialRating,CustomRating,LockedFields,Name"
        )
        
        for e_item in emby_items:
            eid = e_item['Id']
            db_row = emby_id_map.get(eid)
            if not db_row: continue
            
            tmdb_id = db_row['tmdb_id']
            item_type = db_row['item_type']
            item_name = e_item.get('Name', tmdb_id)
            
            db_custom = db_row['custom_rating']
            emby_custom = e_item.get('CustomRating')
            
            db_official_json = db_row['official_rating_json'] or {}
            if isinstance(db_official_json, str):
                try: db_official_json = json.loads(db_official_json)
                except: db_official_json = {}
            
            db_us_rating = db_official_json.get('US')
            emby_official = e_item.get('OfficialRating')

            changes_to_emby = {}
            changes_to_db = {}

            # --- 1. CustomRating (åŒå‘äº’è¡¥) ---
            if db_custom and not emby_custom:
                changes_to_emby['CustomRating'] = db_custom
            elif emby_custom and not db_custom:
                changes_to_db['custom_rating'] = emby_custom
            elif db_custom and emby_custom and db_custom != emby_custom:
                changes_to_emby['CustomRating'] = db_custom

            # --- 2. OfficialRating (å•å‘å¼ºåˆ¶) ---
            # åªè¦ DB æœ‰ US åˆ†çº§ï¼Œä¸” Emby ä¸ä¸€è‡´ï¼Œå°±è¦†ç›–ï¼
            if db_us_rating and db_us_rating != emby_official:
                changes_to_emby['OfficialRating'] = db_us_rating
                
                # è‡ªåŠ¨è§£é” OfficialRating é˜²æ­¢ä¿®æ”¹å¤±è´¥
                locked = e_item.get('LockedFields', [])
                if 'OfficialRating' in locked:
                    locked.remove('OfficialRating')
                    changes_to_emby['LockedFields'] = locked

            # --- æ‰§è¡Œæ›´æ–° ---
            if changes_to_emby:
                success = emby.update_emby_item_details(
                    item_id=eid,
                    new_data=changes_to_emby,
                    emby_server_url=processor.emby_url,
                    emby_api_key=processor.emby_api_key,
                    user_id=processor.emby_user_id
                )
                if success:
                    updated_emby_count += 1
                    # logger.trace(f"  âœ [åŒæ­¥->Emby] {item_name}: {changes_to_emby}")

            if changes_to_db:
                media_db.update_media_metadata_fields(tmdb_id, item_type, changes_to_db)
                updated_db_count += 1
                # logger.trace(f"  âœ [åŒæ­¥->DB] {item_name}: {changes_to_db}")

        progress = int((i / total_items) * 100)
        task_manager.update_status_from_thread(progress, f"åˆ†çº§åŒæ­¥: å·²å¤„ç† {i}/{total_items}...")

    logger.info(f"--- åˆ†çº§åŒæ­¥å®Œæˆ ---")
    logger.info(f"  âœ Emby ä¿®æ­£: {updated_emby_count} æ¡")
    logger.info(f"  âœ DB å›å†™: {updated_db_count} æ¡")
    task_manager.update_status_from_thread(100, f"åˆ†çº§åŒæ­¥å®Œæˆ: Embyä¿®æ­£{updated_emby_count}, DBå›å†™{updated_db_count}")

# --- æ‰«æç›‘æ§ç›®å½•æŸ¥æ¼è¡¥ç¼º ---
def task_scan_monitor_folders(processor):
    """
    ä»»åŠ¡ï¼šæ‰«æé…ç½®çš„ç›‘æ§ç›®å½•ï¼ŒæŸ¥æ‰¾æ•°æ®åº“ä¸­ä¸å­˜åœ¨çš„åª’ä½“ï¼ˆæ¼ç½‘ä¹‹é±¼ï¼‰ï¼Œå¹¶è§¦å‘ä¸»åŠ¨å¤„ç†ã€‚
    ä¼˜åŒ–ï¼š
    1. å›æº¯æ—¶é—´å¯é…ç½®ã€‚
    2. ä¼˜å…ˆæ£€æŸ¥æ—¶é—´æˆ³ï¼Œæé€Ÿè¿‡æ»¤æ—§æ–‡ä»¶ã€‚
    3. æŸ¥åº“æ¯”å¯¹æ–‡ä»¶åï¼Œç¡®ä¿åªå¤„ç†çœŸæ­£æœªå…¥åº“çš„æ–‡ä»¶ã€‚
    4. ã€ä¿®æ­£ã€‘å‘½ä¸­æ’é™¤è·¯å¾„æ—¶ï¼Œç›´æ¥è·³è¿‡å¤„ç†ï¼ˆä¸åˆ·æ–°ï¼‰ï¼Œé˜²æ­¢å› æ— æ³•å…¥åº“å¯¼è‡´çš„æ­»å¾ªç¯åˆ·æ–°ã€‚
    """
    # 1. è·å–é…ç½®
    monitor_enabled = processor.config.get(constants.CONFIG_OPTION_MONITOR_ENABLED)
    monitor_paths = processor.config.get(constants.CONFIG_OPTION_MONITOR_PATHS, [])
    monitor_extensions = processor.config.get(constants.CONFIG_OPTION_MONITOR_EXTENSIONS, constants.DEFAULT_MONITOR_EXTENSIONS)
    lookback_days = processor.config.get(constants.CONFIG_OPTION_MONITOR_SCAN_LOOKBACK_DAYS, constants.DEFAULT_MONITOR_SCAN_LOOKBACK_DAYS)
    
    # è·å–æ’é™¤è·¯å¾„é…ç½®å¹¶è§„èŒƒåŒ–
    monitor_exclude_dirs = processor.config.get(constants.CONFIG_OPTION_MONITOR_EXCLUDE_DIRS, constants.DEFAULT_MONITOR_EXCLUDE_DIRS)
    exclude_paths = [os.path.normpath(d).lower() for d in (monitor_exclude_dirs or [])]

    logger.info(f"  âœ å¼€å§‹æ‰§è¡Œç›‘æ§ç›®å½•æŸ¥æ¼æ‰«æ (å›æº¯ {lookback_days} å¤©)")

    if not monitor_enabled or not monitor_paths:
        logger.info("  âœ å®æ—¶ç›‘æ§æœªå¯ç”¨æˆ–æœªé…ç½®è·¯å¾„ï¼Œè·³è¿‡æ‰«æã€‚")
        return

    valid_exts = set(ext.lower() for ext in monitor_extensions)

    # 2. è·å–å·²çŸ¥ TMDb ID (ç™½åå•)
    known_tmdb_ids = set()
    try:
        with connection.get_db_connection() as conn:
            cursor = conn.cursor()
            cursor.execute("SELECT DISTINCT tmdb_id FROM media_metadata WHERE tmdb_id IS NOT NULL")
            for row in cursor.fetchall():
                known_tmdb_ids.add(str(row['tmdb_id']))
        logger.info(f"  âœ åŠ è½½äº† {len(known_tmdb_ids)} ä¸ªå·²çŸ¥ TMDb ID (ç™½åå•)ã€‚")
    except Exception as e:
        logger.error(f"  ğŸš« æ— æ³•è¯»å–æ•°æ®åº“ç™½åå•ï¼Œä»»åŠ¡ä¸­æ­¢: {e}")
        return
    
    tmdb_regex = r'(?:tmdb|tmdbid)[-_=\s]*(\d+)'
    processed_in_this_run = set()
    
    # Key: tmdb_id, Value: Set[filenames]
    db_assets_cache = {}

    scan_count = 0
    trigger_count = 0
    skipped_old_count = 0
    skipped_exists_count = 0 
    
    now = time.time()
    cutoff_time = now - (lookback_days * 24 * 3600)

    for root_path in monitor_paths:
        if not os.path.exists(root_path):
            logger.warning(f"  âš ï¸ ç›‘æ§è·¯å¾„ä¸å­˜åœ¨: {root_path}")
            continue

        logger.info(f"  âœ æ­£åœ¨æ‰«æç›®å½•: {root_path}")
        
        for dirpath, dirnames, filenames in os.walk(root_path):
            # â˜…â˜…â˜… ä¿®æ­£ï¼šæ’é™¤è·¯å¾„æ£€æŸ¥é€»è¾‘ â˜…â˜…â˜…
            norm_dirpath = os.path.normpath(dirpath).lower()
            hit_exclude = False
            
            for exc_path in exclude_paths:
                if norm_dirpath.startswith(exc_path):
                    hit_exclude = True
                    break
            
            if hit_exclude:
                # â˜…â˜…â˜… å…³é”®ä¿®æ”¹ï¼šç›´æ¥é™é»˜è·³è¿‡ï¼Œä¸æ‰§è¡Œåˆ·æ–° â˜…â˜…â˜…
                # åŸå› ï¼šæ’é™¤çš„æ–‡ä»¶æ°¸è¿œä¸ä¼šå…¥åº“ã€‚å¦‚æœåœ¨è¿™é‡Œåˆ·æ–°ï¼Œæ¯æ¬¡å®šæ—¶ä»»åŠ¡è¿è¡Œï¼ˆåªè¦åœ¨å›æº¯æœŸå†…ï¼‰
                # éƒ½ä¼šé‡å¤åˆ·æ–°è¿™äº›æ–‡ä»¶ï¼Œå¯¼è‡´æ­»å¾ªç¯å’Œæ—¥å¿—åˆ·å±ã€‚
                # æ’é™¤ç›®å½•çš„åˆ·æ–°åº”å®Œå…¨ä¾èµ–â€œå®æ—¶ç›‘æ§â€æˆ– Emby è‡ªèº«çš„è®¡åˆ’ä»»åŠ¡ã€‚
                
                # logger.debug(f"  ğŸš« [æ‰«æè·³è¿‡] å‘½ä¸­æ’é™¤ç›®å½•: {os.path.basename(dirpath)}")
                dirnames[:] = [] # åœæ­¢å‘ä¸‹é€’å½’
                continue 

            folder_name = os.path.basename(dirpath)
            match_folder = re.search(tmdb_regex, folder_name, re.IGNORECASE)
            
            # æå–å½“å‰ç›®å½•å¯èƒ½çš„ ID (ä¼˜å…ˆç”¨æ–‡ä»¶å¤¹ID)
            folder_tmdb_id = match_folder.group(1) if match_folder else None

            for filename in filenames:
                if filename.startswith('.'): continue
                _, ext = os.path.splitext(filename)
                if ext.lower() not in valid_exts: continue
                
                file_path = os.path.join(dirpath, filename)
                
                # â˜…â˜…â˜… ç¬¬ä¸€é“é˜²çº¿ï¼šæ—¶é—´è¿‡æ»¤ (æé€Ÿ) â˜…â˜…â˜…
                try:
                    stat = os.stat(file_path)
                    file_time = max(stat.st_mtime, stat.st_ctime)
                    
                    if lookback_days > 0 and file_time < cutoff_time:
                        skipped_old_count += 1
                        continue 
                except OSError:
                    continue 

                scan_count += 1
                if scan_count % 300 == 0:
                    time.sleep(0.05)
                    dynamic_progress = 50 + int((scan_count % 10000) / 10000 * 30)
                    task_manager.update_status_from_thread(
                        dynamic_progress, 
                        f"æ‰«æä¸­... (å·²æ‰« {scan_count}, è·³è¿‡æ—§æ–‡ä»¶ {skipped_old_count}, è·³è¿‡å·²å­˜ {skipped_exists_count})"
                    )

                # --- ID æå– ---
                target_id = folder_tmdb_id
                
                if not target_id:
                    grandparent_path = os.path.dirname(dirpath)
                    grandparent_name = os.path.basename(grandparent_path)
                    match_grand = re.search(tmdb_regex, grandparent_name, re.IGNORECASE)
                    if match_grand:
                        target_id = match_grand.group(1)
                
                if not target_id:
                    match_file = re.search(tmdb_regex, filename, re.IGNORECASE)
                    if match_file:
                        target_id = match_file.group(1)
                
                # --- åˆ¤å®šé€»è¾‘ ---
                if target_id:
                    if target_id in processed_in_this_run:
                        continue

                    if target_id not in db_assets_cache:
                        db_assets_cache[target_id] = media_db.get_known_filenames_by_tmdb_id(target_id)
                    
                    if filename in db_assets_cache[target_id]:
                        skipped_exists_count += 1
                        continue

                    logger.info(f"  ğŸ” å‘ç°æœªå…¥åº“æ–‡ä»¶: {filename} (ID: {target_id})ï¼Œè§¦å‘æ£€æŸ¥...")
                    try:
                        processor.process_file_actively(file_path)
                        processed_in_this_run.add(target_id)
                        if target_id in db_assets_cache:
                            db_assets_cache[target_id].add(filename)
                        trigger_count += 1
                        time.sleep(1) 
                    except Exception as e:
                        logger.error(f"  ğŸš« å¤„ç†æ–‡ä»¶å¤±è´¥: {e}")

    logger.info(f"  âœ ç›‘æ§ç›®å½•æ‰«æå®Œæˆã€‚æ‰«æ: {scan_count}, è§¦å‘å¤„ç†: {trigger_count}")
    task_manager.update_status_from_thread(100, f"æ‰«æå®Œæˆï¼Œå¤„ç†äº† {trigger_count} ä¸ªæ–°é¡¹ç›®")

# --- ä»æ•°æ®åº“æ¢å¤æœ¬åœ°è¦†ç›–ç¼“å­˜ ---
def task_restore_local_cache_from_db(processor):
    """
    ã€ç¾éš¾æ¢å¤ã€‘ä»æ•°æ®åº“è¯»å–å…ƒæ•°æ®ï¼Œé‡æ–°ç”Ÿæˆæœ¬åœ° override JSON æ–‡ä»¶ã€‚
    ç”¨äºè¯¯åˆ  cache ç›®å½•æˆ–è¿ç§»ç¯å¢ƒåçš„æ•°æ®æ¢å¤ã€‚
    """
    task_name = "æ¢å¤è¦†ç›–ç¼“å­˜"
    logger.trace(f"--- å¼€å§‹æ‰§è¡Œ '{task_name}' ---")
    
    try:
        # 1. è·å–æ‰€æœ‰é¡¶å±‚é¡¹ç›® (Movie, Series)
        task_manager.update_status_from_thread(5, "æ­£åœ¨è¯»å–æ•°æ®åº“...")
        
        items_to_restore = []
        with connection.get_db_connection() as conn:
            cursor = conn.cursor()
            cursor.execute("""
                SELECT * FROM media_metadata 
                WHERE item_type IN ('Movie', 'Series') 
                  AND tmdb_id IS NOT NULL 
                  AND tmdb_id != '0'
            """)
            items_to_restore = [dict(row) for row in cursor.fetchall()]

        total = len(items_to_restore)
        if total == 0:
            task_manager.update_status_from_thread(100, "æ•°æ®åº“ä¸­æ²¡æœ‰å¯æ¢å¤çš„é¡¹ç›®ã€‚")
            return

        logger.info(f"  âœ å‘ç° {total} ä¸ªé¡¹ç›®éœ€è¦æ¢å¤ç¼“å­˜ã€‚")
        
        success_count = 0
        
        for i, item in enumerate(items_to_restore):
            if processor.is_stop_requested():
                logger.warning("  ğŸš« ä»»åŠ¡è¢«ä¸­æ­¢ã€‚")
                break

            # æ¯å¤„ç†50ä¸ªæ–‡ä»¶ï¼Œæš‚åœ 0.01 ç§’ï¼Œé˜²æ­¢ IO/CPU 100% å¡æ­»ç³»ç»Ÿ
            if i % 50 == 0:
                time.sleep(0.01)

            tmdb_id = item['tmdb_id']
            item_type = item['item_type']
            title = item.get('title', tmdb_id)
            
            # æ›´æ–°è¿›åº¦
            if i % 5 == 0:
                progress = int((i / total) * 100)
                task_manager.update_status_from_thread(progress, f"æ­£åœ¨æ¢å¤ ({i+1}/{total}): {title}")

            try:
                # --- A. å‡†å¤‡æ¼”å‘˜æ•°æ® ---
                db_actors = []
                if item.get('actors_json'):
                    try:
                        raw_actors = item['actors_json']
                        actors_link = json.loads(raw_actors) if isinstance(raw_actors, str) else raw_actors
                        
                        actor_tmdb_ids = [a['tmdb_id'] for a in actors_link if 'tmdb_id' in a]
                        
                        if actor_tmdb_ids:
                            with connection.get_db_connection() as conn:
                                cursor = conn.cursor()
                                # æ‰¹é‡æŸ¥è¯¢æ¼”å‘˜è¯¦æƒ…
                                placeholders = ','.join(['%s'] * len(actor_tmdb_ids))
                                sql = f"""
                                    SELECT am.*, pim.primary_name as name
                                    FROM actor_metadata am
                                    LEFT JOIN person_identity_map pim ON am.tmdb_id = pim.tmdb_person_id
                                    WHERE am.tmdb_id IN ({placeholders})
                                """
                                cursor.execute(sql, tuple(actor_tmdb_ids))
                                actor_rows = cursor.fetchall()
                                actor_map = {r['tmdb_id']: dict(r) for r in actor_rows}
                                
                                # ç»„è£…å›æœ‰åºåˆ—è¡¨
                                for link in actors_link:
                                    tid = link.get('tmdb_id')
                                    if tid in actor_map:
                                        full_actor = actor_map[tid].copy()
                                        full_actor['character'] = link.get('character')
                                        full_actor['order'] = link.get('order')
                                        db_actors.append(full_actor)
                                        
                                db_actors.sort(key=lambda x: x.get('order', 999))
                    except Exception as e_actor:
                        logger.warning(f"  âš ï¸ è§£ææ¼”å‘˜æ•°æ®å¤±è´¥ ({title}): {e_actor}")

                # --- B. é‡å»ºä¸» Payload ---
                payload = reconstruct_metadata_from_db(item, db_actors)

                # --- C. å¦‚æœæ˜¯å‰§é›†ï¼Œæ³¨å…¥åˆ†å­£/åˆ†é›†æ•°æ® ---
                if item_type == "Series":
                    with connection.get_db_connection() as conn:
                        cursor = conn.cursor()
                        
                        # æŸ¥åˆ†å­£
                        cursor.execute("SELECT * FROM media_metadata WHERE parent_series_tmdb_id = %s AND item_type = 'Season'", (tmdb_id,))
                        seasons_rows = cursor.fetchall()
                        seasons_data = []
                        for s_row in seasons_rows:
                            s_data = {
                                "id": int(s_row['tmdb_id']) if s_row['tmdb_id'].isdigit() else 0,
                                "name": s_row['title'],
                                "overview": s_row['overview'],
                                "season_number": s_row['season_number'],
                                "air_date": str(s_row['release_date']) if s_row['release_date'] else None,
                                "poster_path": s_row['poster_path']
                            }
                            seasons_data.append(s_data)
                        
                        # æŸ¥åˆ†é›†
                        cursor.execute("SELECT * FROM media_metadata WHERE parent_series_tmdb_id = %s AND item_type = 'Episode'", (tmdb_id,))
                        episodes_rows = cursor.fetchall()
                        episodes_data = {} 
                        
                        for e_row in episodes_rows:
                            s_num = e_row['season_number']
                            e_num = e_row['episode_number']
                            key = f"S{s_num}E{e_num}"
                            
                            e_data = {
                                "id": int(e_row['tmdb_id']) if e_row['tmdb_id'].isdigit() else 0,
                                "name": e_row['title'],
                                "overview": e_row['overview'],
                                "season_number": s_num,
                                "episode_number": e_num,
                                "air_date": str(e_row['release_date']) if e_row['release_date'] else None,
                                "vote_average": e_row['rating'],
                            }
                            episodes_data[key] = e_data

                        if seasons_data: payload['seasons_details'] = seasons_data
                        if episodes_data: payload['episodes_details'] = episodes_data

                # --- D. å†™å…¥æ–‡ä»¶ ---
                # æ„é€ ä¸Šä¸‹æ–‡å¯¹è±¡ (Id='pending' é¿å…è§¦å‘ Emby API è¯·æ±‚)
                fake_item_details = {
                    "Id": "pending", 
                    "Name": title, 
                    "Type": item_type, 
                    "ProviderIds": {"Tmdb": tmdb_id}
                }
                
                processor.sync_item_metadata(
                    item_details=fake_item_details,
                    tmdb_id=tmdb_id,
                    metadata_override=payload
                )
                success_count += 1
                
            except Exception as e_item:
                logger.error(f"  ğŸš« æ¢å¤é¡¹ç›® '{title}' å¤±è´¥: {e_item}")

        final_msg = f"æ¢å¤å®Œæˆï¼æˆåŠŸç”Ÿæˆ {success_count}/{total} ä¸ªé¡¹ç›®çš„æœ¬åœ°ç¼“å­˜æ–‡ä»¶ã€‚"
        logger.info(f"  âœ… {final_msg}")
        task_manager.update_status_from_thread(100, final_msg)

    except Exception as e:
        logger.error(f"æ‰§è¡Œ '{task_name}' æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}", exc_info=True)
        task_manager.update_status_from_thread(-1, f"ä»»åŠ¡å¤±è´¥: {e}")

def task_scan_incomplete_assets(processor):
    """
    ã€æ–°ä»»åŠ¡ - ä¼˜åŒ–ç‰ˆã€‘å…¨åº“æ‰«æèµ„äº§æ•°æ®ä¸å®Œæ•´çš„é¡¹ç›®ã€‚
    ç›´æ¥åˆ©ç”¨ SQL Join è·å–æ‰€éœ€çš„ Emby IDï¼Œæ— éœ€äºŒæ¬¡æŸ¥è¯¢ã€‚
    """
    logger.trace("--- å¼€å§‹æ‰§è¡Œå…¨åº“èµ„äº§å®Œæ•´æ€§æ‰«æ ---")
    
    try:
        # 1. ä»æ•°æ®åº“è·å–â€œå«Œç–‘äººâ€ (å·²åŒ…å«çˆ¶çº§ä¿¡æ¯)
        bad_items = media_db.get_items_with_potentially_bad_assets()
        total = len(bad_items)
        
        if total == 0:
            logger.info("  âœ… æœªå‘ç°åª’ä½“ä¿¡æ¯å¼‚å¸¸çš„é¡¹ç›®ã€‚")
            task_manager.update_status_from_thread(100, "åª’ä½“ä¿¡æ¯æ‰«æå®Œæˆï¼šæ— å¼‚å¸¸")
            return

        logger.info(f"  âš ï¸ å‘ç° {total} ä¸ªé¡¹ç›®çš„åª’ä½“ä¿¡æ¯å¯èƒ½ä¸å®Œæ•´ï¼Œæ­£åœ¨å¤æ ¸å¹¶æ ‡è®°...")
        
        marked_count = 0
        
        with connection.get_db_connection() as conn:
            cursor = conn.cursor()
            
            for i, item in enumerate(bad_items):
                # è§£æèµ„äº§
                raw_assets = item['asset_details_json']
                assets = json.loads(raw_assets) if isinstance(raw_assets, str) else (raw_assets if isinstance(raw_assets, list) else [])
                
                # å¤æ ¸ (è™½ç„¶ SQL å·²ç»ç­›è¿‡ï¼Œä½† Python å†ç¡®è®¤ä¸€æ¬¡æ›´ç¨³å¦¥)
                is_valid = False
                fail_reason = "æœªçŸ¥åŸå› "
                
                for asset in assets:
                    w = asset.get('width')
                    h = asset.get('height')
                    c = asset.get('video_codec')
                    valid, reason = utils.check_stream_validity(w, h, c)
                    if valid:
                        is_valid = True
                        break
                    fail_reason = reason
                
                if not is_valid:
                    # =========================================================
                    # â˜…â˜…â˜… æ ¸å¿ƒä¼˜åŒ–ï¼šç›´æ¥ä» item å­—å…¸ä¸­æå– Emby ID â˜…â˜…â˜…
                    # =========================================================
                    target_log_id = None
                    target_name = item['title']
                    target_type = item['item_type']
                    final_reason = fail_reason
                    
                    if item['item_type'] == 'Movie':
                        # ç”µå½±ï¼šå–è‡ªå·±çš„ Emby ID
                        e_ids = item.get('emby_item_ids_json')
                        if e_ids and len(e_ids) > 0:
                            target_log_id = e_ids[0]
                            
                    elif item['item_type'] == 'Episode':
                        # åˆ†é›†ï¼šå–çˆ¶å‰§é›†çš„ Emby ID (SQL å·²ç» Join å¥½äº†)
                        p_ids = item.get('parent_emby_ids_json')
                        if p_ids and len(p_ids) > 0:
                            target_log_id = p_ids[0]
                        
                        # ä¼˜å…ˆä½¿ç”¨çˆ¶å‰§é›†æ ‡é¢˜
                        if item.get('parent_title'):
                            target_name = item['parent_title']
                            
                        target_type = 'Series'
                        final_reason = f"[S{item['season_number']}E{item['episode_number']}] {fail_reason}"

                    # å…œåº•ï¼šå¦‚æœå®åœ¨æ²¡æœ‰ Emby ID (æå°‘è§)ï¼Œå›é€€åˆ° TMDb ID
                    if not target_log_id:
                        target_log_id = item['parent_series_tmdb_id'] if item['item_type'] == 'Episode' else item['tmdb_id']

                    # å†™å…¥æ—¥å¿—
                    processor.log_db_manager.save_to_failed_log(
                        cursor, target_log_id, target_name, 
                        f"å…¨åº“æ‰«æå‘ç°å¼‚å¸¸: {final_reason}", 
                        target_type, score=0.0
                    )
                    
                    processor.log_db_manager.save_to_processed_log(cursor, target_log_id, target_name, score=0.0)
                    
                    marked_count += 1
                    
                    if i % 10 == 0:
                        logger.info(f"  âœ [æ ‡è®°] {target_name} (ID: {target_log_id}): {final_reason}")

            conn.commit()

        msg = f"æ‰«æå®Œæˆã€‚å…±å‘ç° {total} ä¸ªå¼‚å¸¸é¡¹ï¼Œå·²å°† {marked_count} ä¸ª(å½’å¹¶å)åŠ å…¥å¾…å¤æ ¸åˆ—è¡¨ã€‚"
        logger.info(f"  âœ… {msg}")
        task_manager.update_status_from_thread(100, msg)

    except Exception as e:
        logger.error(f"æ‰§è¡Œèµ„äº§æ‰«æä»»åŠ¡å¤±è´¥: {e}", exc_info=True)
        task_manager.update_status_from_thread(-1, "ä»»åŠ¡å¤±è´¥")